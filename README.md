<div align="center">

# Some Stars

‚≠ê <a href="https://luohongkun.com/"  target="_blank">LuoHongkun</a>ÁöÑstarÂàóË°®ÔºåÊØè6Â∞èÊó∂Ëá™Âä®Êõ¥Êñ∞,ÂèÇËÄÉÈìæÊé•-><a href="https://linux.do/t/topic/115143"  target="_blank">github starÂàóË°®Ëá™Âä®Êõ¥Êñ∞</a> ‚≠ê

</div><br>

## Table of Contents

*   [Python](#python)
*   [C++](#c)
*   [Jupyter Notebook](#jupyter-notebook)
*   [miscellaneous](#miscellaneous)
*   [TypeScript](#typescript)
*   [CSS](#css)
*   [CMake](#cmake)
*   [Swift](#swift)
*   [Kotlin](#kotlin)
*   [TeX](#tex)
*   [Shell](#shell)
*   [Astro](#astro)
*   [JavaScript](#javascript)
*   [HTML](#html)
*   [C](#c-1)
*   [Rust](#rust)
*   [Go](#go)
*   [Lua](#lua)
*   [Vim Script](#vim-script)
*   [Cuda](#cuda)
*   [Roff](#roff)
*   [Dockerfile](#dockerfile)
*   [C#](#c-2)
*   [MATLAB](#matlab)
*   [Vue](#vue)
*   [Makefile](#makefile)
*   [LLVM](#llvm)
*   [Svelte](#svelte)
*   [Matlab](#matlab-1)
*   [Cython](#cython)
*   [SCSS](#scss)
*   [Dart](#dart)
*   [Markdown](#markdown)
*   [Clojure](#clojure)
*   [Java](#java)

## Python

*   [Wenyueh/MinivLLM](https://github.com/Wenyueh/MinivLLM) - Based on Nano-vLLM, a simple replication of vLLM with self-contained paged attention and flash attention implementation

*   [RPL-CS-UCL/litevloc\_code](https://github.com/RPL-CS-UCL/litevloc_code) - LiteVLoc: Map-Lite Visual Localization for Image Goal Navigation

*   [william13077/IAmGoodNavigator](https://github.com/william13077/IAmGoodNavigator) -

*   [facebookresearch/home-robot](https://github.com/facebookresearch/home-robot) - Mobile manipulation research tools for roboticists

*   [MaureenZOU/m3-spatial](https://github.com/MaureenZOU/m3-spatial) - \[ICLR 2025] Official Implementation of M3: 3D-Spatial Multimodal Memory

*   [ika-rwth-aachen/ros2\_unbag](https://github.com/ika-rwth-aachen/ros2_unbag) - A ROS 2 tool for exporting bags to human readable files. Supports pluggable export routines to handle any message type.

*   [NVlabs/vla0](https://github.com/NVlabs/vla0) - VLA-0: Building State-of-the-Art VLAs with Zero Modification

*   [NVlabs/GR00T-WholeBodyControl](https://github.com/NVlabs/GR00T-WholeBodyControl) - Software stack for loco-manipulation experiments across multiple humanoid platforms, with primary support for the Unitree G1. This repository provides whole-body control policies, a teleoperation stack, and a data exporter.

*   [realsee-developer/RealSee3D](https://github.com/realsee-developer/RealSee3D) - RealSee3D: A multi-view RGB-D dataset combining real-world captures and procedurally generated scenes, with extensible annotations for diverse 3D vision research.

*   [Galery23/SAGE-3D\_Official](https://github.com/Galery23/SAGE-3D_Official) - This is the official repository of the paper "Towards Physically Executable 3D Gaussian for Embodied Navigation".

*   [ZHUANGHP/Any-SSR](https://github.com/ZHUANGHP/Any-SSR) - This is the official code for Any-SSR "Analytic Subspace Routing: How Recursive Least Squares Works in Continual Learning of Large Language Model"

*   [Ericonaldo/visual\_wholebody](https://github.com/Ericonaldo/visual_wholebody) - Train a loco-manipulation dog with RL

*   [facebookresearch/spider](https://github.com/facebookresearch/spider) - A general physic-based retargeting framework.

*   [WzcTHU/SeeNav-Agent](https://github.com/WzcTHU/SeeNav-Agent) -

*   [hanruihua/ir-sim](https://github.com/hanruihua/ir-sim) - A  Python-based lightweight robot simulator designed for navigation, control, and reinforcement learning

*   [Any-4D/Any4D](https://github.com/Any-4D/Any4D) - Any4D: Unified Feed-Forward Metric 4D Reconstruction

*   [cmjang/InternNav-deploy](https://github.com/cmjang/InternNav-deploy) - Edge deployment guide for InternNav-based perception and navigation on Unitree Go2 / Go2W / B2 robots (ROS 2, RealSense, Python).

*   [nvidia-isaac/WBC-AGILE](https://github.com/nvidia-isaac/WBC-AGILE) - Whole Body Control for humanoids: AGILE

*   [Xian-Bei/TALO](https://github.com/Xian-Bei/TALO) - Pushing 3D Vision Foundation Models Towards Globally Consistent Online Reconstruction

*   [ContinualAI/avalanche](https://github.com/ContinualAI/avalanche) - Avalanche: an End-to-End Library for Continual Learning based on PyTorch.

*   [arclab-hku/P2M](https://github.com/arclab-hku/P2M) - \[RA-L'25] A Simple LiDAR-centric End-to-end Navigation Framework in Dynamic Environments

*   [3DAgentWorld/VGGT4D](https://github.com/3DAgentWorld/VGGT4D) - The official implementation of the paper ‚ÄúVGGT4D: Mining Motion Cues in Visual Geometry Transformers for 4D Scene Reconstruction.‚Äù

*   [Mayankm96/isaac-spinning-up](https://github.com/Mayankm96/isaac-spinning-up) - Educational Resource for Isaac Lab

*   [open-gigaai/giga-brain-0](https://github.com/open-gigaai/giga-brain-0) - GigaBrain-0: A World Model-Powered Vision-Language-Action Model

*   [wang-kevin3290/scaling-crl](https://github.com/wang-kevin3290/scaling-crl) -

*   [fanegg/Human3R](https://github.com/fanegg/Human3R) - An unified model for 4D human-scene reconstruction

*   [co-me-tokens/CoMe](https://github.com/co-me-tokens/CoMe) - Release repository of our work "Co-Me: Confidence-Guided Token Merging for Visual Geometric Transformers"

*   [BIT-DYN/OpenGraph](https://github.com/BIT-DYN/OpenGraph) - \[RAL 2024] OpenGraphs: Open-Vocabulary Hierarchical 3D Scene Graphs in Large-Scale Outdoor Environments

*   [amazon-far/TWIST2](https://github.com/amazon-far/TWIST2) - \[arXiv 2025] TWIST2: Scalable, Portable, and Holistic Humanoid Data Collection System

*   [Hilti-Research/hilti-trimble-slam-challenge-2026](https://github.com/Hilti-Research/hilti-trimble-slam-challenge-2026) - 360 Camera-Based Slam Challenge 2026 - Early Release

*   [leggedrobotics/pace-sim2real](https://github.com/leggedrobotics/pace-sim2real) - PACE: A systematic approach for sim-to-real transfer of legged robots, identifying actuator and joint dynamics with standard joint encoders.

*   [xuxw98/Online3D](https://github.com/xuxw98/Online3D) - \[CVPR 2024] Memory-based Adapters for Online 3D Scene Perception

*   [LeCAR-Lab/HDMI](https://github.com/LeCAR-Lab/HDMI) -

*   [EGalahad/sim2real](https://github.com/EGalahad/sim2real) -

*   [KumarRobotics/RT-GuIDE](https://github.com/KumarRobotics/RT-GuIDE) - \[RA-L 2025] RT-GuIDE: Real-Time Gaussian Splatting for Information-Driven Exploration

*   [dcharatan/pixelsplat](https://github.com/dcharatan/pixelsplat) - \[CVPR 2024 Oral, Best Paper Runner-Up] Code for "pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction" by David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann

*   [Motphys/MotrixLab](https://github.com/Motphys/MotrixLab) - A general-purpose machine learning architecture designed for robot training

*   [concept-graphs/concept-graphs](https://github.com/concept-graphs/concept-graphs) - Official code release for ConceptGraphs

*   [UnrealZoo/unrealzoo-gym](https://github.com/UnrealZoo/unrealzoo-gym) - \[ICCV 2025 Highlights] Large-scale photo-realistic virtual worlds for embodied AI

*   [rossning92/helicopter-rl](https://github.com/rossning92/helicopter-rl) - Train a reinforcement learning agent (PPO) to play a retro helicopter arcade game using Stable-Baselines3 and a custom Gymnasium environment.

*   [Jonnyffeler/OutdoorSceneGraph](https://github.com/Jonnyffeler/OutdoorSceneGraph) -

*   [WEIFENG2333/VideoCaptioner](https://github.com/WEIFENG2333/VideoCaptioner) - üé¨ Âç°Âç°Â≠óÂπïÂä©Êâã | VideoCaptioner - Âü∫‰∫é LLM ÁöÑÊô∫ËÉΩÂ≠óÂπïÂä©Êâã - ËßÜÈ¢ëÂ≠óÂπïÁîüÊàê„ÄÅÊñ≠Âè•„ÄÅÊ†°Ê≠£„ÄÅÂ≠óÂπïÁøªËØëÂÖ®ÊµÅÁ®ãÂ§ÑÁêÜÔºÅ- A powered tool for easy and efficient video subtitling.

*   [ymy-k/Hi-SAM](https://github.com/ymy-k/Hi-SAM) - \[IEEE TPAMI] Hi-SAM: Marrying Segment Anything Model for Hierarchical Text Segmentation

*   [FlagOpen/RoboCOIN](https://github.com/FlagOpen/RoboCOIN) - RoboCoin + LeRobot integration

*   [facebookresearch/sam-3d-body](https://github.com/facebookresearch/sam-3d-body) - The repository provides code for running inference with the SAM 3D Body Model (3DB), links for downloading the trained model checkpoints and datasets, and example notebooks that show how to use the model.

*   [lovelyyoshino/TradingAgentForMarket](https://github.com/lovelyyoshino/TradingAgentForMarket) -

*   [facebookresearch/sam-3d-objects](https://github.com/facebookresearch/sam-3d-objects) - SAM 3D Objects

*   [AIR-DISCOVER/FreeAskWorld](https://github.com/AIR-DISCOVER/FreeAskWorld) -  \[AAAI 2026 Oral] FreeAskWorld is an interactive simulation framework that integrates large language models (LLMs) for high-level planning and socially grounded interaction in embodied AI.

*   [linglingxiansen/SocialNav-Map](https://github.com/linglingxiansen/SocialNav-Map) -

*   [linglingxiansen/MapNav](https://github.com/linglingxiansen/MapNav) -

*   [Maxwell-Zhao/RoboSimGS](https://github.com/Maxwell-Zhao/RoboSimGS) - Code for "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting"

*   [Chenkehan21/svm-nav](https://github.com/Chenkehan21/svm-nav) -

*   [fastgs/FastGS](https://github.com/fastgs/FastGS) - Offical code for "FastGS: Training 3D Gaussian Splatting in 100 Seconds"

*   [agrimgupta92/derl](https://github.com/agrimgupta92/derl) - Code for "Embodied Intelligence via Learning and Evolution", Gupta et al, Nature Communications

*   [GREAT-WHU/MASt3R-Fusion](https://github.com/GREAT-WHU/MASt3R-Fusion) - Integrating Feed-Forward Visual Model with IMU, GNSS for High-Functionality SLAM.

*   [mrwangyou/SCOPE](https://github.com/mrwangyou/SCOPE) - Official repository of "Expand Your SCOPE, Semantic Cognition Over Potential-based Exploration for Embodied Visual Navigation"

*   [Livioni/OmniVGGT-official](https://github.com/Livioni/OmniVGGT-official) - OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer

*   [sair-lab/AirRoom](https://github.com/sair-lab/AirRoom) - \[CVPR 2025] AirRoom: Objects Matter in Room Reidentification

*   [ByteDance-Seed/Depth-Anything-3](https://github.com/ByteDance-Seed/Depth-Anything-3) - Depth Anything 3

*   [JIEKE66633/One-click-cleaning-of-C-drive](https://github.com/JIEKE66633/One-click-cleaning-of-C-drive) - Âè™ÈúÄËΩªÊùæ‰∏ÄÁÇπÔºåÂç≥ÂèØÂÆâÂÖ®È´òÊïàÁöÑÊ∏ÖÁêÜCÁõòÊÆãÁïôÂíåÂûÉÂúæÔºåÂπ∂‰∏îÂØπÁîµËÑëÊØ´Êó†Âç±Èô©

*   [LeapLabTHU/AdaptiveNN](https://github.com/LeapLabTHU/AdaptiveNN) - \[Nature Machine Intelligence 2025] Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception

*   [zhoubohan0/NOLO](https://github.com/zhoubohan0/NOLO) - \[IROS 2025 oral] Official implementation of NOLO: Navigate Only Look Once

*   [zhaozijie2022/m3w-marl](https://github.com/zhaozijie2022/m3w-marl) - Official implementation of the paper "Learning and Planning Multi-Agent Tasks via a MoE-based World Model"

*   [HybridRobotics/whole\_body\_tracking](https://github.com/HybridRobotics/whole_body_tracking) -

*   [MrZihan/Dynam3D](https://github.com/MrZihan/Dynam3D) - Official implementation of "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation" (NeurIPS'25 Oral)

*   [facebookresearch/Online-3DGS-Monocular](https://github.com/facebookresearch/Online-3DGS-Monocular) - Code repo for the SIGGRAPH paper "Monocular Online Reconstruction with Enhanced Detail Preservation". Project page https//poiw.github.io/MODP/index.html

*   [wsakobe/TrackVLA](https://github.com/wsakobe/TrackVLA) - \[CoRL 2025] Repository relating to "TrackVLA: Embodied Visual Tracking in the Wild"

*   [unified-force/UniFP](https://github.com/unified-force/UniFP) - CoRL2025 UniFP: Learning a Unified Policy for Position and Force Control in Legged Loco-Manipulation

*   [666ghj/BettaFish](https://github.com/666ghj/BettaFish) - ÂæÆËàÜÔºö‰∫∫‰∫∫ÂèØÁî®ÁöÑÂ§öAgentËàÜÊÉÖÂàÜÊûêÂä©ÊâãÔºåÊâìÁ†¥‰ø°ÊÅØËåßÊàøÔºåËøòÂéüËàÜÊÉÖÂéüË≤åÔºåÈ¢ÑÊµãÊú™Êù•Ëµ∞ÂêëÔºåËæÖÂä©ÂÜ≥Á≠ñÔºÅ‰ªé0ÂÆûÁé∞Ôºå‰∏ç‰æùËµñ‰ªª‰ΩïÊ°ÜÊû∂„ÄÇ

*   [worldbench/3EED](https://github.com/worldbench/3EED) - \[NeurIPS 2025 DB Track] 3EED: Ground Everything Everywhere in 3D

*   [DAVIAN-Robotics/ACG](https://github.com/DAVIAN-Robotics/ACG) - Code for "ACG: Action Coherence Guidance for Flow-based VLA Models"

*   [newton-physics/newton](https://github.com/newton-physics/newton) - An open-source, GPU-accelerated physics simulation engine built upon NVIDIA Warp, specifically targeting roboticists and simulation researchers.

*   [cshizhe/VLN-DUET](https://github.com/cshizhe/VLN-DUET) - Official implementation of Think Global, Act Local: Dual-scale GraphTransformer for Vision-and-Language Navigation (CVPR'22 Oral).

*   [lovelyyoshino/VLFM-Commit](https://github.com/lovelyyoshino/VLFM-Commit) - ÈÄÇÈÖçCUDA11.8„ÄÅhabitat-sim0.2.4ÁâàÊú¨ÁöÑVLFMÔºåÂπ∂ÁªôÂá∫ËØ¶ÁªÜÁöÑ‰ª£Á†ÅÁêÜËß£Ê≥®Èáä

*   [Fudan-MAGIC-Lab/VINGS-Mono](https://github.com/Fudan-MAGIC-Lab/VINGS-Mono) - Source code for \[TRO2025] VINGS-Mono: Visual Inertial Gaussian Splatting Monocular SLAM in Large Scenes.

*   [deepseek-ai/DeepSeek-OCR](https://github.com/deepseek-ai/DeepSeek-OCR) - Contexts Optical Compression

*   [aubingazhib/LightGlueStick](https://github.com/aubingazhib/LightGlueStick) - a Fast and Robust Glue for Joint Point-Line Matching

*   [ReinFlow/ReinFlow](https://github.com/ReinFlow/ReinFlow) - \[NeurIPS 2025] Flow x RL. "ReinFlow: Fine-tuning Flow Policy with Online Reinforcement Learning". Support VLAs e.g., pi0, pi0.5. Fully open-sourced.

*   [woyut/NavQ\_ICCV25](https://github.com/woyut/NavQ_ICCV25) - Implementation of "NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation" (ICCV 2025)

*   [Ausbxuse/Humanoid-Everyday](https://github.com/Ausbxuse/Humanoid-Everyday) - Humanoid dataset for learning

*   [NHirose/OmniVLA](https://github.com/NHirose/OmniVLA) - Official repository for OmniVLA training and inference code

*   [IRMVLab/I2PNet](https://github.com/IRMVLab/I2PNet) - \[TRO 2025] Codes for "End-to-end 2D-3D Registration between Image and LiDAR Point Cloud for Vehicle Localization"

*   [MobiusLqm/MoDGS](https://github.com/MobiusLqm/MoDGS) - Official Implementation of paper accepted by ICLR2025-MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos with Depth Priors

*   [xieyuser/UniGS](https://github.com/xieyuser/UniGS) - Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering

*   [imlixinyang/FlashWorld](https://github.com/imlixinyang/FlashWorld) - Code for "FlashWorld: High-quality 3D Scene Generation within Seconds"

*   [OpenHelix-Team/Spatial-Forcing](https://github.com/OpenHelix-Team/Spatial-Forcing) - Official implementation of Spatial-Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model

*   [starVLA/starVLA](https://github.com/starVLA/starVLA) - StarVLA: A Lego-like Codebase for Vision-Language-Action Model Developing

*   [jzhzhang/Uni-NaVid](https://github.com/jzhzhang/Uni-NaVid) - \[RSS 2025] Uni-NaVid: A Video-based Vision-Language-Action Model for Unifying Embodied Navigation Tasks.

*   [karpathy/nanochat](https://github.com/karpathy/nanochat) - The best ChatGPT that $100 can buy.

*   [Inception3D/TTT3R](https://github.com/Inception3D/TTT3R) - A simple state update rule to enhance length generalization for CUT3R

*   [Zxy-MLlab/LIBERO-PRO](https://github.com/Zxy-MLlab/LIBERO-PRO) - LIBERO-PRO is the official repository of the LIBERO-PRO ‚Äî an evaluation extension of the original LIBERO  benchmark

*   [Eku127/habitat-data-collector](https://github.com/Eku127/habitat-data-collector) - Habitat-based tools for dynamic arrangement and data recording

*   [geyan21/ManiFlow\_Policy](https://github.com/geyan21/ManiFlow_Policy) - \[CoRL 2025] ManiFlow: A General Robot Manipulation Policy via Consistency Flow Training

*   [MIV-XJTU/JanusVLN](https://github.com/MIV-XJTU/JanusVLN) - Official implementation for "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation"

*   [WECENG/ticket-purchase](https://github.com/WECENG/ticket-purchase) - Â§ßÈ∫¶Ëá™Âä®Êä¢Á•®ÔºåÊîØÊåÅ‰∫∫Âëò„ÄÅÂüéÂ∏Ç„ÄÅÊó•ÊúüÂú∫Ê¨°„ÄÅ‰ª∑Ê†ºÈÄâÊã©

*   [fscdc/RewardMap](https://github.com/fscdc/RewardMap) - \[arxiv 2025] RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning

*   [OpenHelix-Team/VLA-RFT](https://github.com/OpenHelix-Team/VLA-RFT) - VLA-RFT: Vision-Language-Action Models with Reinforcement Fine-Tuning

*   [luohongk/Embodied-AI-Daily](https://github.com/luohongk/Embodied-AI-Daily) - üìöËøô‰∏™‰ªìÂ∫ìÊòØÂú®arxiv‰∏äÊî∂ÈõÜÁöÑÊúâÂÖ≥VLNÔºåVLAÔºåWorld ModelÔºåSLAMÔºåGaussian Splatting,ÈùûÁ∫øÊÄß‰ºòÂåñÁ≠âÁõ∏ÂÖ≥ËÆ∫Êñá„ÄÇÊØèÂ§©ÈÉΩ‰ºöËá™Âä®Êõ¥Êñ∞ÔºÅissueÂå∫ÂüüÊòØÊúÄÊñ∞10ÁØáËÆ∫Êñá

*   [jmanhype/vggt-mps](https://github.com/jmanhype/vggt-mps) - VGGT 3D Vision Agent optimized for Apple Silicon with Metal Performance Shaders

*   [AIGeeksGroup/Nav-R1](https://github.com/AIGeeksGroup/Nav-R1) - Nav-R1: Reasoning and Navigation in Embodied Scenes

*   [Alibaba-NLP/DeepResearch](https://github.com/Alibaba-NLP/DeepResearch) - Tongyi Deep Research, the Leading Open-source Deep Research Agent

*   [InternRobotics/InternVLA-A1](https://github.com/InternRobotics/InternVLA-A1) - InternVLA-A1: Unifying Understanding, Generation, and Action for Robotic Manipulation‚Äã

*   [BIT-DYN/omnimap](https://github.com/BIT-DYN/omnimap) - \[TRO 2025] OmniMap: A General Mapping Framework Integrating Optics, Geometry, and Semantics

*   [InternRobotics/InternVLA-M1](https://github.com/InternRobotics/InternVLA-M1) - InternVLA-M1: A Spatially Guided Vision-Language-Action Framework for Generalist Robot Policy

*   [facebookresearch/map-anything](https://github.com/facebookresearch/map-anything) - MapAnything: Universal Feed-Forward Metric 3D Reconstruction

*   [RUC-NLPIR/FlashRAG](https://github.com/RUC-NLPIR/FlashRAG) - ‚ö°FlashRAG: A Python Toolkit for Efficient RAG Research (WWW2025 Resource)

*   [Vid2Sim/Vid2Sim](https://github.com/Vid2Sim/Vid2Sim) - \[CVPR 25] Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation

*   [manycore-research/SpatialGen](https://github.com/manycore-research/SpatialGen) - \[3DV 2026] SpatialGen: Layout-guided 3D Indoor Scene Generation

*   [PRIME-RL/SimpleVLA-RL](https://github.com/PRIME-RL/SimpleVLA-RL) - SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning

*   [NJU-3DV/SpatialVID](https://github.com/NJU-3DV/SpatialVID) - SpatialVID: A Large-Scale Video Dataset with Spatial Annotations

*   [AIGeeksGroup/3D-R1](https://github.com/AIGeeksGroup/3D-R1) - 3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding

*   [wenhuiwei-ustc/BotVIO](https://github.com/wenhuiwei-ustc/BotVIO) -

*   [mystorm16/FastVGGT](https://github.com/mystorm16/FastVGGT) - Code for FastVGGT: Training-Free Acceleration of Visual Geometry Transformer

*   [vllm-project/vllm](https://github.com/vllm-project/vllm) - A high-throughput and memory-efficient inference and serving engine for LLMs

*   [zhangganlin/vista-slam](https://github.com/zhangganlin/vista-slam) - \[3DV 2026] ViSTA-SLAM: Visual SLAM with Symmetric Two-view Association

*   [stepfun-ai/Step-Audio2](https://github.com/stepfun-ai/Step-Audio2) - Step-Audio 2 is an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation.

*   [OpenHelix-Team/LLaVA-VLA](https://github.com/OpenHelix-Team/LLaVA-VLA) - LLaVA-VLA: A Simple Yet Powerful Vision-Language-Action Model \[Actively Maintainedüî•]

*   [JiuTian-VL/CogVLA](https://github.com/JiuTian-VL/CogVLA) - \[NeurIPS 2025] CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification

*   [Heathcliff-saku/BSC-Nav](https://github.com/Heathcliff-saku/BSC-Nav) - This repository is the official implementation of our paper (From reactive to cognitive: brain-inspired spatial intelligence for embodied agents)

*   [LetheSec/HuggingFace-Download-Accelerator](https://github.com/LetheSec/HuggingFace-Download-Accelerator) - Âà©Áî®HuggingFaceÁöÑÂÆòÊñπ‰∏ãËΩΩÂ∑•ÂÖ∑‰ªéÈïúÂÉèÁΩëÁ´ôËøõË°åÈ´òÈÄü‰∏ãËΩΩ„ÄÇ

*   [vuer-ai/vuer](https://github.com/vuer-ai/vuer) - Vuer is a 3D visualization tool for robotics and VR applications.

*   [Tencent-Hunyuan/Hunyuan-GameCraft-1.0](https://github.com/Tencent-Hunyuan/Hunyuan-GameCraft-1.0) - Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with Hybrid History Condition

*   [Tokishx/DifNav](https://github.com/Tokishx/DifNav) - This is the source code to paper ‚ÄúDAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation‚Äù.

*   [cvg/FrontierNet](https://github.com/cvg/FrontierNet) - \[RA-L 2025] FrontierNet: Learning Visual Cues to Explore

*   [CrystalSixone/VLN\_CLASH](https://github.com/CrystalSixone/VLN_CLASH) - This is the official repository for VLN-CLASH.

*   [sgl-project/sglang](https://github.com/sgl-project/sglang) - SGLang is a high-performance serving framework for large language models and multimodal models.

*   [OpenGalaxea/G0](https://github.com/OpenGalaxea/G0) - Galaxea's first VLA release

*   [ziyan-xiaoyu/SpatialMQA](https://github.com/ziyan-xiaoyu/SpatialMQA) -

*   [haotian-liu/LLaVA](https://github.com/haotian-liu/LLaVA) - \[NeurIPS'23 Oral] Visual Instruction Tuning (LLaVA) built towards GPT-4V level capabilities and beyond.

*   [Stanford-TML/HEAD\_rl\_deploy](https://github.com/Stanford-TML/HEAD_rl_deploy) - Official implementation of HEAD CoRL 2025

*   [Zhoues/RoboRefer](https://github.com/Zhoues/RoboRefer) - \[NeurIPS 2025] Official implementation of "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics"

*   [openai/gpt-oss](https://github.com/openai/gpt-oss) - gpt-oss-120b and gpt-oss-20b are two open-weight language models by OpenAI

*   [GuHuangAI/LaDiWM](https://github.com/GuHuangAI/LaDiWM) - code for CoRL2025 "LaDiWM: A Latent Diffusion-based World Model for Predictive Manipulation"

*   [unique1i/SceneSplat](https://github.com/unique1i/SceneSplat) - \[ICCV 2025 Oral] SceneSplat - Gaussian Splatting-based Scene Understanding with Vision-Language Pretraining

*   [wangyr22/DepthGS](https://github.com/wangyr22/DepthGS) - Official implementation of IROS 2025 paper Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline

*   [sapientinc/HRM](https://github.com/sapientinc/HRM) - Hierarchical Reasoning Model Official Release

*   [dfki-ric/better\_launch](https://github.com/dfki-ric/better_launch) - A better replacement for the ROS2 launch system: intuitive, simple, memorable.

*   [chengine/splatnav](https://github.com/chengine/splatnav) -

*   [maturk/dn-splatter](https://github.com/maturk/dn-splatter) - DN-Splatter + AGS-Mesh: Depth and Normal Priors for Gaussian Splatting

*   [Tencent-Hunyuan/HunyuanWorld-1.0](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0) - Generating Immersive, Explorable, and Interactive 3D Worlds from Words or Pixels with Hunyuan3D World Model

*   [Feliciaxyao/NavMorph](https://github.com/Feliciaxyao/NavMorph) - Official implementation of NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments (ICCV'25).

*   [NVlabs/Long-RL](https://github.com/NVlabs/Long-RL) - Long-RL: Scaling RL to Long Sequences (NeurIPS 2025)

*   [RayFronts/RayFronts](https://github.com/RayFronts/RayFronts) - \[IROS 2025] Source code for "RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration"

*   [ShaohonChen/Qwen3-SmVL](https://github.com/ShaohonChen/Qwen3-SmVL) - Â∞ÜSmolVLM2ÁöÑËßÜËßâÂ§¥‰∏éQwen3-0.6BÊ®°ÂûãËøõË°å‰∫ÜÊãºÊé•ÂæÆË∞É

*   [yyfz/Pi3](https://github.com/yyfz/Pi3) - Code of œÄ^3: Permutation-Equivariant Visual Geometry Learning

*   [wzzheng/StreamVGGT](https://github.com/wzzheng/StreamVGGT) - Code for Streaming 4D Visual Geometry Transformer

*   [leandro-svg/HybridTrack](https://github.com/leandro-svg/HybridTrack) - \[RA-L25/ICRA26] HybridTrack: A Hybrid Approach for Robust Multi-Object Tracking

*   [wencan25/Fast3D](https://github.com/wencan25/Fast3D) - \[ACM MM 2025] Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding

*   [Selen-Suyue/MBA](https://github.com/Selen-Suyue/MBA) - \[RA-L 2025] :kissing\_cat: Motion Before Action: Diffusing Object Motion as Manipulation Condition

*   [lisj575/GaussianUDF](https://github.com/lisj575/GaussianUDF) - Code Release for CVPR (2025), "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting"

*   [ColinQiyangLi/qc](https://github.com/ColinQiyangLi/qc) -

*   [facebookresearch/hydra](https://github.com/facebookresearch/hydra) - Hydra is a framework for elegantly configuring complex applications

*   [DengKaiCQ/VGGT-Long](https://github.com/DengKaiCQ/VGGT-Long) - Official implement of VGGT-Long

*   [Zhangwenyao1/DreamVLA](https://github.com/Zhangwenyao1/DreamVLA) - \[NeurIPS 2025] DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge

*   [Sirui-Xu/InterMimic](https://github.com/Sirui-Xu/InterMimic) - \[CVPR 2025 Highlight] InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions

*   [HorizonRobotics/EmbodiedGen](https://github.com/HorizonRobotics/EmbodiedGen) - Towards a Generative 3D World Engine for Embodied Intelligence

*   [yang-zj1026/legged-loco](https://github.com/yang-zj1026/legged-loco) - Low-level locomotion policy training in Isaac Lab

*   [NVlabs/VILA](https://github.com/NVlabs/VILA) - VILA is a family of state-of-the-art vision language models (VLMs) for diverse multimodal AI tasks across the edge, data center, and cloud.

*   [bytedance/F-16](https://github.com/bytedance/F-16) - F-16 is a powerful video large language model (LLM) that perceives high-frame-rate videos, which is developed by the Department of Electronic Engineering at Tsinghua University and ByteDance.

*   [InternRobotics/StreamVLN](https://github.com/InternRobotics/StreamVLN) - Official implementation of the paper: "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling"

*   [AnjieCheng/NaVILA](https://github.com/AnjieCheng/NaVILA) - \[RSS'25] This repository is the implementation of "NaVILA: Legged Robot Vision-Language-Action Model for Navigation"

*   [WHU-USI3DV/PatchAugNet](https://github.com/WHU-USI3DV/PatchAugNet) - PatchAugNet: Patch feature augmentation-based heterogeneous point cloud place recognition in large-scale street scenes

*   [InternRobotics/AnySplat](https://github.com/InternRobotics/AnySplat) - \[SIGGRAPH Asia 2025 (ACM TOG)] AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views

*   [OpenGVLab/InternVL](https://github.com/OpenGVLab/InternVL) - \[CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o.  Êé•ËøëGPT-4oË°®Áé∞ÁöÑÂºÄÊ∫êÂ§öÊ®°ÊÄÅÂØπËØùÊ®°Âûã

*   [FlagOpen/RoboBrain2.0](https://github.com/FlagOpen/RoboBrain2.0) - RoboBrain 2.0: Advanced version of RoboBrain. See Better. Think Harder. Do Smarter. üéâüéâüéâ

*   [unitreerobotics/unitree\_rl\_lab](https://github.com/unitreerobotics/unitree_rl_lab) - This is a repository for reinforcement learning implementation for Unitree robots, based on IsaacLab.

*   [liuff19/LangScene-X](https://github.com/liuff19/LangScene-X) - \[ICCV 2025] LangScene-X: Reconstruct Generalizable 3D Language-Embedded Scenes with TriMap Video Diffusion

*   [OpenDriveLab/DetAny3D](https://github.com/OpenDriveLab/DetAny3D) - \[ICCV 2025] Detect Anything 3D in the Wild

*   [avlmaps/AVLMaps](https://github.com/avlmaps/AVLMaps) - \[ISER 2023] The official implementation of Audio Visual Language Maps for Robot Navigation

*   [youjie-zhou/FMF-SLAM](https://github.com/youjie-zhou/FMF-SLAM) -

*   [google-research/valan](https://github.com/google-research/valan) - Vision and Language Agent Navigation

*   [InternRobotics/CronusVLA](https://github.com/InternRobotics/CronusVLA) - \[AAAI26 oral] CronusVLA: Towards Efficient and Robust Manipulation via Multi-Frame Vision-Language-Action Modeling

*   [zst1406217/VR-Robo](https://github.com/zst1406217/VR-Robo) - \[RA-L 2025] VR-Robo: A Real-to-Sim-to-Real Framework for Visual Robot Navigation and Locomotion

*   [ML-GSAI/LLaDA-V](https://github.com/ML-GSAI/LLaDA-V) -

*   [MIT-SPARK/Clio](https://github.com/MIT-SPARK/Clio) -

*   [hovsg/HOV-SG](https://github.com/hovsg/HOV-SG) - \[RSS2024] Official implementation of "Hierarchical Open-Vocabulary 3D Scene Graphs for Language-Grounded Robot Navigation"

*   [aau-cns/radar\_transformer](https://github.com/aau-cns/radar_transformer) - Transformer-based deep learning architecture for 3D point matching in sparse radar point clouds

*   [iMoonLab/yolov13](https://github.com/iMoonLab/yolov13) - Implementation of "YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception".

*   [nianticlabs/marepo](https://github.com/nianticlabs/marepo) - \[CVPR 2024 Highlight] Map-Relative Pose Regression for Visual Re-Localization

*   [ahydchh/Impromptu-VLA](https://github.com/ahydchh/Impromptu-VLA) -

*   [ut-amrl/creste\_public](https://github.com/ut-amrl/creste_public) - \[RSS 2025] CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance

*   [JohannaXie/GauSS-MI](https://github.com/JohannaXie/GauSS-MI) - \[RSS 2025] GauSS-MI: Gaussian Splatting Shannon Mutual Information for Active 3D Reconstruction

*   [hnuzhy/YOTO](https://github.com/hnuzhy/YOTO) - \[RSS2025] Code for my paper "You Only Teach Once: Learn One-Shot Bimanual Robotic Manipulation from Video Demonstrations"

*   [Qi-Zhangyang/GPT4Scene-and-VLN-R1](https://github.com/Qi-Zhangyang/GPT4Scene-and-VLN-R1) - GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models

*   [tsinghua-fib-lab/Mem4Nav](https://github.com/tsinghua-fib-lab/Mem4Nav) - Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System

*   [PRBonn/PINGS](https://github.com/PRBonn/PINGS) - üìå PINGS: Gaussian Splatting Meets Distance Fields within a Point-Based Implicit Neural Map \[RSS' 25]

*   [Tencent/DepthCrafter](https://github.com/Tencent/DepthCrafter) - \[CVPR 2025 Highlight] DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos

*   [siyuhsu/vla-cache](https://github.com/siyuhsu/vla-cache) - \[NeurIPS 2025] VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation

*   [LMCache/LMCache](https://github.com/LMCache/LMCache) - Supercharge Your LLM with the Fastest KV Cache Layer

*   [openai/openai-cs-agents-demo](https://github.com/openai/openai-cs-agents-demo) - Demo of a customer service use case implemented with the OpenAI Agents SDK

*   [3DTopia/MaterialAnything](https://github.com/3DTopia/MaterialAnything) - \[CVPR 2025 Highlight] Material Anything: Generating Materials for Any 3D Object via Diffusion

*   [LeCAR-Lab/ASAP](https://github.com/LeCAR-Lab/ASAP) - \[RSS 2025] "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills"

*   [zhaihongjia/PanoGS](https://github.com/zhaihongjia/PanoGS) - \[CVPR 2025] PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding

*   [GeeeekExplorer/nano-vllm](https://github.com/GeeeekExplorer/nano-vllm) - Nano vLLM

*   [Fediory/HVI-CIDNet](https://github.com/Fediory/HVI-CIDNet) - \[CVPR2025 && NTIRE2025] HVI: A New Color Space for Low-light Image Enhancement (Official Implementation)

*   [isaac-sim/IsaacSim](https://github.com/isaac-sim/IsaacSim) - NVIDIA Isaac Sim‚Ñ¢ is an open-source application on NVIDIA Omniverse for developing, simulating, and testing AI-driven robots in realistic virtual environments.

*   [jzhzhang/3DAwareNav](https://github.com/jzhzhang/3DAwareNav) - \[CVPR 2023] We propose a framework for the challenging 3D-aware ObjectNav based on two straightforward sub-policies. The two sub-polices, namely corner-guided exploration policy and category-aware identification policy, simultaneously perform by utilizing online fused 3D points as observation.

*   [Fanqi-Lin/OneTwoVLA](https://github.com/Fanqi-Lin/OneTwoVLA) - Official implementation of "OneTwoVLA: A Unified Vision-Language-Action Model with Adaptive Reasoning"

*   [realcrane/3D-student-splatting-and-scooping](https://github.com/realcrane/3D-student-splatting-and-scooping) - This is the source code of our CVPR 2025 Best Paper Honourable Mention paper: 3D Student Splatting and Scooping

*   [microsoft/qlib](https://github.com/microsoft/qlib) - Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped with https://github.com/microsoft/RD-Agent to automate R\&D process.

*   [agent0ai/agent-zero](https://github.com/agent0ai/agent-zero) - Agent Zero AI framework

*   [Shubhamsaboo/awesome-llm-apps](https://github.com/Shubhamsaboo/awesome-llm-apps) - Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.

*   [facebookresearch/habitat-lab](https://github.com/facebookresearch/habitat-lab) - A modular high-level library to train embodied AI agents across a variety of tasks and environments.

*   [karpathy/minGPT](https://github.com/karpathy/minGPT) - A minimal PyTorch re-implementation of the OpenAI GPT (Generative Pretrained Transformer) training

*   [allenzren/open-pi-zero](https://github.com/allenzren/open-pi-zero) - Re-implementation of pi0 vision-language-action (VLA) model from Physical Intelligence

*   [Physical-Intelligence/real-time-chunking-kinetix](https://github.com/Physical-Intelligence/real-time-chunking-kinetix) - Simulated experiments for "Real-Time Execution of Action Chunking Flow Policies".

*   [B0B8K1ng/WMNavigation](https://github.com/B0B8K1ng/WMNavigation) - \[IROS'25 Oral] WMNav: Integrating Vision-Language Models into World Models for Object Goal Navigation

*   [Physical-Intelligence/openpi](https://github.com/Physical-Intelligence/openpi) -

*   [nunchaku-tech/nunchaku](https://github.com/nunchaku-tech/nunchaku) - \[ICLR2025 Spotlight] SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models

*   [InternRobotics/NavDP](https://github.com/InternRobotics/NavDP) - Official implementation of the paper: "NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance"

*   [GeWu-Lab/AnyTouch](https://github.com/GeWu-Lab/AnyTouch) - The repo for "AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors", ICLR 2025

*   [dvlab-research/LISA](https://github.com/dvlab-research/LISA) - Project Page for "LISA: Reasoning Segmentation via Large Language Model"

*   [InternRobotics/InternUtopia](https://github.com/InternRobotics/InternUtopia) - A simulation platform for versatile Embodied AI research and developments.

*   [JunweiLiang/awesome\_lists](https://github.com/JunweiLiang/awesome_lists) - Awesome Lists for Tenure-Track Assistant Professors and PhD students. (Âä©ÁêÜÊïôÊéà/ÂçöÂ£´ÁîüÁîüÂ≠òÊåáÂçó)

*   [Zeying-Gong/Falcon](https://github.com/Zeying-Gong/Falcon) - Official Code for "From Cognition to Precognition: A Future-Aware Framework for Social Navigation" (ICRA 2025)

*   [Zeying-Gong/ascent](https://github.com/Zeying-Gong/ascent) - This is the official code for "Stairway to Success: An Online Floor-Aware Zero-Shot Object-Goal Navigation Framework via LLM-Driven Coarse-to-Fine Exploration"

*   [GradientSpaces/Rectified-Point-Flow](https://github.com/GradientSpaces/Rectified-Point-Flow) - \[NeurIPS 2025, Spotlight] Rectified Point Flow: Generic Point Cloud Pose Estimation

*   [diankun-wu/Spatial-MLLM](https://github.com/diankun-wu/Spatial-MLLM) - Official implementation of Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence

*   [openvla/openvla](https://github.com/openvla/openvla) - OpenVLA: An open-source vision-language-action model for robotic manipulation.

*   [Eku127/DualMap](https://github.com/Eku127/DualMap) - \[RAL-25] An online open-vocabulary mapping system that enables natural language querying to navigate dynamic scenes, with ROS support.

*   [MIT-SPARK/VGGT-SLAM](https://github.com/MIT-SPARK/VGGT-SLAM) - VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold

*   [lichunshang/deep\_ekf\_vio](https://github.com/lichunshang/deep_ekf_vio) -

*   [SunYangtian/UniGeo](https://github.com/SunYangtian/UniGeo) - UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation

*   [Zie619/n8n-workflows](https://github.com/Zie619/n8n-workflows) - all of the workflows of n8n i could find (also from the site itself)

*   [Paper2Poster/Paper2Poster](https://github.com/Paper2Poster/Paper2Poster) - \[NeurIPS 2025 D\&B] Open-source Multi-agent Poster Generation from Papers

*   [resemble-ai/chatterbox](https://github.com/resemble-ai/chatterbox) - SoTA open-source TTS

*   [VITA-Group/VLM-3R](https://github.com/VITA-Group/VLM-3R) - VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D Reconstruction

*   [NMS05/DinoV2-SigLIP-Phi3-LoRA-VLM](https://github.com/NMS05/DinoV2-SigLIP-Phi3-LoRA-VLM) -

*   [Fosowl/agenticSeek](https://github.com/Fosowl/agenticSeek) - Fully Local Manus AI. No APIs, No $200 monthly bills. Enjoy an autonomous agent that thinks, browses the web, and code for the sole cost of electricity. üîî Official updates only via twitter @Martin993886460 (Beware of fake account)

*   [YanyuanQiao/Open-Nav](https://github.com/YanyuanQiao/Open-Nav) - \[ICRA 2025] Official implementation of Open-Nav: Exploring Zero-Shot Vision-and-Language Navigation in Continuous Environment with Open-Source LLMs

*   [facebookresearch/habitat-challenge](https://github.com/facebookresearch/habitat-challenge) - Code for the habitat challenge

*   [DreamTechAI/Direct3D-S2](https://github.com/DreamTechAI/Direct3D-S2) - \[NeurIPS 2025] Direct3D‚ÄëS2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention

*   [AILab-CVC/YOLO-World](https://github.com/AILab-CVC/YOLO-World) - \[CVPR 2024] Real-Time Open-Vocabulary Object Detection

*   [GengzeZhou/NavGPT-2](https://github.com/GengzeZhou/NavGPT-2) - \[ECCV 2024] Official implementation of NavGPT-2: Unleashing Navigational Reasoning Capability for Large Vision-Language Models

*   [hojonathanho/diffusion](https://github.com/hojonathanho/diffusion) - Denoising Diffusion Probabilistic Models

*   [AUTOMATIC1111/stable-diffusion-webui](https://github.com/AUTOMATIC1111/stable-diffusion-webui) - Stable Diffusion web UI

*   [hanruihua/neupan\_ros](https://github.com/hanruihua/neupan_ros) - ROS Wrapper of NeuPAN planner

*   [AgibotTech/agibot\_x1\_train](https://github.com/AgibotTech/agibot_x1_train) - The reinforcement learning training code for AgiBot X1.

*   [unitreerobotics/unitree\_rl\_gym](https://github.com/unitreerobotics/unitree_rl_gym) -

*   [siyuanliii/masa](https://github.com/siyuanliii/masa) - Official Implementation of CVPR24 highlight paper: Matching Anything by Segmenting Anything

*   [OpenDriveLab/UniVLA](https://github.com/OpenDriveLab/UniVLA) - \[RSS 2025] Learning to Act Anywhere with Task-centric Latent Actions

*   [JiuhaiChen/BLIP3o](https://github.com/JiuhaiChen/BLIP3o) - Official implementation of BLIP3o-Series

*   [apple/ml-fastvlm](https://github.com/apple/ml-fastvlm) - This repository contains the official implementation of "FastVLM: Efficient Vision Encoding for Vision Language Models" - CVPR 2025

*   [xming521/WeClone](https://github.com/xming521/WeClone) - üöÄ One-stop solution for creating your digital avatar from chat history üí° Fine-tune LLMs with your chat logs to capture your unique style, then bind to a chatbot to bring your digital self to life.  ‰ªéËÅäÂ§©ËÆ∞ÂΩïÂàõÈÄ†Êï∞Â≠óÂàÜË∫´ÁöÑ‰∏ÄÁ´ôÂºèËß£ÂÜ≥ÊñπÊ°à

*   [bagh2178/UniGoal](https://github.com/bagh2178/UniGoal) - \[CVPR 2025] UniGoal: Towards Universal Zero-shot Goal-oriented Navigation

*   [IDEA-Research/GroundingDINO](https://github.com/IDEA-Research/GroundingDINO) - \[ECCV 2024] Official implementation of the paper "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"

*   [harry0703/MoneyPrinterTurbo](https://github.com/harry0703/MoneyPrinterTurbo) - Âà©Áî®AIÂ§ßÊ®°ÂûãÔºå‰∏ÄÈîÆÁîüÊàêÈ´òÊ∏ÖÁü≠ËßÜÈ¢ë Generate short videos with one click using AI LLM.

*   [QitaoZhao/DiffusionSfM](https://github.com/QitaoZhao/DiffusionSfM) - \[CVPR 2025] "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion" official implementation.

*   [Brummi/anycam](https://github.com/Brummi/anycam) - Official repository for "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos" (CVPR 2025)

*   [lyp-deeplearning/LiftFeat](https://github.com/lyp-deeplearning/LiftFeat) - Code for "LiftFeat: 3D Geometry-Aware Local Feature Matching", ICRA2025

*   [huggingface/nanoVLM](https://github.com/huggingface/nanoVLM) - The simplest, fastest repository for training/finetuning small-sized VLMs.

*   [gradslam/gradslam](https://github.com/gradslam/gradslam) - gradslam is an open source differentiable dense SLAM library for PyTorch

*   [MrZihan/GridMM](https://github.com/MrZihan/GridMM) - Official implementation of GridMM: Grid Memory Map for Vision-and-Language Navigation (ICCV'23).

*   [liangpan99/TokenHSI](https://github.com/liangpan99/TokenHSI) - \[CVPR 2025 Oral] TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization

*   [MrZihan/HNR-VLN](https://github.com/MrZihan/HNR-VLN) - Official implementation of Lookahead Exploration with Neural Radiance Representation for Continuous Vision-Language Navigation (CVPR'24 Highlight).

*   [lllyasviel/FramePack](https://github.com/lllyasviel/FramePack) - Lets make video diffusion practical!

*   [cshizhe/onav\_rim](https://github.com/cshizhe/onav_rim) -

*   [DefaultRui/BEV-Scene-Graph](https://github.com/DefaultRui/BEV-Scene-Graph) - \[ICCV23] Bird‚Äôs-Eye-View Scene Graph for Vision-Language Navigation

*   [chen-judge/MapGPT](https://github.com/chen-judge/MapGPT) - \[ACL 24] The official implementation of MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation.

*   [FarInHeight/To-Match-or-Not-to-Match](https://github.com/FarInHeight/To-Match-or-Not-to-Match) - Official code for "To Match or Not to Match: Revisiting Image Matching for Reliable Visual Place Recognition" CVPR IMW 2025

*   [amaralibey/MixVPR](https://github.com/amaralibey/MixVPR) - MixVPR: Feature Mixing for Visual Place Recognition (WACV 2023)

*   [amaralibey/gsv-cities](https://github.com/amaralibey/gsv-cities) - GSV-Cities: a large-scale dataset for visual place recognition

*   [jiangxinke/Agentic-RAG-R1](https://github.com/jiangxinke/Agentic-RAG-R1) - Agentic RAG R1 Framework via Reinforcement Learning

*   [NVIDIA-AI-IOT/ros2\_nanollm](https://github.com/NVIDIA-AI-IOT/ros2_nanollm) - ROS2 nodes for LLM, VLM, VLA

*   [ZiYang-xie/WorldGen](https://github.com/ZiYang-xie/WorldGen) - üåç WorldGen - Generate Any 3D Scene in Seconds

*   [facebookresearch/flow\_matching](https://github.com/facebookresearch/flow_matching) - A PyTorch library for implementing flow matching algorithms, featuring continuous and discrete flow matching implementations. It includes practical examples for both text and image modalities.

*   [ybgdgh/L3MVN](https://github.com/ybgdgh/L3MVN) - Leveraging Large Language Models for Visual Target Navigation

*   [ibaiGorordo/vggt-pytorch-inference](https://github.com/ibaiGorordo/vggt-pytorch-inference) - Repository for running the VGGT model in PyTorch

*   [facebookresearch/nwm](https://github.com/facebookresearch/nwm) - Official code for the CVPR 2025 paper "Navigation World Models".

*   [DefaultRui/VLN-VER](https://github.com/DefaultRui/VLN-VER) - \[CVPR24] Volumetric Environment Representation for Vision-Language Navigation

*   [pablovela5620/mini-dpvo](https://github.com/pablovela5620/mini-dpvo) -

*   [EricTan7/RAM](https://github.com/EricTan7/RAM) - \[CVPR2025] Official implementation of RAM

*   [Jirl-upenn/VLMnav](https://github.com/Jirl-upenn/VLMnav) - End-to-End Navigation with VLMs

*   [lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet) - Let us control diffusion models!

*   [naokiyokoyama/ovon](https://github.com/naokiyokoyama/ovon) - Open Vocabulary Object Navigation

*   [bdaiinstitute/vlfm](https://github.com/bdaiinstitute/vlfm) - The repository provides code associated with the paper VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation (ICRA 2024)

*   [isaac-sim/IsaacLab](https://github.com/isaac-sim/IsaacLab) - Unified framework for robot learning built on NVIDIA Isaac Sim

*   [NVlabs/HOVER](https://github.com/NVlabs/HOVER) - HOVER

*   [cvlab-kaist/ZeroCo](https://github.com/cvlab-kaist/ZeroCo) - CVPR 2025 (Highlight) : Official implementation of "Cross-View Completion Models are Zero-shot Correspondence Estimators"

*   [NVlabs/PyCuVSLAM](https://github.com/NVlabs/PyCuVSLAM) - Highly accurate and efficient VSLAM system for Python

*   [SWE-agent/SWE-agent](https://github.com/SWE-agent/SWE-agent) - SWE-agent takes a GitHub issue and tries to automatically fix it, using your LM of choice. It can also be employed for offensive cybersecurity or competitive coding challenges. \[NeurIPS 2024]

*   [KTH-RPL/OneMap](https://github.com/KTH-RPL/OneMap) - \[ICRA'25] One Map to Find Them All: Real-time Open-Vocabulary Mapping for Zero-shot Multi-Object Navigation

*   [isri-aist/RoboManipBaselines](https://github.com/isri-aist/RoboManipBaselines) - A software framework integrating various imitation learning methods and benchmark environments for robotic manipulation

*   [AlbertoJaenal/MapAbstractionVPR](https://github.com/AlbertoJaenal/MapAbstractionVPR) - Implementation for Image database abstracion

*   [hanyang-21/VideoScene](https://github.com/hanyang-21/VideoScene) - \[CVPR 2025 Highlight] VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step

*   [subframe7536/maple-font](https://github.com/subframe7536/maple-font) - Maple Mono: Open source monospace font with round corner, ligatures and Nerd-Font icons for IDE and terminal, fine-grained customization options. Â∏¶ËøûÂ≠óÂíåÊéßÂà∂Âè∞ÂõæÊ†áÁöÑÂúÜËßíÁ≠âÂÆΩÂ≠ó‰ΩìÔºå‰∏≠Ëã±ÊñáÂÆΩÂ∫¶ÂÆåÁæé2:1ÔºåÁªÜÁ≤íÂ∫¶ÁöÑËá™ÂÆö‰πâÈÄâÈ°π

*   [sintel-dev/Orion](https://github.com/sintel-dev/Orion) - Unsupervised time series anomaly detection library

*   [lus6-Jenny/RINGSharp](https://github.com/lus6-Jenny/RINGSharp) - \[IEEE T-RO 2025] RING#: PR-by-PE Global Localization with Roto-translation Equivariant Gram Learning.

*   [yuliangguo/depth\_any\_camera](https://github.com/yuliangguo/depth_any_camera) - \[CVPR 2025] Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera

*   [SpatialVLA/SpatialVLA](https://github.com/SpatialVLA/SpatialVLA) - üî• SpatialVLA: a spatial-enhanced vision-language-action model that is trained on 1.1 Million real robot episodes. Accepted at RSS 2025.

*   [apple/ml-matrix3d](https://github.com/apple/ml-matrix3d) - \[CVPR 2025 Highlight] Matrix3D: Large Photogrammetry Model All-in-One

*   [FlagOpen/RoboBrain](https://github.com/FlagOpen/RoboBrain) - \[CVPR 2025] RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete. Official Repository.

*   [arajv/SayNav](https://github.com/arajv/SayNav) - Grounding Large Language Models for Dynamic Planning to Navigation in New Environments

*   [BAAI-DCAI/SpatialBot](https://github.com/BAAI-DCAI/SpatialBot) - The official repo for "SpatialBot: Precise Spatial Understanding with Vision Language Models.

*   [facebookresearch/RAM](https://github.com/facebookresearch/RAM) - A framework to study AI models in Reasoning, Alignment, and use of Memory (RAM).

*   [honghd16/GSA-VLN](https://github.com/honghd16/GSA-VLN) - Official repository of General Scene Adaptation for Vision-and-Language Navigation (ICLR'2025)

*   [MarSaKi/ETPNav](https://github.com/MarSaKi/ETPNav) - \[TPAMI 2024] Official repo of "ETPNav: Evolving Topological Planning for Vision-Language Navigation in Continuous Environments"

*   [Chenkehan21/CA-Nav-code](https://github.com/Chenkehan21/CA-Nav-code) -

*   [CrystalSixone/VLN-GOAT](https://github.com/CrystalSixone/VLN-GOAT) - Repository for Vision-and-Language Navigation via Causal Learning (Accepted by CVPR 2024)

*   [GAMMA-UMD-Outdoor-Navigation/BehAV](https://github.com/GAMMA-UMD-Outdoor-Navigation/BehAV) - BehAV: Behavioral Rule Guided Autonomy Using VLM for Robot Navigation in Outdoor Scenes (ICRA'25)

*   [dillonloh/AdaVLN](https://github.com/dillonloh/AdaVLN) - IsaacSim Extension for Dynamic Objects in Matterport3D Environments for AdaVLN research

*   [vlmaps/vlmaps](https://github.com/vlmaps/vlmaps) - \[ICRA2023] Implementation of Visual Language Maps for Robot Navigation

*   [GradientSpaces/WildGS-SLAM](https://github.com/GradientSpaces/WildGS-SLAM) - \[CVPR 2025] WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments

*   [SakanaAI/AI-Scientist-v2](https://github.com/SakanaAI/AI-Scientist-v2) - The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search

*   [zd11024/NaviLLM](https://github.com/zd11024/NaviLLM) - \[CVPR 2024] The code for paper 'Towards Learning a Generalist Model for Embodied Navigation'

*   [LlamaFamily/Llama-Chinese](https://github.com/LlamaFamily/Llama-Chinese) - Llama‰∏≠ÊñáÁ§æÂå∫ÔºåÂÆûÊó∂Ê±áÊÄªÊúÄÊñ∞LlamaÂ≠¶‰π†ËµÑÊñôÔºåÊûÑÂª∫ÊúÄÂ•ΩÁöÑ‰∏≠ÊñáLlamaÂ§ßÊ®°ÂûãÂºÄÊ∫êÁîüÊÄÅÔºåÂÆåÂÖ®ÂºÄÊ∫êÂèØÂïÜÁî®

*   [RoboVerseOrg/RoboVerse](https://github.com/RoboVerseOrg/RoboVerse) - RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning

*   [yuancaimaiyi/collaborationSfM](https://github.com/yuancaimaiyi/collaborationSfM) - ‰ºóÂåÖSfM

*   [hanruihua/NeuPAN](https://github.com/hanruihua/NeuPAN) - \[TRO 2025] NeuPAN: Direct Point Robot Navigation with End-to-End Model-based Learning.

*   [NVIDIAGameWorks/kaolin](https://github.com/NVIDIAGameWorks/kaolin) - A PyTorch Library for Accelerating 3D Deep Learning Research

*   [wyf3/llm\_related](https://github.com/wyf3/llm_related) - Â§çÁé∞Â§ßÊ®°ÂûãÁõ∏ÂÖ≥ÁÆóÊ≥ïÂèä‰∏Ä‰∫õÂ≠¶‰π†ËÆ∞ÂΩï

*   [rvp-group/Splat-LOAM](https://github.com/rvp-group/Splat-LOAM) - \[ICCV 25] 2D Gaussian Splatting based LiDAR Odometry And Mapping

*   [MAC-VO/MAC-VO](https://github.com/MAC-VO/MAC-VO) - \[ICRA 2025 Best Paper] MAC-VO: Metrics-aware Covariance for Learning-based Stereo Visual Odometry

*   [ffrivera0/reloc3r](https://github.com/ffrivera0/reloc3r) - \[CVPR 2025] Relative camera pose estimation and visual localization with Reloc3r

*   [lpiccinelli-eth/UniK3D](https://github.com/lpiccinelli-eth/UniK3D) - \[CVPR 2025] UniK3D: Universal Camera Monocular 3D Estimation

*   [rerun-io/pi0-lerobot](https://github.com/rerun-io/pi0-lerobot) -

*   [yzqin/dexmv-sim](https://github.com/yzqin/dexmv-sim) - DexMV: Imitation Learning for Dexterous Manipulation from Human Videos, ECCV 2022

*   [LSXI7/MINIMA](https://github.com/LSXI7/MINIMA) - \[CVPR 2025] MINIMA: Modality Invariant Image Matching

*   [om-ai-lab/VLM-R1](https://github.com/om-ai-lab/VLM-R1) - Solve Visual Understanding with Reinforced VLMs

*   [shengjun-zhang/GGN](https://github.com/shengjun-zhang/GGN) - \[NeurIPS 2024] Gaussian Graph Network: Learning Efficient and Generalizable Gaussian Representations from Multi-view Images

*   [mindverse/Second-Me](https://github.com/mindverse/Second-Me) - Train your AI self, amplify you, bridge the world

*   [yanyan-li/4DGS-SLAM](https://github.com/yanyan-li/4DGS-SLAM) - Instead of removing dynamic objects as distractors and reconstructing only static environments, this paper proposes an efficient architecture that incrementally tracks camera poses and establishes the 4D Gaussian radiance fields in unknown scenarios by using a sequence of RGB-D images.

*   [manycore-research/SpatialLM](https://github.com/manycore-research/SpatialLM) - \[NeurIPS 2025] SpatialLM: Training Large Language Models for Structured Indoor Modeling

*   [nv-tlabs/3dgrut](https://github.com/nv-tlabs/3dgrut) - Ray tracing and hybrid rasterization of Gaussian particles

*   [nianticlabs/ace](https://github.com/nianticlabs/ace) - \[CVPR 2023 - Highlight] Accelerated Coordinate Encoding (ACE): Learning to Relocalize in Minutes using RGB and Poses

*   [sunfanyunn/LayoutVLM](https://github.com/sunfanyunn/LayoutVLM) - Official code for "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models" (CVPR 2025)

*   [roomtour3d/roomtour3d-NaviLLM](https://github.com/roomtour3d/roomtour3d-NaviLLM) - \[CVPR 2025] RoomTour3D - Geometry-aware, cheap and automatic data from web videos for embodied navigation

*   [open-mmlab/OpenPCDet](https://github.com/open-mmlab/OpenPCDet) - OpenPCDet Toolbox for LiDAR-based 3D Object Detection.

*   [PengYu-Team/GEODE\_dataset](https://github.com/PengYu-Team/GEODE_dataset) - Extending the Robustness of LiDAR SLAM to Geometrically Degenerate Scenarios

*   [PRBonn/kiss-slam](https://github.com/PRBonn/kiss-slam) - A LiDAR SLAM system that just works

*   [VSLAM-LAB/VSLAM-LAB](https://github.com/VSLAM-LAB/VSLAM-LAB) - A Comprehensive Framework for Visual SLAM Systems and Datasets

*   [HCI-LMC/VLN-SUSA](https://github.com/HCI-LMC/VLN-SUSA) - \[AAAI 2026] Official code for "Agent Journey Beyond RGB: Unveiling Hybrid Semantic-Spatial Environmental Representations for Vision-and-Language Navigation"

*   [QVPR/Patch-NetVLAD](https://github.com/QVPR/Patch-NetVLAD) - Code for the CVPR2021 paper "Patch-NetVLAD: Multi-Scale Fusion of Locally-Global Descriptors for Place Recognition"

*   [Xiaoming-Zhao/PointNav-VO](https://github.com/Xiaoming-Zhao/PointNav-VO) - \[ICCV 2021] Official implementation of "The Surprising Effectiveness of Visual Odometry Techniques for Embodied PointGoal Navigation"

*   [jiachenzhu/DyT](https://github.com/jiachenzhu/DyT) - Code release for DynamicTanh (DyT)

*   [HKUDS/AI-Researcher](https://github.com/HKUDS/AI-Researcher) - \[NeurIPS2025] "AI-Researcher: Autonomous Scientific Innovation" -- A production-ready version: https://novix.science/chat

*   [facebookresearch/vggt](https://github.com/facebookresearch/vggt) - \[CVPR 2025 Best Paper Award] VGGT: Visual Geometry Grounded Transformer

*   [LeeBY68/Hier-SLAM](https://github.com/LeeBY68/Hier-SLAM) - üå≥ \[ICRA'25] Hier-SLAM: Semantic Gaussian Splatting SLAM with Hierarchical Categorical Representation

*   [graphdeco-inria/hierarchical-3d-gaussians](https://github.com/graphdeco-inria/hierarchical-3d-gaussians) - Official implementation of the SIGGRAPH 2024 paper "A Hierarchical 3D Gaussian Representation for Real-Time Rendering of Very Large Datasets"

*   [ali-vilab/MangaNinjia](https://github.com/ali-vilab/MangaNinjia) - \[CVPR 2025 Highlight] Official implementation of "MangaNinja: Line Art Colorization with Precise Reference Following"

*   [FoundationVision/GLEE](https://github.com/FoundationVision/GLEE) - \[CVPR2024 Highlight]GLEE: General Object Foundation Model for Images and Videos at Scale

*   [InternRobotics/EmbodiedScan](https://github.com/InternRobotics/EmbodiedScan) - \[CVPR 2024 & NeurIPS 2024] EmbodiedScan: A Holistic Multi-Modal 3D Perception Suite Towards Embodied AI

*   [NVlabs/FoundationStereo](https://github.com/NVlabs/FoundationStereo) - \[CVPR 2025 Best Paper Nomination] FoundationStereo: Zero-Shot Stereo Matching

*   [THU-MIG/yoloe](https://github.com/THU-MIG/yoloe) - YOLOE: Real-Time Seeing Anything \[ICCV 2025]

*   [NVlabs/curobo](https://github.com/NVlabs/curobo) - CUDA Accelerated Robot Library

*   [whu-lyh/SaliencyI2PLoc](https://github.com/whu-lyh/SaliencyI2PLoc) - Official code of SaliencyI2PLoc

*   [robot-learning-freiburg/LCDNet](https://github.com/robot-learning-freiburg/LCDNet) - PyTorch code for training LCDNet for loop closure detection in LiDAR SLAM. http://rl.uni-freiburg.de/research/lidar-slam-lc

*   [MarSaKi/VLN-BEVBert](https://github.com/MarSaKi/VLN-BEVBert) - \[ICCV 2023} Official repo of "BEVBert: Multimodal Map Pre-training for Language-guided Navigation"

*   [crepuscularlight/SemanticLoopClosure](https://github.com/crepuscularlight/SemanticLoopClosure) - Master thesis regarding semantic loop closure

*   [Ghiara/LEGION](https://github.com/Ghiara/LEGION) - Official implementation of paper on Nature Machine Intelligence: "Preserving and Combining Knowledge in Robotic Lifelong Reinforcement Learning"

*   [OpenHands/OpenHands](https://github.com/OpenHands/OpenHands) - üôå OpenHands: AI-Driven Development

*   [Zhefan-Xu/isaac-go2-ros2](https://github.com/Zhefan-Xu/isaac-go2-ros2) - Unitree Go2 simulation platform for testing navigation, decision-making and autonomous tasks. (NVIDIA Isaac/ROS2)

*   [YicongHong/Recurrent-VLN-BERT](https://github.com/YicongHong/Recurrent-VLN-BERT) - Code of the CVPR 2021 Oral paper: A Recurrent Vision-and-Language BERT for Navigation

*   [gaoxiangjun/Mani-GS](https://github.com/gaoxiangjun/Mani-GS) - \[CVPR' 2025'] Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh

*   [convexsplatting/convex-splatting](https://github.com/convexsplatting/convex-splatting) - \[CVPR 2025 - Highlight] Original implementation of "3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes"

*   [jzhzhang/NaVid-VLN-CE](https://github.com/jzhzhang/NaVid-VLN-CE) - \[RSS 2024 & RSS 2025] VLN-CE evaluation code of NaVid and Uni-NaVid

*   [jacobkrantz/VLN-CE](https://github.com/jacobkrantz/VLN-CE) - Vision-and-Language Navigation in Continuous Environments using Habitat

*   [JeffLIrion/python-graphslam](https://github.com/JeffLIrion/python-graphslam) - Graph SLAM solver in Python

*   [Stability-AI/generative-models](https://github.com/Stability-AI/generative-models) - Generative Models by Stability AI

*   [vdorbala/LGX](https://github.com/vdorbala/LGX) - Code for LGX (Language Guided Exploration). We use LLMs to perform embodied robot navigation in a zero-shot manner.

*   [PKU-VCL-3DV/SLAM3R](https://github.com/PKU-VCL-3DV/SLAM3R) - \[CVPR 2025 Highlight] Real-time dense scene reconstruction with SLAM3R

*   [fanegg/Feat2GS](https://github.com/fanegg/Feat2GS) - \[CVPR2025] Feat2GS: Probing Visual Foundation Models with Gaussian Splatting

*   [MrZihan/Sim2Real-VLN-3DFF](https://github.com/MrZihan/Sim2Real-VLN-3DFF) - Official implementation of Sim-to-Real Transfer via 3D Feature Fields for Vision-and-Language Navigation  (CoRL'24).

*   [csiro-robotics/Pair-VPR](https://github.com/csiro-robotics/Pair-VPR) - \[IEEE RA-L 2025] The official repository for Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification for Visual Place Recognition with Vision Transformers

*   [facebookresearch/fast3r](https://github.com/facebookresearch/fast3r) - \[CVPR 2025] Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass

*   [XiaohanLei/GaussNav](https://github.com/XiaohanLei/GaussNav) - PyTorch implementation of paper: GaussNav: Gaussian Splatting for Visual Navigation

*   [dmar-bonn/active-gs](https://github.com/dmar-bonn/active-gs) - \[RA-L2025] ActiveGS: Active Scene Reconstruction Using Gaussian Splatting

*   [WU-CVGL/Omni-Scene](https://github.com/WU-CVGL/Omni-Scene) - \[CVPR2025] Omni-Scene: Omni-Gaussian Representation for Ego-Centric Sparse-View Scene Reconstruction

*   [liw95/LightLoc](https://github.com/liw95/LightLoc) - \[CVPR2025] LightLoc: Learning Outdoor LiDAR Localization at Light Speed

*   [hzxie/GaussianCity](https://github.com/hzxie/GaussianCity) - The official implementation of "GaussianCity: Generative Gaussian Splatting for Unbounded 3D City Generation". (CVPR 2025)

*   [showlab/ShowUI](https://github.com/showlab/ShowUI) - \[CVPR 2025] Open-source, End-to-end, Vision-Language-Action model for GUI Agent & Computer Use.

*   [iris0329/SeeGround](https://github.com/iris0329/SeeGround) - \[CVPR'25] SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding

*   [pengwangucla/DeLS-3D](https://github.com/pengwangucla/DeLS-3D) - The code for DeLS-3D of CVPR 2018

*   [rpng/calc](https://github.com/rpng/calc) - Convolutional Autoencoder for Loop Closure

*   [CASIA-LMC-Lab/FastSAM](https://github.com/CASIA-LMC-Lab/FastSAM) - Fast Segment Anything

*   [HKUST-Aerial-Robotics/SG-Reg](https://github.com/HKUST-Aerial-Robotics/SG-Reg) - \[T-RO 2025] SG-Reg: Generalizable and Efficient Scene Graph Registration

*   [rmurai0610/MASt3R-SLAM](https://github.com/rmurai0610/MASt3R-SLAM) - \[CVPR 2025] MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors

*   [AirVLN/AirVLN](https://github.com/AirVLN/AirVLN) -

*   [xuxw98/ESAM](https://github.com/xuxw98/ESAM) - \[ICLR 2025, Oral] EmbodiedSAM: Online Segment Any 3D Thing in Real Time

*   [GeLuzhou/Dynamic-GSG](https://github.com/GeLuzhou/Dynamic-GSG) - \[IROS 25] Dynamic 3D Gaussian Scene Graphs for Environment Adaptation

*   [fishmarch/ROSTools](https://github.com/fishmarch/ROSTools) -

*   [sair-lab/AirCode](https://github.com/sair-lab/AirCode) - AirCode: A Robust Object Encoding Method (RA-L, ICRA 2022)

*   [jingyaogong/minimind](https://github.com/jingyaogong/minimind) - üöÄüöÄ „ÄåÂ§ßÊ®°Âûã„Äç2Â∞èÊó∂ÂÆåÂÖ®‰ªé0ËÆ≠ÁªÉ26MÁöÑÂ∞èÂèÇÊï∞GPTÔºÅüåè Train a 26M-parameter GPT from scratch in just 2h!

*   [url-kaist/MambaGlue](https://github.com/url-kaist/MambaGlue) - MambaGlue: Fast and Robust Local Feature Matching With Mamba @ ICRA'25

*   [fraunhoferhhi/AT-GS](https://github.com/fraunhoferhhi/AT-GS) - Adaptive and Temporally Consistent Gaussian Surfels for Multi-view Dynamic Reconstruction

*   [sunsmarterjie/yolov12](https://github.com/sunsmarterjie/yolov12) - \[NeurIPS 2025] YOLOv12: Attention-Centric Real-Time Object Detectors

*   [HKUDS/GraphGPT](https://github.com/HKUDS/GraphGPT) - \[SIGIR'2024] "GraphGPT: Graph Instruction Tuning for Large Language Models"

*   [luigifreda/pyslam](https://github.com/luigifreda/pyslam) - pySLAM is a hybrid Python/C++ Visual SLAM pipeline supporting monocular, stereo, and RGB-D cameras. It provides a broad set of modern local and global feature extractors, multiple loop-closure strategies, a volumetric reconstruction module, integrated depth-prediction models, and semantic segmentation capabilities for enhanced scene understanding.

*   [wangyizhao/PRIOR-SLAM](https://github.com/wangyizhao/PRIOR-SLAM) - PRIOR-SLAM: Enabling Visual SLAM for Loop Closure under Large Viewpoint Variations

*   [Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4) - Open-sourced codes for MiniGPT-4 and MiniGPT-v2 (https://minigpt-4.github.io, https://minigpt-v2.github.io/)

*   [yuanzhoulvpi2017/vscode\_debug\_transformers](https://github.com/yuanzhoulvpi2017/vscode_debug_transformers) -

*   [DavideCatto/XFeat-ONNX](https://github.com/DavideCatto/XFeat-ONNX) -

*   [cvg/limap](https://github.com/cvg/limap) - A toolbox for mapping and localization with line features.

*   [BJHYZJ/DovSG](https://github.com/BJHYZJ/DovSG) - \[RA-L 2025] Dynamic Open-Vocabulary 3D Scene Graphs for Long-term Language-Guided Mobile Manipulation

*   [yang-zj1026/NaVILA-Bench](https://github.com/yang-zj1026/NaVILA-Bench) - Vision-Language Navigation Benchmark in Isaac Lab

*   [CUT3R/CUT3R](https://github.com/CUT3R/CUT3R) - Official implementation of Continuous 3D Perception Model with Persistent State

*   [QwenLM/Qwen3](https://github.com/QwenLM/Qwen3) - Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.

*   [LYX0501/InstructNav](https://github.com/LYX0501/InstructNav) -

*   [huggingface/open-r1](https://github.com/huggingface/open-r1) - Fully open reproduction of DeepSeek-R1

*   [fudan-zvg/DG-SLAM](https://github.com/fudan-zvg/DG-SLAM) - \[NeurIPS 2024] DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization

*   [deepseek-ai/DeepSeek-Coder](https://github.com/deepseek-ai/DeepSeek-Coder) - DeepSeek Coder: Let the Code Write Itself

*   [Irvingao/Point-DETR3D](https://github.com/Irvingao/Point-DETR3D) - \[AAAI 2024] Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-Supervised 3D Object Detection

*   [HaoyiZhu/SPA](https://github.com/HaoyiZhu/SPA) - \[ICLR 2025] SPA: 3D Spatial-Awareness Enables Effective Embodied Representation

*   [zezhishao/DailyArXiv](https://github.com/zezhishao/DailyArXiv) - Daily ArXiv Papers.

*   [microsoft/MoGe](https://github.com/microsoft/MoGe) - \[CVPR'25 Oral] MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision

*   [NVlabs/InstantSplat](https://github.com/NVlabs/InstantSplat) - InstantSplat: Sparse-view SfM-free Gaussian Splatting in Seconds

*   [Nanne/pytorch-NetVlad](https://github.com/Nanne/pytorch-NetVlad) - Pytorch implementation of NetVlad including training on Pittsburgh.

*   [VITA-Group/MM3DGS-SLAM](https://github.com/VITA-Group/MM3DGS-SLAM) - \[IROS 2024] MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision, Depth, and Inertial Measurements

*   [hmz-15/Interactive-Predicate-Learning](https://github.com/hmz-15/Interactive-Predicate-Learning) - InterPreT: Interactive Predicate Learning from Language Feedback for Generalizable Task Planning (RSS 2024)

*   [google-deepmind/mujoco\_menagerie](https://github.com/google-deepmind/mujoco_menagerie) - A collection of high-quality models for the MuJoCo physics engine, curated by Google DeepMind.

*   [GarlanLou/LF-GNSS](https://github.com/GarlanLou/LF-GNSS) - LF-GNSS: A Fundamental Framework for Exploring Learning and Filtering Integration in GNSS

*   [Aceinna/gnss-ins-sim](https://github.com/Aceinna/gnss-ins-sim) - Open-source GNSS + inertial navigation, sensor fusion simulator.  Motion trajectory generator, sensor models, and navigation

*   [modelscope/FunClip](https://github.com/modelscope/FunClip) - Open-source, accurate and easy-to-use video speech recognition & clipping tool, LLM based AI clipping intergrated.

*   [naver/mast3r](https://github.com/naver/mast3r) - Grounding Image Matching in 3D with MASt3R

*   [opendilab/LightZero](https://github.com/opendilab/LightZero) - \[NeurIPS 2023 Spotlight] LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios (awesome MCTS)

*   [naver/dust3r](https://github.com/naver/dust3r) - DUSt3R: Geometric 3D Vision Made Easy

*   [OpenDriveLab/AgiBot-World](https://github.com/OpenDriveLab/AgiBot-World) - \[IROS 2025 Award Finalist] The Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems

*   [facebookresearch/DiT](https://github.com/facebookresearch/DiT) - Official PyTorch Implementation of "Scalable Diffusion Models with Transformers"

*   [RoboTwin-Platform/RoboTwin](https://github.com/RoboTwin-Platform/RoboTwin) - RoboTwin 2.0 Offical Repo

*   [deepseek-ai/DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3) -

*   [MachinePerceptionLab/QQ-SLAM](https://github.com/MachinePerceptionLab/QQ-SLAM) -

*   [YvanYin/Metric3D](https://github.com/YvanYin/Metric3D) - The repo for "Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image" and "Metric3Dv2: A Versatile Monocular Geometric Foundation Model..."

*   [modelscope/ms-agent](https://github.com/modelscope/ms-agent) - MS-Agent: Lightweight Framework for Empowering Agents with Autonomous Exploration in Complex Task Scenarios

*   [Genesis-Embodied-AI/Genesis](https://github.com/Genesis-Embodied-AI/Genesis) - A generative world for general-purpose robotics & embodied AI learning.

*   [ZexinHe/Neural-LightRig](https://github.com/ZexinHe/Neural-LightRig) - \[CVPR2025] Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion

*   [gramuah/ros4vsn](https://github.com/gramuah/ros4vsn) - Evaluation of Visual Semantic Navigation Models in Real Robots

*   [noodle-lab/GaussianSpa](https://github.com/noodle-lab/GaussianSpa) - Project website: https://noodle-lab.github.io/gaussianspa/

*   [PDFMathTranslate/PDFMathTranslate](https://github.com/PDFMathTranslate/PDFMathTranslate) - \[EMNLP 2025 Demo] PDF scientific paper translation with preserved formats - Âü∫‰∫é AI ÂÆåÊï¥‰øùÁïôÊéíÁâàÁöÑ PDF ÊñáÊ°£ÂÖ®ÊñáÂèåËØ≠ÁøªËØëÔºåÊîØÊåÅ Google/DeepL/Ollama/OpenAI Á≠âÊúçÂä°ÔºåÊèê‰æõ CLI/GUI/MCP/Docker/Zotero

*   [myhhub/stock](https://github.com/myhhub/stock) - stockËÇ°Á•®.Ëé∑ÂèñËÇ°Á•®Êï∞ÊçÆ,ËÆ°ÁÆóËÇ°Á•®ÊåáÊ†á,Á≠πÁ†ÅÂàÜÂ∏É,ËØÜÂà´ËÇ°Á•®ÂΩ¢ÊÄÅ,ÁªºÂêàÈÄâËÇ°,ÈÄâËÇ°Á≠ñÁï•,ËÇ°Á•®È™åËØÅÂõûÊµã,ËÇ°Á•®Ëá™Âä®‰∫§Êòì,ÊîØÊåÅPCÂèäÁßªÂä®ËÆæÂ§á„ÄÇ

*   [ayoussf/SuperPoint-PrP](https://github.com/ayoussf/SuperPoint-PrP) -

*   [PKU-YuanGroup/Open-Sora-Plan](https://github.com/PKU-YuanGroup/Open-Sora-Plan) - This project aim to reproduce Sora (Open AI T2V model), we wish the open source community contribute to this project.

*   [hustvl/DiffusionDrive](https://github.com/hustvl/DiffusionDrive) - \[CVPR 2025 Highlight] Truncated Diffusion Model for Real-Time End-to-End Autonomous Driving

*   [ispc-lab/HRegNet](https://github.com/ispc-lab/HRegNet) - \[ICCV 2021] HRegNet: A Hierarchical Network for Large-scale Outdoor LiDAR Point Cloud Registration

*   [ranahanocka/point2mesh](https://github.com/ranahanocka/point2mesh) - Reconstruct Watertight Meshes from Point Clouds \[SIGGRAPH 2020]

*   [TianxingChen/G3Flow](https://github.com/TianxingChen/G3Flow) - \[CVPR 25] G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation

*   [TheBlewish/Automated-AI-Web-Researcher-Ollama](https://github.com/TheBlewish/Automated-AI-Web-Researcher-Ollama) - A python program that turns an LLM, running on Ollama, into an automated researcher, which will with a single query determine focus areas to investigate, do websearches and scrape content from various relevant websites and do research for you all on its own! And more, not limited to but including saving the findings for you!

*   [facebookresearch/neuralfeels](https://github.com/facebookresearch/neuralfeels) - Neural feels with neural fields: Visuo-tactile perception for in-hand manipulation

*   [ori-drs/oxford\_spires\_dataset](https://github.com/ori-drs/oxford_spires_dataset) - \[IJRR 2025] Lidar-visual dataset with ground truth 3D map for SLAM/NeRF

*   [blazzbyte/OpenInterpreterUI](https://github.com/blazzbyte/OpenInterpreterUI) - Simplify code execution with Open Interpreter UI Project with Streamlit. A user-friendly GUI for Python, JavaScript, and more. Pay-as-you-go, no subscriptions. Ideal for beginners.

*   [fudan-zvg/gaussian-raytracing](https://github.com/fudan-zvg/gaussian-raytracing) -

*   [ChenYutongTHU/SplatFormer](https://github.com/ChenYutongTHU/SplatFormer) - \[ICLR' 25] SplatFormer: Point Transformer for Robust 3D Gaussian Splatting

*   [akawincent/ZED-data-collector](https://github.com/akawincent/ZED-data-collector) - In this project, ZED camera is used to extract image, IMU, pose data and convert them into a dataset format as ground truth for evaluation of other SLAM systems

*   [Parskatt/RoMa](https://github.com/Parskatt/RoMa) - \[CVPR 2024] RoMa: Robust Dense Feature Matching; RoMa is the robust dense feature matcher capable of estimating pixel-dense warps and reliable certainties for almost any image pair.

*   [microsoft/autogen](https://github.com/microsoft/autogen) - A programming framework for agentic AI

*   [KwanWaiPang/Gaussian-SLAM\_comment](https://github.com/KwanWaiPang/Gaussian-SLAM_comment) - Gaussian-SLAMÁöÑ‰∏≠ÊñáÊ≥®Èáä

*   [open-mmlab/mmdetection](https://github.com/open-mmlab/mmdetection) - OpenMMLab Detection Toolbox and Benchmark

*   [InternRobotics/VLM-Grounder](https://github.com/InternRobotics/VLM-Grounder) - \[CoRL 2024] VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding

*   [GREAT-WHU/GREAT-Dataset](https://github.com/GREAT-WHU/GREAT-Dataset) -

*   [VladimirYugay/Gaussian-SLAM](https://github.com/VladimirYugay/Gaussian-SLAM) - Gaussian-SLAM: Photo-realistic Dense SLAM with Gaussian Splatting

*   [MisEty/RTG-SLAM](https://github.com/MisEty/RTG-SLAM) - RTG-SLAM: Real-time 3D Reconstruction at Scale Using Gaussian Splatting (ACM SIGGRAPH 2024)

*   [Tencent-Hunyuan/Tencent-Hunyuan-Large](https://github.com/Tencent-Hunyuan/Tencent-Hunyuan-Large) -

*   [cvg/NoPoSplat](https://github.com/cvg/NoPoSplat) - \[ICLR'25 Oral] No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images

*   [megvii-research/MCTrack](https://github.com/megvii-research/MCTrack) - \[IROS2025]This is the offical implementation of the paper "MCTrack: A Unified 3D Multi-Object Tracking Framework for Autonomous Driving"

*   [nv-tlabs/SCube](https://github.com/nv-tlabs/SCube) - \[NeurIPS 2024] SCube: Instant Large-Scale Scene Reconstruction using VoxSplats

*   [princeton-vl/RAFT-Stereo](https://github.com/princeton-vl/RAFT-Stereo) -

*   [ChenHoy/DROID-Splat](https://github.com/ChenHoy/DROID-Splat) - End-to-End SLAM with camera calibration, monocular prior integration and dense Rendering

*   [openinterpreter/open-interpreter](https://github.com/openinterpreter/open-interpreter) - A natural language interface for computers

*   [robot-learning-freiburg/CL-SLAM](https://github.com/robot-learning-freiburg/CL-SLAM) - Continual SLAM: Beyond Lifelong Simultaneous Localization and Mapping through Continual Learning. http://continual-slam.cs.uni-freiburg.de

*   [ywyeli/Place3D](https://github.com/ywyeli/Place3D) - \[NeurIPS'24 Spotlight] Is Your LiDAR Placement Optimized for 3D Scene Understanding?

*   [TommyZihao/openvino\_tonypi](https://github.com/TommyZihao/openvino_tonypi) - Âü∫‰∫éOpenVINOÔºåÊú¨Âú∞ÈÉ®ÁΩ≤Â§ßÊ®°ÂûãÊô∫ËÉΩ‰ΩìAgentÔºåÊéßÂà∂TonyPi‰∫∫ÂΩ¢Êú∫Âô®‰∫∫

*   [donydchen/mvsplat](https://github.com/donydchen/mvsplat) - üåä \[ECCV'24 Oral] MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images

*   [songw-zju/LiDAR2Map](https://github.com/songw-zju/LiDAR2Map) - The official implementation of "LiDAR2Map: In Defense of LiDAR-Based Semantic Map Construction Using Online Camera Distillation" (CVPR 2023)

*   [zhaihongjia/SplatLoc](https://github.com/zhaihongjia/SplatLoc) - \[TVCG 2025] SplatLoc: 3D Gaussian Splatting-based Visual Localization for Augmented Reality

*   [NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) - TensorRT LLM provides users with an easy-to-use Python API to define Large Language Models (LLMs) and supports state-of-the-art optimizations to perform inference efficiently on NVIDIA GPUs. TensorRT LLM also contains components to create Python and C++ runtimes that orchestrate the inference execution in a performant way.

*   [NanmiCoder/MediaCrawler](https://github.com/NanmiCoder/MediaCrawler) - Â∞èÁ∫¢‰π¶Á¨îËÆ∞ | ËØÑËÆ∫Áà¨Ëô´„ÄÅÊäñÈü≥ËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅÂø´ÊâãËßÜÈ¢ë | ËØÑËÆ∫Áà¨Ëô´„ÄÅB Á´ôËßÜÈ¢ë ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÂæÆÂçöÂ∏ñÂ≠ê ÔΩú ËØÑËÆ∫Áà¨Ëô´„ÄÅÁôæÂ∫¶Ë¥¥ÂêßÂ∏ñÂ≠ê ÔΩú ÁôæÂ∫¶Ë¥¥ÂêßËØÑËÆ∫ÂõûÂ§çÁà¨Ëô´  | Áü•‰πéÈóÆÁ≠îÊñáÁ´†ÔΩúËØÑËÆ∫Áà¨Ëô´

*   [microsoft/BitNet](https://github.com/microsoft/BitNet) - Official inference framework for 1-bit LLMs

*   [hkchengrex/Cutie](https://github.com/hkchengrex/Cutie) - \[CVPR 2024 Highlight] Putting the Object Back Into Video Object Segmentation

*   [520xyxyzq/3DGS-CD](https://github.com/520xyxyzq/3DGS-CD) - 3DGS-based change detection for physical object rearrangement

*   [facebookresearch/lingua](https://github.com/facebookresearch/lingua) - Meta Lingua: a lean, efficient, and easy-to-hack codebase to research LLMs.

*   [google/nerfies](https://github.com/google/nerfies) - This is the code for Deformable Neural Radiance Fields, a.k.a. Nerfies.

*   [Owen718/LongPrompt-LLamaGen](https://github.com/Owen718/LongPrompt-LLamaGen) - This repository provides an improved LLamaGen Model, fine-tuned on 500,000 high-quality images, each accompanied by over 300 token prompts. And it's also powered by additional prompt refining features for improved performance.

*   [openai/improved-diffusion](https://github.com/openai/improved-diffusion) - Release for Improved Denoising Diffusion Probabilistic Models

*   [RuijieZhu94/MotionGS](https://github.com/RuijieZhu94/MotionGS) - \[NeurIPS 2024] MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting

*   [hzy46/Deep-Learning-21-Examples](https://github.com/hzy46/Deep-Learning-21-Examples) - „Ää21‰∏™È°πÁõÆÁé©ËΩ¨Ê∑±Â∫¶Â≠¶‰π†‚Äî‚Äî‚ÄîÂü∫‰∫éTensorFlowÁöÑÂÆûË∑µËØ¶Ëß£„ÄãÈÖçÂ•ó‰ª£Á†Å

*   [StanfordVL/3DSceneGraph](https://github.com/StanfordVL/3DSceneGraph) - The data skeleton from "3D Scene Graph: A Structure for Unified Semantics, 3D Space, and Camera" http://3dscenegraph.stanford.edu

*   [cvg/depthsplat](https://github.com/cvg/depthsplat) - \[CVPR'25] DepthSplat: Connecting Gaussian Splatting and Depth

*   [HKUDS/LightRAG](https://github.com/HKUDS/LightRAG) - \[EMNLP2025] "LightRAG: Simple and Fast Retrieval-Augmented Generation"

*   [princeton-vl/DROID-SLAM](https://github.com/princeton-vl/DROID-SLAM) -

*   [jiaoZ7688/YOLOPX](https://github.com/jiaoZ7688/YOLOPX) -

*   [Nightmare-n/DepthAnyVideo](https://github.com/Nightmare-n/DepthAnyVideo) - Depth Any Video with Scalable Synthetic Data (ICLR 2025)

*   [linyicheng1/EdgePoint](https://github.com/linyicheng1/EdgePoint) - EdgePoint: Learning Efficient Keypoint Extraction and Description for Edge Devices

*   [minwoo0611/HeLiOS](https://github.com/minwoo0611/HeLiOS) - \[ICRA2025] HeLiOS: Heterogeneous LiDAR Place Recognition

*   [uzh-rpg/bflow](https://github.com/uzh-rpg/bflow) - Official implementation of "Dense Continuous-Time Optical Flow from Event Cameras"

*   [VITA-Group/LightGaussian](https://github.com/VITA-Group/LightGaussian) - \[NeurIPS 2024 Spotlight]"LightGaussian: Unbounded 3D Gaussian Compression with 15x Reduction and 200+ FPS", Zhiwen Fan, Kevin Wang, Kairun Wen, Zehao Zhu, Dejia Xu, Zhangyang Wang

*   [uzh-rpg/deep\_ev\_tracker](https://github.com/uzh-rpg/deep_ev_tracker) - Repository relating to "Data-driven Feature Tracking for Event Cameras" (CVPR, 2023, Award Candidate) and "Data-driven Feature Tracking for Event Cameras with and without Frames" (T-PAMI 2025)

*   [IRMVLab/DVLO](https://github.com/IRMVLab/DVLO) - \[ECCV 2024 Oral] DVLO: Deep Visual-LiDAR Odometry with Local-to-Global Feature Fusion and Bi-Directional Structure Alignment

*   [QiZS-BIT/GSPR](https://github.com/QiZS-BIT/GSPR) - \[IEEE IROS'25] GSPR: Multimodal Place Recognition using 3D Gaussian Splatting for Autonomous Driving

*   [hustvl/osp](https://github.com/hustvl/osp) - \[ECCV 2024] Occupancy as Set of Points

*   [HuangJunJie2017/BEVDet](https://github.com/HuangJunJie2017/BEVDet) - Code base of the BEVDet series .

*   [city-super/Octree-AnyGS](https://github.com/city-super/Octree-AnyGS) - Octree-GS

*   [buaacyw/MeshAnythingV2](https://github.com/buaacyw/MeshAnythingV2) - \[ICCV 2025] From anything to mesh like human artists. Official impl. of "MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization"

*   [roboflow/supervision](https://github.com/roboflow/supervision) - We write your reusable computer vision tools. üíú

*   [yifanlu0227/ChatSim](https://github.com/yifanlu0227/ChatSim) - \[CVPR2024 Highlight] Editable Scene Simulation for Autonomous Driving via LLM-Agent Collaboration

*   [cjy1992/interp-e2e-driving](https://github.com/cjy1992/interp-e2e-driving) - Interpretable End-to-end Urban Autonomous Driving with Latent Deep Reinforcement Learning

*   [Robertwyq/PanoOcc](https://github.com/Robertwyq/PanoOcc) - \[CVPR 2024] PanoOcc: Unified Occupancy Representation for Camera-based 3D Panoptic Segmentation

*   [TheAlgorithms/Python](https://github.com/TheAlgorithms/Python) - All Algorithms implemented in Python

*   [morrisfl/UniFEx](https://github.com/morrisfl/UniFEx) - Framework for computationally efficient training of universal image feature extraction models.

*   [PeidongLi/SSR](https://github.com/PeidongLi/SSR) - \[ICLR 2025] The official implementation of SSR

*   [qintonguav/ParkingE2E](https://github.com/qintonguav/ParkingE2E) -

*   [hanyangyu1021/LMGaussian](https://github.com/hanyangyu1021/LMGaussian) - official implementation of LM-Gaussian

*   [pytorch/pytorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python with strong GPU acceleration

*   [eth-ait/GaussianHaircut](https://github.com/eth-ait/GaussianHaircut) - Gaussian Haircut: Human Hair Reconstruction with Strand-Aligned 3D Gaussians

*   [pyg-team/pytorch-frame](https://github.com/pyg-team/pytorch-frame) - Tabular Deep Learning Library for PyTorch

*   [jkulhanek/wild-gaussians](https://github.com/jkulhanek/wild-gaussians) - \[NeurIPS'24] WildGaussians: 3D Gaussian Splatting In the Wild

*   [bassamlab/SigmaRL](https://github.com/bassamlab/SigmaRL) - SigmaRL: A Sample-Efficient and Generalizable Multi-Agent Reinforcement Learning Framework for Motion Planning

*   [DLR-MI/UTrack](https://github.com/DLR-MI/UTrack) - Multi-Object Tracking with Uncertain Detections \[ECCV 2024 UnCV]

*   [stanfordnlp/dspy](https://github.com/stanfordnlp/dspy) - DSPy: The framework for programming‚Äînot prompting‚Äîlanguage models

*   [TempleRAIL/drl\_vo\_nav](https://github.com/TempleRAIL/drl_vo_nav) - \[T-RO 2023] DRL-VO: Learning to Navigate Through Crowded Dynamic Scenes Using Velocity Obstacles

*   [lucasbrynte/gasfm](https://github.com/lucasbrynte/gasfm) - Implementation of the CVPR 2024 paper "Learning Structure-from-Motion with Graph Attention Networks".

*   [SPengLiang/OccupancyM3D](https://github.com/SPengLiang/OccupancyM3D) - \[CVPR 2024] Learning Occupancy for Monocular 3D Object Detection

*   [zhangganlin/GlORIE-SLAM](https://github.com/zhangganlin/GlORIE-SLAM) - GlORIE-SLAM: Globally Optimized RGB-only Implicit Encoding Point Cloud SLAM

*   [jbriales/rgbd\_benchmark\_tools](https://github.com/jbriales/rgbd_benchmark_tools) - Tools for TUM RGBD Dataset Benchmark

*   [yastrebksv/TennisProject](https://github.com/yastrebksv/TennisProject) - Tennis analysis using deep learning and machine¬†learning

*   [cvg/GeoCalib](https://github.com/cvg/GeoCalib) - GeoCalib: Learning Single-image Calibration with Geometric Optimization (ECCV 2024)

*   [NVIDIA/TransformerEngine](https://github.com/NVIDIA/TransformerEngine) - A library for accelerating Transformer models on NVIDIA GPUs, including using 8-bit and 4-bit floating point (FP8 and FP4) precision on Hopper, Ada and Blackwell GPUs, to provide better performance with lower memory utilization in both training and inference.

*   [lus6-Jenny/RING](https://github.com/lus6-Jenny/RING) - \[IEEE T-RO 2023] Source code of RING and RING++ for loop closure detection in LiDAR SLAM.

*   [hacksider/Deep-Live-Cam](https://github.com/hacksider/Deep-Live-Cam) - real time face swap and one-click video deepfake with only a single image

*   [GANWANSHUI/GaussianOcc](https://github.com/GANWANSHUI/GaussianOcc) - (ICCV 2025) GaussianOcc: Fully Self-supervised and Efficient 3D Occupancy Estimation with Gaussian Splatting

*   [GradientSpaces/LoopSplat](https://github.com/GradientSpaces/LoopSplat) - \[3DV 2025, Oral] LoopSplat: Loop Closure by Registering 3D Gaussian Splats

*   [zhaofuq/LOD-3DGS](https://github.com/zhaofuq/LOD-3DGS) - LetsGo: Large-Scale Garage Modeling and Rendering via LiDAR-Assisted Gaussian(Published in SIGGRAPH Asia 2024)

*   [hjr37/CP-SLAM](https://github.com/hjr37/CP-SLAM) - CP-SLAM: Collaborative Neural Point-based SLAM

*   [cvg/nicer-slam](https://github.com/cvg/nicer-slam) - \[3DV'24 Best Paper Honorable Mention] NICER-SLAM: Neural Implicit Scene Encoding for RGB SLAM

*   [lpiccinelli-eth/UniDepth](https://github.com/lpiccinelli-eth/UniDepth) - Universal Monocular Metric Depth Estimation

*   [JeongminB/E-D3DGS](https://github.com/JeongminB/E-D3DGS) - \[ECCV 2024] Official repository for "Per-Gaussian Embedding-Based Deformation for Deformable 3D Gaussian Splatting"

*   [spla-tam/SplaTAM](https://github.com/spla-tam/SplaTAM) - SplaTAM: Splat, Track & Map 3D Gaussians for Dense RGB-D SLAM (CVPR 2024)

*   [sparolab/SOLiD](https://github.com/sparolab/SOLiD) - SOTA LiDAR Global Descriptor in LiDAR Place Recognition (accepted in RA-L'24 w/ ICRA'25)

*   [IPNL-POLYU/UrbanNavDataset](https://github.com/IPNL-POLYU/UrbanNavDataset) - UrbanNav:An Open-sourced Multisensory Dataset for Benchmarking Positioning Algorithms Designed for Urban Areas

*   [YuxueYang1204/TrimGS](https://github.com/YuxueYang1204/TrimGS) - Trim 3D Gaussian Splatting for Accurate Geometry Representation

*   [open-mmlab/mmtracking](https://github.com/open-mmlab/mmtracking) - OpenMMLab Video Perception Toolbox. It supports Video Object Detection (VID), Multiple Object Tracking (MOT), Single Object Tracking (SOT), Video Instance Segmentation (VIS) with a unified framework.

*   [huggingface/transformers](https://github.com/huggingface/transformers) - ü§ó Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.

*   [openai/openai-python](https://github.com/openai/openai-python) - The official Python library for the OpenAI API

*   [llmbev/talk2bev](https://github.com/llmbev/talk2bev) - Talk2BEV: Language-Enhanced Bird's Eye View Maps (ICRA'24)

*   [liuyuan-pal/SyncDreamer](https://github.com/liuyuan-pal/SyncDreamer) - \[ICLR 2024 Spotlight] SyncDreamer: Generating Multiview-consistent Images from a Single-view Image

*   [fudan-zvg/4d-gaussian-splatting](https://github.com/fudan-zvg/4d-gaussian-splatting) - \[ICLR 2024] Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting

*   [eriksandstroem/Loopy-SLAM](https://github.com/eriksandstroem/Loopy-SLAM) -

*   [qinzheng93/GeoTransformer](https://github.com/qinzheng93/GeoTransformer) - \[CVPR2022] Geometric Transformer for Fast and Robust Point Cloud Registration

*   [yanyan-li/GeoGaussian](https://github.com/yanyan-li/GeoGaussian) - GeoGaussian: Geometry-aware Gaussian Splatting for Scene Rendering

*   [Parskatt/DeDoDe](https://github.com/Parskatt/DeDoDe) - \[3DV 2024 Oral] DeDoDe üé∂ Detect, Don't Describe --- Describe, Don't Detect, for Local Feature Matching

*   [ericzzj1989/BALF](https://github.com/ericzzj1989/BALF) - \[WACV 2024] BALF: Simple and Efficient Blur Aware Local Feature Detector

*   [lyakaap/NetVLAD-pytorch](https://github.com/lyakaap/NetVLAD-pytorch) - PyTorch implementation of NetVLAD & Online Hardest Triplet Loss.

*   [xiaobiaodu/DreamCar](https://github.com/xiaobiaodu/DreamCar) - \[RA-L 2024] DreamCar: Leveraging Car-specific Prior for in-the-wild 3D Car Reconstruction

*   [nianticlabs/acezero](https://github.com/nianticlabs/acezero) - \[ECCV 2024 - Oral] ACE0 is a learning-based structure-from-motion approach that estimates camera parameters of sets of images by learning a multi-view consistent, implicit scene representation.

*   [meta-llama/llama-models](https://github.com/meta-llama/llama-models) - Utilities intended for use with Llama models.

*   [cs230-stanford/cs230-code-examples](https://github.com/cs230-stanford/cs230-code-examples) - Code examples in pyTorch and Tensorflow for CS230

*   [ddbourgin/numpy-ml](https://github.com/ddbourgin/numpy-ml) - Machine learning, in numpy

*   [lucidrains/vit-pytorch](https://github.com/lucidrains/vit-pytorch) - Implementation of Vision Transformer, a simple way to achieve SOTA in vision classification with only a single transformer encoder, in Pytorch

*   [tarashakhurana/4d-occ-forecasting](https://github.com/tarashakhurana/4d-occ-forecasting) - CVPR 2023: Official code for \`Point Cloud Forecasting as a Proxy for 4D Occupancy Forecasting'

*   [uoip/stereo\_msckf](https://github.com/uoip/stereo_msckf) - Python implementation of Multi-State Constraint Kalman Filter (MSCKF) for Vision-aided Inertial Navigation.

*   [fundamentalvision/BEVFormer](https://github.com/fundamentalvision/BEVFormer) - \[ECCV 2022] This is the official implementation of BEVFormer, a camera-only framework for autonomous driving perception, e.g., 3D object detection and semantic map segmentation.

*   [NVlabs/FB-BEV](https://github.com/NVlabs/FB-BEV) - Official PyTorch implementation of FB-BEV & FB-OCC - Forward-backward view transformation for vision-centric autonomous driving perception

*   [OpenDriveLab/OccNet](https://github.com/OpenDriveLab/OccNet) - \[ICCV 2023] OccNet: Scene as Occupancy

*   [Tsinghua-MARS-Lab/Occ3D](https://github.com/Tsinghua-MARS-Lab/Occ3D) -

*   [ViewFormerOcc/ViewFormer-Occ](https://github.com/ViewFormerOcc/ViewFormer-Occ) - \[ECCV 2024] ViewFormer: Exploring Spatiotemporal Modeling for Multi-View 3D Occupancy Perception via View-Guided Transformers

*   [MCG-NJU/SparseOcc](https://github.com/MCG-NJU/SparseOcc) - \[ECCV 2024] Fully Sparse 3D Occupancy Prediction & RayIoU Evaluation Metric

*   [VISION-SJTU/SparseOcc](https://github.com/VISION-SJTU/SparseOcc) - Official implementation for 'SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction' (CVPR 2024)

*   [weiyithu/SurroundOcc](https://github.com/weiyithu/SurroundOcc) - \[ICCV 2023] SurroundOcc: Multi-camera 3D Occupancy Prediction for Autonomous Driving

*   [autonomousvision/occupancy\_networks](https://github.com/autonomousvision/occupancy_networks) - This repository contains the code for the paper "Occupancy Networks - Learning 3D Reconstruction in Function Space"

*   [SY-007-Research/3dgs\_render\_python](https://github.com/SY-007-Research/3dgs_render_python) -

*   [Ferry-Li/SI-SOD](https://github.com/Ferry-Li/SI-SOD) - ICML2024: Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection

*   [Ferry-Li/SI\_Metric](https://github.com/Ferry-Li/SI_Metric) - A portable computation of Size-Invariant Metrics for ICML2024: Size-invariance Matters: Rethinking Metrics and Losses for Imbalanced Multi-object Salient Object Detection

*   [autonomousvision/mip-splatting](https://github.com/autonomousvision/mip-splatting) - \[CVPR'24 Best Student Paper] Mip-Splatting: Alias-free 3D Gaussian Splatting

*   [Vincentqyw/image-matching-webui](https://github.com/Vincentqyw/image-matching-webui) - ü§ó image matching webui

*   [LiheYoung/Depth-Anything](https://github.com/LiheYoung/Depth-Anything) - \[CVPR 2024] Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data. Foundation Model for Monocular Depth Estimation

*   [microsoft/graphrag](https://github.com/microsoft/graphrag) - A modular graph-based Retrieval-Augmented Generation (RAG) system

*   [rvp-group/vbr-devkit](https://github.com/rvp-group/vbr-devkit) - Vision Benchmark in Rome Development Kit

*   [utiasSTARS/pykitti](https://github.com/utiasSTARS/pykitti) - Python tools for working with KITTI data.

*   [huang-yh/GaussianFormer](https://github.com/huang-yh/GaussianFormer) - \[ECCV 2024] Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction

*   [TQTQliu/MVSGaussian](https://github.com/TQTQliu/MVSGaussian) - \[ECCV 2024] MVSGaussian: Fast Generalizable Gaussian Splatting Reconstruction from Multi-View Stereo

*   [buaacyw/MeshAnything](https://github.com/buaacyw/MeshAnything) - \[ICLR 2025] From anything to mesh like human artists. Official impl. of "MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers"

*   [swc-17/SparseDrive](https://github.com/swc-17/SparseDrive) - SparseDrive: End-to-End Autonomous Driving via Sparse Scene Representation

*   [minghanqin/LangSplat](https://github.com/minghanqin/LangSplat) - Official implementation of the paper "LangSplat: 3D Language Gaussian Splatting" \[CVPR2024 Highlight]

*   [Xinyu-Yi/TransPose](https://github.com/Xinyu-Yi/TransPose) - A real-time motion capture system that estimates poses and global translations using only 6 inertial measurement units

*   [Awesome3DGS/3D-Gaussian-Splatting-Papers](https://github.com/Awesome3DGS/3D-Gaussian-Splatting-Papers) - 3DÈ´òÊñØËÆ∫ÊñáÔºåÊåÅÁª≠Êõ¥Êñ∞ÔºåÊ¨¢Ëøé‰∫§ÊµÅËÆ®ËÆ∫„ÄÇ

*   [JonathonLuiten/Dynamic3DGaussians](https://github.com/JonathonLuiten/Dynamic3DGaussians) -

*   [cvg/glue-factory](https://github.com/cvg/glue-factory) - Training library for local feature detection and matching

*   [cvg/LightGlue](https://github.com/cvg/LightGlue) - LightGlue: Local Feature Matching at Light Speed (ICCV 2023)

*   [muskie82/MonoGS](https://github.com/muskie82/MonoGS) - \[CVPR'24 Highlight & Best Demo Award] Gaussian Splatting SLAM

*   [lukas-blecher/LaTeX-OCR](https://github.com/lukas-blecher/LaTeX-OCR) - pix2tex: Using a ViT to convert images of equations into LaTeX code.

*   [tjiiv-cprg/EPro-PnP](https://github.com/tjiiv-cprg/EPro-PnP) - \[CVPR 2022 Best Student Paper] EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation

*   [ChiWeiHsiao/DeepVO-pytorch](https://github.com/ChiWeiHsiao/DeepVO-pytorch) - PyTorch Implementation of DeepVO

*   [cvg/nice-slam](https://github.com/cvg/nice-slam) - \[CVPR'22] NICE-SLAM: Neural Implicit Scalable Encoding for SLAM

*   [yanyan-li/SLAM-BOOK](https://github.com/yanyan-li/SLAM-BOOK) - ËøôÊòØ‰∏ÄÊú¨ÂÖ≥‰∫éSLAMÁöÑ‰π¶Á®øÔºåÂ∏åÊúõËÉΩÊ∏ÖÊ•öÁöÑ‰ªãÁªçSLAMÁ≥ªÁªü‰∏≠ÁöÑ‰ΩøÁî®ÁöÑÂá†‰ΩïÊñπÊ≥ïÂíåÊ∑±Â∫¶Â≠¶‰π†ÊñπÊ≥ï„ÄÇ‰π¶Á®øÊúÄÂêéÂ∫îËØ•‰ºöËææÂà∞200È°µÂ∑¶Âè≥Ôºå‰π¶Á®øÊØèÁ´†ÂØπÂ∫îÁöÑ‰ª£Á†Å‰πü‰ºöË¢´Êï¥ÁêÜÂá∫Êù•„ÄÇ

*   [Shiaoming/Python-VO](https://github.com/Shiaoming/Python-VO) - A simple python implemented frame-by-frame visual odometry with SuperPoint feature detector and SuperGlue feature matcher.

*   [openxrlab/xrdslam](https://github.com/openxrlab/xrdslam) - Platform for Deep Learning based SLAM

*   [IRMVLab/SNI-SLAM](https://github.com/IRMVLab/SNI-SLAM) - \[CVPR 2024 & TPAMI 2025] SNI-SLAM: Semantic Neural Implicit SLAM

*   [simondlevy/TinyEKF](https://github.com/simondlevy/TinyEKF) - Lightweight C/C++ Extended Kalman Filter with Python for prototyping

*   [meta-llama/llama3](https://github.com/meta-llama/llama3) - The official Meta Llama 3 GitHub site

*   [binary-husky/gpt\_academic](https://github.com/binary-husky/gpt_academic) - ‰∏∫GPT/GLMÁ≠âLLMÂ§ßËØ≠Ë®ÄÊ®°ÂûãÊèê‰æõÂÆûÁî®Âåñ‰∫§‰∫íÊé•Âè£ÔºåÁâπÂà´‰ºòÂåñËÆ∫ÊñáÈòÖËØª/Ê∂¶Ëâ≤/ÂÜô‰Ωú‰ΩìÈ™åÔºåÊ®°ÂùóÂåñËÆæËÆ°ÔºåÊîØÊåÅËá™ÂÆö‰πâÂø´Êç∑ÊåâÈíÆ&ÂáΩÊï∞Êèí‰ª∂ÔºåÊîØÊåÅPythonÂíåC++Á≠âÈ°πÁõÆÂâñÊûê&Ëá™ËØëËß£ÂäüËÉΩÔºåPDF/LaTexËÆ∫ÊñáÁøªËØë&ÊÄªÁªìÂäüËÉΩÔºåÊîØÊåÅÂπ∂Ë°åÈóÆËØ¢Â§öÁßçLLMÊ®°ÂûãÔºåÊîØÊåÅchatglm3Á≠âÊú¨Âú∞Ê®°Âûã„ÄÇÊé•ÂÖ•ÈÄö‰πâÂçÉÈóÆ, deepseekcoder, ËÆØÈ£ûÊòüÁÅ´, ÊñáÂøÉ‰∏ÄË®Ä, llama2, rwkv, claude2, mossÁ≠â„ÄÇ

*   [aipixel/GPS-Gaussian](https://github.com/aipixel/GPS-Gaussian) - \[CVPR 2024 Highlight] The official repo for ‚ÄúGPS-Gaussian: Generalizable Pixel-wise 3D Gaussian Splatting for Real-time Human Novel View Synthesis‚Äù

*   [JiuSan-WesternRegion/KF-GINS-PyVersion](https://github.com/JiuSan-WesternRegion/KF-GINS-PyVersion) - A python version of the KF-GINS

*   [graphdeco-inria/gaussian-splatting](https://github.com/graphdeco-inria/gaussian-splatting) - Original reference implementation of "3D Gaussian Splatting for Real-Time Radiance Field Rendering"

*   [ToniRV/NeRF-SLAM](https://github.com/ToniRV/NeRF-SLAM) - NeRF-SLAM: Real-Time Dense Monocular SLAM with Neural Radiance Fields. https://arxiv.org/abs/2210.13641   +   Sigma-Fusion: Probabilistic Volumetric Fusion for Dense Monocular SLAM  https://arxiv.org/abs/2210.01276

*   [electech6/NeRF-Based-SLAM-Incredible-Insights](https://github.com/electech6/NeRF-Based-SLAM-Incredible-Insights) -

*   [labmlai/annotated\_deep\_learning\_paper\_implementations](https://github.com/labmlai/annotated_deep_learning_paper_implementations) - üßë‚Äçüè´ 60+ Implementations/tutorials of deep learning papers with side-by-side notes üìù; including transformers (original, xl, switch, feedback, vit, ...), optimizers (adam, adabelief, sophia, ...), gans(cyclegan, stylegan2, ...), üéÆ reinforcement learning (ppo, dqn), capsnet, distillation, ... üß†

*   [apachecn/ailearning](https://github.com/apachecn/ailearning) - AiLearningÔºöÊï∞ÊçÆÂàÜÊûê+Êú∫Âô®Â≠¶‰π†ÂÆûÊàò+Á∫øÊÄß‰ª£Êï∞+PyTorch+NLTK+TF2

*   [google-research/omniglue](https://github.com/google-research/omniglue) - Code release for CVPR'24 submission 'OmniGlue'

*   [zhzhang023/LR-Net](https://github.com/zhzhang023/LR-Net) -

*   [autonomousvision/gaussian-opacity-fields](https://github.com/autonomousvision/gaussian-opacity-fields) - \[SIGGRAPH Asia'24 & TOG] Gaussian Opacity Fields: Efficient Adaptive Surface Reconstruction in Unbounded Scenes

*   [JokerJohn/SLAMTools](https://github.com/JokerJohn/SLAMTools) - A script toolkit for SLAM research, including but not limited to various plotting functions, ROS bag processing, and more.

## C++

*   [ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp) - LLM inference in C/C++

*   [cchester25/RLIV\_GS](https://github.com/cchester25/RLIV_GS) -

*   [OctoMap/octomap](https://github.com/OctoMap/octomap) - An Efficient Probabilistic 3D Mapping Framework Based on Octrees. Contains the main OctoMap library, the viewer octovis, and dynamicEDT3D.

*   [shanmo/OrcVIO](https://github.com/shanmo/OrcVIO) - The monocular version of OrcVIO, which reconstructs object using both semantic keypoints and bounding boxes

*   [ethz-asl/panoptic\_mapping](https://github.com/ethz-asl/panoptic_mapping) - A flexible submap-based framework towards spatio-temporally consistent volumetric mapping and scene understanding.

*   [GYH-WHU/SPP\_SPV](https://github.com/GYH-WHU/SPP_SPV) - Êú¨È°πÁõÆÊòØ‰∏Ä‰∏™Âü∫‰∫éC++ÂíåMATLABÁöÑGNSSÂçïÁÇπÂÆö‰Ωç‰∏éÊµãÈÄüËß£ÁÆóÁ≥ªÁªüÔºåÊîØÊåÅGPSÂíåBDSÂèåÁ≥ªÁªüËÅîÂêàÂÆö‰Ωç„ÄÇÁ≥ªÁªüËÉΩÂ§üËß£Á†ÅNovAtel OEM7Ê†ºÂºèÊï∞ÊçÆÔºåÈááÁî®‰º™Ë∑ùËßÇÊµãÂÄºËøõË°åÂçïÁÇπÂÆö‰ΩçÂíåÂ§öÊôÆÂãíËßÇÊµãÂÄºËøõË°åÈÄüÂ∫¶Ëß£ÁÆóÔºåÂπ∂ÂÆûÁé∞ÁîµÁ¶ªÂ±Ç„ÄÅÂØπÊµÅÂ±Ç„ÄÅÂç´ÊòüÈíüÂ∑ÆÁ≠âËØØÂ∑ÆÊîπÊ≠£„ÄÇÁ≥ªÁªüÊèê‰æõ‰∫ãÂêéÂ§ÑÁêÜ„ÄÅÂÆûÊó∂Â§ÑÁêÜÂíåÊï∞ÊçÆÈááÈõÜ‰∏âÁßçÂ∑•‰ΩúÊ®°ÂºèÔºåÊîØÊåÅECEF„ÄÅBLH„ÄÅENUÂùêÊ†áÁ≥ªËΩ¨Êç¢ÔºåÂπ∂ÂèØËÆ°ÁÆóÂÆö‰ΩçËØØÂ∑Æ‰∏éÁ≤æÂ∫¶ÊåáÊ†á„ÄÇÊ≠§Â§ñÔºåÁ≥ªÁªüÈõÜÊàê‰∫ÜMATLABÂèØËßÜÂåñÂäüËÉΩÔºåËá™Âä®ÁîüÊàêËØØÂ∑ÆÂàÜÊûêÂõæË°®Ôºå‰∏∫GNSSÂÆö‰ΩçÁÆóÊ≥ïÁöÑÁ†îÁ©∂ÂíåÂ∫îÁî®Êèê‰æõÂÆåÊï¥ÁöÑÂ∑•ÂÖ∑Âπ≥Âè∞„ÄÇ

*   [ZikangYuan/sr\_livo](https://github.com/ZikangYuan/sr_livo) - \[RA-L 2024] A LiDAR-inertial-visual odometry and mapping system based on the sweep reconstruction method

*   [koide3/glim](https://github.com/koide3/glim) - GLIM: versatile and extensible point cloud-based 3D localization and mapping framework

*   [Robotic-Developer-Road/FAST-LIVO2](https://github.com/Robotic-Developer-Road/FAST-LIVO2) - FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry

*   [MIT-SPARK/Hydra](https://github.com/MIT-SPARK/Hydra) - A system for building 3D Scene Graphs from sensor data in real-time

*   [zhanjiawang/plane\_localization](https://github.com/zhanjiawang/plane_localization) - This is a ROS package for indoor global localization (relocation) based on plane octree and plane features.

*   [3dv-casia/LSLM\_VLoc](https://github.com/3dv-casia/LSLM_VLoc) - \[RAL 2024] Lightweight Structured Line Map Based Visual Localization

*   [rabienrose/crowdsourcing\_visual\_positioning\_system](https://github.com/rabienrose/crowdsourcing_visual_positioning_system) - System of recording data, mapping, visualization and positioning based on mobile phone sensors

*   [EdgarFx/BoWG\_VINS\_Loop](https://github.com/EdgarFx/BoWG_VINS_Loop) - Integrates Bag-of-Word-Groups (BoWG) loop closure detection into VINS-Fusion

*   [EdgarFx/BoWG](https://github.com/EdgarFx/BoWG) - The official source code for "Bag of Word Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing" (IROS 2025)

*   [linyicheng1/LET-NET2](https://github.com/linyicheng1/LET-NET2) - An end-to-end lightweight CNN designed for sparse corner extraction and tracking

*   [WeijieMax/EyeReal](https://github.com/WeijieMax/EyeReal) - Offcial Code of EyeReal

*   [LiangHongY/fusions\_slam](https://github.com/LiangHongY/fusions_slam) - fastlio+rtk+speed,ieskf

*   [2toinf/X-VLA](https://github.com/2toinf/X-VLA) - The offical Implementation of "Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model"

*   [InternRobotics/OpenHomie](https://github.com/InternRobotics/OpenHomie) - Open-sourced code for "HOMIE: Humanoid Loco-Manipulation with Isomorphic Exoskeleton Cockpit".

*   [google-deepmind/mujoco](https://github.com/google-deepmind/mujoco) - Multi-Joint dynamics with Contact. A general purpose physics simulator.

*   [rpng/sqrtVINS](https://github.com/rpng/sqrtVINS) - Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking

*   [gaoxiang12/lightning-lm](https://github.com/gaoxiang12/lightning-lm) - Lidar Localization and Mapping

*   [robotics-upo/D-LIO](https://github.com/robotics-upo/D-LIO) - D-LIO: 6DoF Direct LiDAR-Inertial Odometry based on Simultaneous Truncated Distance Field Mapping

*   [SteveMacenski/slam\_toolbox](https://github.com/SteveMacenski/slam_toolbox) - Slam Toolbox for lifelong mapping and localization in potentially massive maps with ROS

*   [hku-mars/BALM](https://github.com/hku-mars/BALM) - An efficient and consistent bundle adjustment for lidar mapping

*   [HorizonRobotics/GeoFlowSlam](https://github.com/HorizonRobotics/GeoFlowSlam) - A Robust Tightly-Coupled RGBD-Inertial and Legged Odometry Fusion SLAM for Dynamic Legged Robotics

*   [KumarRobotics/kr\_3d\_active\_ms\_slam](https://github.com/KumarRobotics/kr_3d_active_ms_slam) - \[RA-L 2024] 3D Active Metric-Semantic SLAM

*   [Zhefan-Xu/LV-DOT](https://github.com/Zhefan-Xu/LV-DOT) - LV-DOT: LiDAR-Visual Dynamic Obstacle Detection and Tracking (C++/Python/ROS)

*   [hku-mars/VoxelMap](https://github.com/hku-mars/VoxelMap) - \[RA-L 2022] An efficient and probabilistic adaptive voxel mapping method for LiDAR odometry

*   [ethz-mrl/OKVIS2-X](https://github.com/ethz-mrl/OKVIS2-X) - OKVIS2-X: Open Keyframe-based Visual-Inertial SLAM Configurable with Dense Depth or LiDAR, and GNSS

*   [ZikangYuan/sr\_lio](https://github.com/ZikangYuan/sr_lio) - \[IROS 2024] A  LiDAR-inertial odometry (LIO) package that can adjust the execution frequency beyond the sweep frequency

*   [openai/openai-icpc-2025](https://github.com/openai/openai-icpc-2025) - OpenAI 2025 ICPC Submissions

*   [PRBonn/rko\_lio](https://github.com/PRBonn/rko_lio) - A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modelling

*   [ShuoYangRobotics/Cerberus](https://github.com/ShuoYangRobotics/Cerberus) - Visual-Inertial-Leg Odometry For Legged Robots

*   [huashu996/Onion-LO](https://github.com/huashu996/Onion-LO) -

*   [hku-mars/LIV\_handhold\_2](https://github.com/hku-mars/LIV_handhold_2) - LIV-Eye: A Low-Cost LiDAR-Inertial-Visual Fusion 3D Sensor for Robotics and Embodied AI.

*   [qiayuanl/legged\_control](https://github.com/qiayuanl/legged_control) - NMPC, WBC, state estimation, and sim2real framework for legged robots based on OCS2 and ros-controls

*   [taichi-dev/taichi](https://github.com/taichi-dev/taichi) - Productive, portable, and performant GPU programming in Python.

*   [niessner/Matterport](https://github.com/niessner/Matterport) - Matterport3D is a pretty awesome dataset for RGB-D machine learning tasks :)

*   [ethz-asl/COIN-LIO](https://github.com/ethz-asl/COIN-LIO) - ü™ô COIN-LIO: Complementary Intensity-Augmented LiDAR Inertial Odometry (ICRA 2024)

*   [princeton-vl/DPVO](https://github.com/princeton-vl/DPVO) - Deep Patch Visual Odometry/SLAM

*   [APRIL-ZJU/Gaussian-LIC](https://github.com/APRIL-ZJU/Gaussian-LIC) - \[ICRA 2025] Gaussian-LIC: Real-Time Photo-Realistic SLAM with Gaussian Splatting and LiDAR-Inertial-Camera Fusion

*   [GTLIDAR/emobipednav](https://github.com/GTLIDAR/emobipednav) - emotion-aware Social Navigation for Bipedal Robots with Deep Reinforcement Learning

*   [zhouyong1234/my\_ekf\_package](https://github.com/zhouyong1234/my_ekf_package) - ‰ΩøÁî®Âç°Â∞îÊõºÊª§Ê≥¢ÂÆûÁé∞Â§ö‰º†ÊÑüÂô®Êï∞ÊçÆËûçÂêà

*   [HViktorTsoi/PV-LIO](https://github.com/HViktorTsoi/PV-LIO) - A probabilistic voxelmap-based LiDAR-Inertial Odometry.

*   [lab-sun/SLAMesh](https://github.com/lab-sun/SLAMesh) - The official implementation of SLAMesh.

*   [hyye/lio-mapping](https://github.com/hyye/lio-mapping) - Implementation of Tightly Coupled 3D Lidar Inertial Odometry and Mapping (LIO-mapping)

*   [nubot-nudt/SG-SLAM](https://github.com/nubot-nudt/SG-SLAM) - \[IROS 25] Leveraging Semantic Graphs for Efficient and Robust LiDAR SLAM

*   [xz00/fast-lio2-map-based-localization](https://github.com/xz00/fast-lio2-map-based-localization) - map-based localization.Modified from fast-lio2.

*   [shahram95/SuperPointSLAM3](https://github.com/shahram95/SuperPointSLAM3) -

*   [QCL0920/AF-RLIO](https://github.com/QCL0920/AF-RLIO) - \[ICRA 2025] AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information forRobust Odometry in Challenging Environments

*   [hku-mars/STD](https://github.com/hku-mars/STD) - A 3D point cloud descriptor for place recognition

*   [yuhaozhang7/NGD-SLAM](https://github.com/yuhaozhang7/NGD-SLAM) - \[IROS 2025] NGD-SLAM: Towards Real-Time Dynamic SLAM without GPU.

*   [hku-mars/M2Mapping](https://github.com/hku-mars/M2Mapping) - \[ICRA 2025] Neural Surface Reconstruction and Rendering for LiDAR-Visual Systems

*   [hku-mars/ImMesh](https://github.com/hku-mars/ImMesh) - ImMesh: An Immediate LiDAR Localization and Meshing Framework

*   [openvinotoolkit/openvino](https://github.com/openvinotoolkit/openvino) - OpenVINO‚Ñ¢ is an open source toolkit for optimizing and deploying AI inference

*   [sjtuyinjie/Ground-Fusion2](https://github.com/sjtuyinjie/Ground-Fusion2) - Ground-Fusion++: a modular sensor-fusion SLAM system(IROS2025)

*   [url-kaist/TRAVEL](https://github.com/url-kaist/TRAVEL) - Traversable ground and above-ground object segmentation using graph representation of 3D LiDAR scans

*   [jiachengliu3/OpenWBC](https://github.com/jiachengliu3/OpenWBC) - VR-based Robot Teleoperation and Data Collection System for Unitree G1

*   [zm0612/funny\_lidar\_slam](https://github.com/zm0612/funny_lidar_slam) - A real-time multifunctional Lidar SLAM package.

*   [arclab-hku/ESVIO](https://github.com/arclab-hku/ESVIO) - (RAL2023+IROS2023) ESVIO: Event-based Stereo Visual Inertial Odometry

*   [NVIDIA-ISAAC-ROS/isaac\_ros\_pose\_estimation](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_pose_estimation) - Deep learned, NVIDIA-accelerated 3D object pose estimation

*   [chengwei0427/ESKF\_LIO](https://github.com/chengwei0427/ESKF_LIO) - IESKF-LIO reference to fast\_lio1.0(ÂèÇËÄÉfast-lioÊó©ÊúüÁâàÊú¨ÔºåÂ§çÁé∞ÁöÑfast-lio2)

*   [Mechazo11/ros2\_orb\_slam3](https://github.com/Mechazo11/ros2_orb_slam3) - A ROS2 Humble package that natively implementing ORB-SLAM3 V1.0 VSLAM framework

*   [GuoYongyu/Dynamic-Line-ORB-SLAM2](https://github.com/GuoYongyu/Dynamic-Line-ORB-SLAM2) - A ORB-SLAM2 instance with dynamic object removing and point-line features optimization.

*   [ZikangYuan/dynamic\_lio](https://github.com/ZikangYuan/dynamic_lio) - \[IROS 2025] A LiDAR-inertial odometry for dynamic environments

*   [xbpeng/DeepMimic](https://github.com/xbpeng/DeepMimic) - Motion imitation with deep reinforcement learning.

*   [zhh2005757/FAST-LIO-Multi-Sensor-Fusion](https://github.com/zhh2005757/FAST-LIO-Multi-Sensor-Fusion) - Fusing GNSS and wheel measurements based on FAST-LIO and IKFOM

*   [yjsx/CELLmap](https://github.com/yjsx/CELLmap) - \[ICRA 2025]CELLmap: Enhancing LiDAR SLAM through Elastic and Lightweight Spherical Map Representation

*   [microsoft/WSL](https://github.com/microsoft/WSL) - Windows Subsystem for Linux

*   [unitreerobotics/unitree\_ros\_to\_real](https://github.com/unitreerobotics/unitree_ros_to_real) -

*   [unitreerobotics/unitree\_mujoco](https://github.com/unitreerobotics/unitree_mujoco) -

*   [Geekgineer/YOLOs-CPP](https://github.com/Geekgineer/YOLOs-CPP) - A high-performance C++ headers for real-time object detection and segmentation using YOLO models, leveraging ONNX Runtime and OpenCV for seamless integration. Supports multiple YOLO (v5, v7, v8, v9, v10, v11, v12) with optimized inference on CPU and GPU. Includes sample code, scripts for image, video, and live camera inference, and quantization.

*   [zydddd/CornerVINS](https://github.com/zydddd/CornerVINS) -

*   [HuangCongQing/pcl-learning](https://github.com/HuangCongQing/pcl-learning) - üî•PCLÔºàPoint Cloud LibraryÔºâÁÇπ‰∫ëÂ∫ìÂ≠¶‰π†ËÆ∞ÂΩï

*   [lavaman131/dinov2.cpp](https://github.com/lavaman131/dinov2.cpp) - DINOv2 inference engine written in C/C++ using ggml and OpenCV.

*   [libing64/pose\_ekf](https://github.com/libing64/pose_ekf) - Extented Kalman Filter for 6D pose estimation using gps, imu, magnetometer and sonar sensor.

*   [HKUST-Aerial-Robotics/A-LOAM](https://github.com/HKUST-Aerial-Robotics/A-LOAM) - Advanced implementation of LOAM

*   [KTH-RPL/dufomap](https://github.com/KTH-RPL/dufomap) - \[RA-L'24] DUFOMap: Efficient Dynamic Awareness Mapping

*   [ZikangYuan/voxel\_svio](https://github.com/ZikangYuan/voxel_svio) - \[RA-L 2025 Accept without Revision] A stereo visual-inertial odometry system based on voxel map

*   [ayushgaud/path\_planning](https://github.com/ayushgaud/path_planning) - Quadcopter path planning using RRT\* and minimum jerk trajectory generation

*   [BohemianRhapsodyz/PSINS-ROS](https://github.com/BohemianRhapsodyz/PSINS-ROS) - A Strapdown Inertial Navigation System (PSINS) C++ algorithm and Integrated Navigation (GNSS/INS/Odometry) algorithm based on Kalman Filter for ROS

*   [TJU-Aerial-Robotics/YOPO](https://github.com/TJU-Aerial-Robotics/YOPO) - You Only Plan Once: A Learning Based Quadrotor Planner

*   [gisbi-kim/lt-mapper](https://github.com/gisbi-kim/lt-mapper) - A Modular Framework for LiDAR-based Lifelong Mapping

*   [RayShark0605/On\_the\_fly\_SfM](https://github.com/RayShark0605/On_the_fly_SfM) -

*   [hku-mars/FAST-Calib](https://github.com/hku-mars/FAST-Calib) - A Handy Extrinsic Calibration Tool for LiDAR-camera Systems.

*   [deepglint/FAST\_LIO\_LOCALIZATION\_HUMANOID](https://github.com/deepglint/FAST_LIO_LOCALIZATION_HUMANOID) - Localization by LiDAR for Humanoid(like Unitree G1)

*   [MIT-SPARK/Kimera-VIO-ROS](https://github.com/MIT-SPARK/Kimera-VIO-ROS) - ROS wrapper for Kimera-VIO

*   [LC-Robotics/FreeDOM](https://github.com/LC-Robotics/FreeDOM) - FreeDOM: Online Dynamic Object Removal Framework for Static Map Construction Based on Conservative Free Space Estimation \[RA-L 25]

*   [NVIDIA-ISAAC-ROS/isaac\_ros\_visual\_slam](https://github.com/NVIDIA-ISAAC-ROS/isaac_ros_visual_slam) - Visual SLAM/odometry package based on NVIDIA-accelerated cuVSLAM

*   [RainerKuemmerle/g2o](https://github.com/RainerKuemmerle/g2o) - g2o: A General Framework for Graph Optimization

*   [FeiGeChuanShu/Mask2Former-ncnn](https://github.com/FeiGeChuanShu/Mask2Former-ncnn) - naive c++ version of Mask2Former with ncnn

*   [DreamWaterFound/Prerequisites-of-On-line-Semantic-VSLAM](https://github.com/DreamWaterFound/Prerequisites-of-On-line-Semantic-VSLAM) - Âú®Á∫øËØ≠‰πâËßÜËßâSLAMÂü∫Á°ÄÔºöC++ËØ≠Ë®ÄÁ®ãÂ∫è‰∏≠Ë∞ÉÁî®PythonÂÆûÁé∞ÁöÑÂõæÂÉèÂàÜÂâ≤ÁΩëÁªú„ÄÅËé∑ÂèñÂàÜÂâ≤ÁªìÊûú

*   [DreamWaterFound/Codes](https://github.com/DreamWaterFound/Codes) - Ëá™Â∑±ÁöÑ‰∏Ä‰∫õÈõ∂Êï£‰ª£Á†ÅÂêàÈõÜ

*   [sair-lab/GroundSLAM](https://github.com/sair-lab/GroundSLAM) - GroundSLAM: A Robust Visual SLAM System for Warehouse Robots Using Ground Textures

*   [weihaoysgs/vins-fast](https://github.com/weihaoysgs/vins-fast) - VINS has been completely reconstructed and rewritten using C++ object-oriented, and supports stereo or stereo+ IMU.

*   [weihaoysgs/ssvio](https://github.com/weihaoysgs/ssvio) - A lightweight setero visual SLAM system implementation, including complete closed-loop detection, front-end tracking, back-end optimization, visualization and other parts.

*   [SlamMate/CDS-SLAM-Semantic-mapping-in-dynamic-environment](https://github.com/SlamMate/CDS-SLAM-Semantic-mapping-in-dynamic-environment) - This project is the result of my undergraduate dissertation. The localization in dynamic environment is to deploy TensorRT optimized YOLOX in the front end of ORB-SLAM3 for object detection, and then eliminate all points belonging to the human bounding box. At the same time, the semantic information is sent to the mapping module to dye the 3D point cloud. The disadvantage of this project is that in the localization  module, only the points belonging to people are processed, because people are dynamic most of the time. In the mapping module, we did not segment semantic objects accurately, resulting in wrong coloring of point clouds of other objects.

*   [rpng/ov\_plane](https://github.com/rpng/ov_plane) - A monocular plane-aided visual-inertial odometry

*   [KumarRobotics/SLIDE\_SLAM](https://github.com/KumarRobotics/SLIDE_SLAM) - SlideSLAM: Sparse, Lightweight, Decentralized Metric-Semantic SLAM for Multi-Robot Navigation

*   [lixiny/ORB-SLAM2-DualCam](https://github.com/lixiny/ORB-SLAM2-DualCam) - üéì  SJTU M.S. Dissertation. Âü∫‰∫éÂ§öÁõ∏Êú∫ÁöÑÂêåÊ≠•ÂÆö‰Ωç‰∏éÂª∫ÂõæÊñπÊ≥ïÁ†îÁ©∂

*   [chengwei0427/II-NVM](https://github.com/chengwei0427/II-NVM) - \[RA-L'25 & IROS'25] II-NVM: Enhancing Map Accuracy and Consistency with Normal Vector-Assisted Mapping

*   [superxslam/SuperOdom](https://github.com/superxslam/SuperOdom) - A highly robust and accurate LiDAR-only,  LiDAR-inertial odometry

*   [Tang-KaiKai/EDLine](https://github.com/Tang-KaiKai/EDLine) - Line Segment Extraction Algorithm( less than 2ms in 1280\*720 gray image  )

*   [ycdfwzy/PL-MSCKF](https://github.com/ycdfwzy/PL-MSCKF) -

*   [lian-yue0515/D-LI-Init](https://github.com/lian-yue0515/D-LI-Init) - Dynamic Initialization for LiDAR-inertial SLAM

*   [haosulab/SAPIEN](https://github.com/haosulab/SAPIEN) - SAPIEN Embodied AI Platform

*   [HITSZ-NRSL/RCPCC](https://github.com/HITSZ-NRSL/RCPCC) - \[ICRA 2025] Real-Time LiDAR Point Cloud Compression and Transmission for Resource-constrained Robots

*   [christopherdoer/rio](https://github.com/christopherdoer/rio) - RIO - EKF-based Radar Inertial Odometry using 4D mmWave radar sensors

*   [HKUST-Aerial-Robotics/FALCON](https://github.com/HKUST-Aerial-Robotics/FALCON) - \[T-RO 2024] FALCON: Fast Autonomous Aerial Exploration using Coverage Path Guidance.

*   [Happy-ZZX/PL-VIWO](https://github.com/Happy-ZZX/PL-VIWO) - Lightweight and Robust Point-Line Monocular Visual Inertial Wheel Odometry (IROS2025)

*   [RoboSense-Robotics/robosense\_ac\_slam](https://github.com/RoboSense-Robotics/robosense_ac_slam) - A Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry (LIVO).

*   [Zhefan-Xu/Intent-MPC](https://github.com/Zhefan-Xu/Intent-MPC) - \[IEEE RA-L'25] Intent Prediction-Driven Model Predictive Control for UAV Planning and Navigation in Dynamic Environments (C++/ROS)

*   [KumarRobotics/AllocNet](https://github.com/KumarRobotics/AllocNet) - A lightweight learning-based trajectory optimization framework.

*   [Livox-SDK/livox\_mapping](https://github.com/Livox-SDK/livox_mapping) - A mapping package for Livox LiDARs

*   [InternRobotics/HorizonGS](https://github.com/InternRobotics/HorizonGS) - \[CVPR 2025] Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale Aerial-to-Ground Scenes

*   [ShijieZhou-UCLA/feature-3dgs](https://github.com/ShijieZhou-UCLA/feature-3dgs) - \[CVPR 2024 Highlight] Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable Distilled Feature Fields

*   [yanliang-wang/FAST\_LIO\_LC](https://github.com/yanliang-wang/FAST_LIO_LC) - The tight integration of FAST-LIO with Radius-Search-based loop closure module.

*   [gabime/spdlog](https://github.com/gabime/spdlog) - Fast C++ logging library.

*   [hku-mars/GS-SDF](https://github.com/hku-mars/GS-SDF) - \[IROS 2025] LiDAR-Augmented Gaussian Splatting and Neural SDF for Geometrically Consistent Rendering and Reconstruction

*   [JokerJohn/Cloud\_Map\_Evaluation](https://github.com/JokerJohn/Cloud_Map_Evaluation) - \[RAL' 25 & IROS‚Äò 25] MapEval: Towards Unified, Robust and Efficient SLAM Map Evaluation Framework.

*   [SHAILAB-IPEC/OpenFly-Platform](https://github.com/SHAILAB-IPEC/OpenFly-Platform) -

*   [RobustFieldAutonomyLab/LeGO-LOAM](https://github.com/RobustFieldAutonomyLab/LeGO-LOAM) - LeGO-LOAM: Lightweight and Ground-Optimized Lidar Odometry and Mapping on Variable Terrain

*   [HKUST-Aerial-Robotics/SLIM](https://github.com/HKUST-Aerial-Robotics/SLIM) -

*   [SlamCabbage/NDTMC](https://github.com/SlamCabbage/NDTMC) - \[IROS 2024] A 3D Global Descriptor For Loop Closure Detection. NDT-Map-Code.

*   [Joanna-HE/LIGO.](https://github.com/Joanna-HE/LIGO.) -

*   [gogojjh/M-LOAM](https://github.com/gogojjh/M-LOAM) - Robust Odometry and Mapping for Multi-LiDAR Systems with Online Extrinsic Calibration

*   [JokerJohn/PALoc](https://github.com/JokerJohn/PALoc) - \[TMECH'2024] Official codes of ‚ÄùPALoc: Advancing SLAM Benchmarking with Prior-Assisted 6-DoF Trajectory Generation and Uncertainty Estimation‚Äú

*   [sikang/mpl\_ros](https://github.com/sikang/mpl_ros) - A ROS wrapper for trajectory planning based on motion primitives

*   [Zhefan-Xu/NavRL](https://github.com/Zhefan-Xu/NavRL) - \[IEEE RA-L'25] NavRL: Learning Safe Flight in Dynamic Environments (NVIDIA Isaac/Python/ROS1/ROS2)

*   [Zhefan-Xu/CERLAB-UAV-Autonomy](https://github.com/Zhefan-Xu/CERLAB-UAV-Autonomy) - \[CMU] A Versatile and Modular Framework Designed for Autonomous Unmanned Aerial Vehicles \[UAVs] (C++/ROS/PX4)

*   [JD-hust/gs-dso](https://github.com/JD-hust/gs-dso) - a monocular direct sparse odometry with prior continuous 3D gaussian maps for indoor environments

*   [YWL0720/YOLO\_ORB\_SLAM3\_with\_pointcloud\_map](https://github.com/YWL0720/YOLO_ORB_SLAM3_with_pointcloud_map) - This code is an extended version of YOLO\_ORB\_SLAM3, which adds the functionality of creating dense point cloud maps.

*   [engcang/FAST-LIO-SAM](https://github.com/engcang/FAST-LIO-SAM) - a SLAM implementation combining FAST-LIO2 with pose graph optimization and loop closing based on LIO-SAM paper

*   [lausen001/LIO-SAM-DetailedNote](https://github.com/lausen001/LIO-SAM-DetailedNote) - LIO-SAMÊ∫êÁ†ÅËØ¶ÁªÜÊ≥®ÈáäÔºå3D SLAMËûçÂêàÊøÄÂÖâ„ÄÅIMU„ÄÅGPS

*   [deepseek-ai/FlashMLA](https://github.com/deepseek-ai/FlashMLA) - FlashMLA: Efficient Multi-head Latent Attention Kernels

*   [78/xiaozhi-esp32](https://github.com/78/xiaozhi-esp32) - An MCP-based chatbot | ‰∏Ä‰∏™Âü∫‰∫éMCPÁöÑËÅäÂ§©Êú∫Âô®‰∫∫

*   [brucezhcw/VINS-Explorer](https://github.com/brucezhcw/VINS-Explorer) - A Super Tightly Coupled Visual-Inertial State Estimator

*   [fishmarch/ORB-SLAM3-Dense](https://github.com/fishmarch/ORB-SLAM3-Dense) -

*   [fishmarch/ORB\_SLAM3\_Fixed](https://github.com/fishmarch/ORB_SLAM3_Fixed) - Fixed some bugs of original ORB\_SLAM3

*   [mp3guy/ElasticFusion](https://github.com/mp3guy/ElasticFusion) - Real-time dense visual SLAM system

*   [ethz-asl/maplab](https://github.com/ethz-asl/maplab) - A Modular and Multi-Modal Mapping Framework

*   [MIT-SPARK/Spatial-Hash](https://github.com/MIT-SPARK/Spatial-Hash) - Minimal C++ library for spatial data structures based on voxel-block-hashing

*   [MIT-SPARK/Khronos](https://github.com/MIT-SPARK/Khronos) - Spatio-Temporal Metric-Semantic SLAM

*   [shichaoy/cube\_slam](https://github.com/shichaoy/cube_slam) - CubeSLAM: Monocular 3D Object Detection and SLAM

*   [HKUST-Aerial-Robotics/GS-LIVO](https://github.com/HKUST-Aerial-Robotics/GS-LIVO) -

*   [ethz-mrl/GSFusion](https://github.com/ethz-mrl/GSFusion) - GSFusion: Online RGB-D Mapping Where Gaussian Splatting Meets TSDF Fusion

*   [shg8/3DGS.cpp](https://github.com/shg8/3DGS.cpp) -  A cross-platform, high performance renderer for Gaussian Splatting using Vulkan Compute. Supports Windows, Linux, macOS, iOS, and visionOS

*   [hyperlogic/splatapult](https://github.com/hyperlogic/splatapult) - A 3d gaussian splatting renderer in C++ and OpenGL

*   [MIT-SPARK/Kimera-VIO](https://github.com/MIT-SPARK/Kimera-VIO) - Visual Inertial Odometry with SLAM capabilities and 3D Mesh generation.

*   [colmap/colmap](https://github.com/colmap/colmap) - COLMAP - Structure-from-Motion and Multi-View Stereo

*   [HKUST-Aerial-Robotics/G3Reg](https://github.com/HKUST-Aerial-Robotics/G3Reg) - A fast and robust global registration library for outdoor LiDAR point clouds.

*   [hku-mars/FAST-LIVO](https://github.com/hku-mars/FAST-LIVO) - A Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry (LIVO).

*   [jedeschaud/ct\_icp](https://github.com/jedeschaud/ct_icp) - CT-ICP: Continuous-Time LiDAR Odometry

*   [hku-mars/r3live](https://github.com/hku-mars/r3live) - A Robust, Real-time, RGB-colored, LiDAR-Inertial-Visual tightly-coupled state Estimation and mapping package

*   [gisbi-kim/SC-LIO-SAM](https://github.com/gisbi-kim/SC-LIO-SAM) - LiDAR-inertial SLAM: Scan Context + LIO-SAM

*   [lovelyyoshino/FAST\_LIO2\_Noted](https://github.com/lovelyyoshino/FAST_LIO2_Noted) - FAST\_LIO2\_Noted ‰∏≠ÊñáÊ≥®ÈáäÁâà

*   [luohongk/SuperVINS](https://github.com/luohongk/SuperVINS) - üìñ\[IEEE Sensors Journal (JSEN) ] SuperVINS: A Real-Time Visual-Inertial SLAM Framework for Challenging Imaging Conditions (integrated deep learning features)

*   [linyicheng1/ceres-example](https://github.com/linyicheng1/ceres-example) - some ceres examples with notes

*   [XRIM-Lab/GS-CPR](https://github.com/XRIM-Lab/GS-CPR) - \[ICLR 2025] Official repo of "GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting"

*   [PJLab-ADG/SensorsCalibration](https://github.com/PJLab-ADG/SensorsCalibration) - OpenCalib: A Multi-sensor Calibration Toolbox for Autonomous Driving

*   [IF-A-CAT/LIR-LIVO](https://github.com/IF-A-CAT/LIR-LIVO) - LIR-LIVO: A Lightweight,Robust Lidar/Vision/Inertial Odometry with Illumination-Resilient Deep Features

*   [PJLab-ADG/Livox-Mapping](https://github.com/PJLab-ADG/Livox-Mapping) - An all-in-one and ready-to-use LiDAR-inertial odometry system for Livox LiDAR

*   [ROBOT-WSC/BEV-LSLAM](https://github.com/ROBOT-WSC/BEV-LSLAM) - BEV-LSLAM: A Novel and Compact BEV LiDAR SLAM for Outdoor Environment

*   [YungeCui/BoW3D](https://github.com/YungeCui/BoW3D) - \[RA-L] BoW3D: Bag of Words for Real-Time Loop Closing in 3D LiDAR SLAM.

*   [hku-mars/SUPER](https://github.com/hku-mars/SUPER) -

*   [sdwyc/ROLO](https://github.com/sdwyc/ROLO) - This is a ROS package for lidar odometry implementation using rotation optimization method.

*   [liquorleaf/OmniGS](https://github.com/liquorleaf/OmniGS) - \[WACV 2025] OmniGS: Fast Radiance Field Reconstruction using Omnidirectional Gaussian Splatting

*   [johannes-graeter/limo](https://github.com/johannes-graeter/limo) - Lidar-Monocular Visual Odometry

*   [HKUST-Aerial-Robotics/ESVO](https://github.com/HKUST-Aerial-Robotics/ESVO) - This repository maintains the implementation of "Event-based Stereo Visual Odometry".

*   [strasdat/Sophus](https://github.com/strasdat/Sophus) - C++ implementation of Lie Groups using Eigen.

*   [YWL0720/YOLO\_ORB\_SLAM3](https://github.com/YWL0720/YOLO_ORB_SLAM3) - This is an improved version of ORB-SLAM3 that adds an object detection module implemented with YOLOv5 to achieve SLAM in dynamic environments.

*   [ACFR-RPG/DynOSAM](https://github.com/ACFR-RPG/DynOSAM) - Offical code release for DynoSAM: Dynamic Object Smoothing And Mapping. Accepted Transactions on Robotics (Visual SLAM SI). A visual SLAM framework and pipeline for Dynamic environements, estimating for the motion/pose of objects and their structure, as well as the camera odometry and static map.

*   [suchetanrs/ORB-SLAM3-ROS2-Docker](https://github.com/suchetanrs/ORB-SLAM3-ROS2-Docker) - This repository contains a full wrapper class for running ORB-SLAM3 on a docker container with ROS2 Humble with Ubuntu 22.04.

*   [introlab/rtabmap](https://github.com/introlab/rtabmap) - RTAB-Map library and standalone application

*   [hku-mars/FAST\_LIO](https://github.com/hku-mars/FAST_LIO) - A computationally efficient and robust LiDAR-inertial odometry (LIO) package

*   [YibinWu/LIO-EKF](https://github.com/YibinWu/LIO-EKF) - \[ICRA2024] Maybe the simplest LiDAR-inertial odometry that one can have.

*   [lpercc/HA3D\_simulator](https://github.com/lpercc/HA3D_simulator) - Official implementation of Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions (NeurIPS DB Track'24 Spotlight).

*   [google/minja](https://github.com/google/minja) - A minimalistic C++ Jinja templating engine for LLM chat templates

*   [Unsigned-Long/eKalibr](https://github.com/Unsigned-Long/eKalibr) - \[IEEE RA-L 2025] eKalibr: Event Camera Calibration Framework

*   [onnx/onnx-tensorrt](https://github.com/onnx/onnx-tensorrt) - ONNX-TensorRT: TensorRT backend for ONNX

*   [tqdm/tqdm.cpp](https://github.com/tqdm/tqdm.cpp) - C++ port of tqdm

*   [ethz-asl/lidar\_align](https://github.com/ethz-asl/lidar_align) - A simple method for finding the extrinsic calibration between a 3D lidar and a 6-dof pose sensor

*   [alexhua/Aria2-Manager](https://github.com/alexhua/Aria2-Manager) - A useful tool to run Aria2 in the background easily

*   [MrNeRF/Light\_Glue\_CPP](https://github.com/MrNeRF/Light_Glue_CPP) - CPP Implementation of "LightGlue: Local Feature Matching at Light Speed"

*   [HKUST-Aerial-Robotics/RIO](https://github.com/HKUST-Aerial-Robotics/RIO) - Optimization Based and Point Uncertainty Aware Radar-inertial Odometry for 4D Radar System

*   [OctoMap/octomap\_msgs](https://github.com/OctoMap/octomap_msgs) - ROS package to provide messages and serializations / conversion for the OctoMap library

*   [pierotofy/OpenSplat](https://github.com/pierotofy/OpenSplat) - Production-grade 3D gaussian splatting with CPU/GPU support for Windows, Mac and Linux üöÄ

*   [SJTU-ViSYS/Ground-Fusion](https://github.com/SJTU-ViSYS/Ground-Fusion) - Ground-Fusion: A Low-cost Ground SLAM System Robust to Corner Cases (ICRA2024)

*   [wlxing1901/eroam](https://github.com/wlxing1901/eroam) -

*   [tum-vision/dvo\_slam](https://github.com/tum-vision/dvo_slam) - Dense Visual Odometry and SLAM

*   [Zhefan-Xu/time\_optimizer](https://github.com/Zhefan-Xu/time_optimizer) - \[IEEE ICRA'24] Optimal Trajectory Time Allocation Library for Autonomous Robots (C++/ROS)

*   [UnknownFreeOccupied/ufomap](https://github.com/UnknownFreeOccupied/ufomap) - UFOMap: An Efficient Probabilistic 3D Mapping Framework That Embraces the Unknown

*   [lturing/ORB\_SLAM3\_ROS](https://github.com/lturing/ORB_SLAM3_ROS) -

*   [lturing/vins\_fusion\_pangolin](https://github.com/lturing/vins_fusion_pangolin) -

*   [unitreerobotics/point\_lio\_unilidar](https://github.com/unitreerobotics/point_lio_unilidar) - Point-LIO algorithm for Unitree LiDAR products.

*   [hku-mars/IKFoM](https://github.com/hku-mars/IKFoM) - A computationally efficient and convenient toolkit of iterated Kalman filter.

*   [facebookresearch/Replica-Dataset](https://github.com/facebookresearch/Replica-Dataset) - The Replica Dataset v1 as published in https://arxiv.org/abs/1906.05797 .

*   [stereolabs/zed-sdk](https://github.com/stereolabs/zed-sdk) - ‚ö°Ô∏èThe spatial perception framework for rapidly building smart robots and spaces

*   [stereolabs/zed-ros-wrapper](https://github.com/stereolabs/zed-ros-wrapper) - ROS wrapper for the ZED SDK

*   [hku-mars/ikd-Tree](https://github.com/hku-mars/ikd-Tree) - This repository provides implementation of an incremental k-d tree for robotic applications.

*   [triple-Mu/YOLOv8-TensorRT](https://github.com/triple-Mu/YOLOv8-TensorRT) - YOLOv8 using TensorRT accelerate !

*   [Glencsa/YOLOv8-ORB-SLAM3](https://github.com/Glencsa/YOLOv8-ORB-SLAM3) - YOLOv8-ORB-SLAM3: Semantic SLAM with dynamic feature point removal

*   [YWL0720/I2EKF-LO](https://github.com/YWL0720/I2EKF-LO) - \[IROS 2024] I2EKF-LO: A Dual-Iteration Extended Kalman Filter based  LiDAR Odometry

*   [arclab-hku/Event\_based\_VO-VIO-SLAM](https://github.com/arclab-hku/Event_based_VO-VIO-SLAM) - HKU-Dataset for Event-based VO/VIO/SLAM

*   [LeonardoDiCaprio1/Map\_ORBSLAM\_ROS](https://github.com/LeonardoDiCaprio1/Map_ORBSLAM_ROS) - You can densely map datasets through RVIZ and create your own TUM dataset to create maps

*   [qdLMF/VINS-Fusion-GPU-BA](https://github.com/qdLMF/VINS-Fusion-GPU-BA) - A CUDA reimplementation of Bundle Adjustment for VINS-Fusion

*   [halismai/photobundle](https://github.com/halismai/photobundle) - Photometric Bundle Adjustment for Vision-Based SLAM

*   [NKU-MobFly-Robotics/LRAE](https://github.com/NKU-MobFly-Robotics/LRAE) - LRAE: Large-Region-Aware Safe and Fast Autonomous Exploration of Ground Robots for Uneven Terrains, RA-L, 2024

*   [TUMFTM/ORB\_SLAM3\_RGBL](https://github.com/TUMFTM/ORB_SLAM3_RGBL) - RGB-L: An Extension to Integrate LiDAR Data into ORB-SLAM3

*   [ZJU-FAST-Lab/ego-planner](https://github.com/ZJU-FAST-Lab/ego-planner) -

*   [HKUST-Aerial-Robotics/FM-Fusion](https://github.com/HKUST-Aerial-Robotics/FM-Fusion) - \[RA-L] FM-Fusion: Instance-aware Semantic Mapping Boosted by Vision-Language Foundation Models

*   [GDUT-Kyle/faster\_lio\_sam](https://github.com/GDUT-Kyle/faster_lio_sam) - FASTER-LIO-SAM: A SLAM system based on iVox and GTSAM.

*   [NAIL-HNU/ESVO2](https://github.com/NAIL-HNU/ESVO2) -

*   [Taeyoung96/GRIL-Calib](https://github.com/Taeyoung96/GRIL-Calib) - \[RA-L 2024] GRIL-Calib: Targetless Ground Robot IMU-LiDAR Extrinsic Calibration Method using Ground Plane Motion Constraints

*   [Yaepiii/C-LOAM](https://github.com/Yaepiii/C-LOAM) - A Compact LiDAR Odometry and Mapping with Dynamic Removal \[ICUS 2024]

*   [Yaepiii/TRLO](https://github.com/Yaepiii/TRLO) - \[T-IM 2025] TRLO: An Efficient LiDAR Odometry with 3D Dynamic Object Tracking and Removal

*   [MrNeRF/LichtFeld-Studio](https://github.com/MrNeRF/LichtFeld-Studio) - LichtFeld Studio: Where reality and the digital world blend.

*   [xieyuser/GS-LIVM](https://github.com/xieyuser/GS-LIVM) - \[ICCV'25] GS-LIVM: Real-Time Photo-Realistic LiDAR-Inertial-Visual Mapping with Gaussian Splatting

*   [Unsigned-Long/Useful-Functions](https://github.com/Unsigned-Long/Useful-Functions) - Give it a try! Try and die!

*   [Unsigned-Long/slam-tricks](https://github.com/Unsigned-Long/slam-tricks) - small, powerful and beautiful slam tricks with theory and practice

*   [farhad-dalirani/StereoVision-SLAM](https://github.com/farhad-dalirani/StereoVision-SLAM) - StereoVision-SLAM is a real-time visual stereo SLAM (Simultaneous Localization and Mapping)

*   [gaoxiang12/faster-lio](https://github.com/gaoxiang12/faster-lio) - Faster-LIO: Lightweight Tightly Coupled Lidar-inertial Odometry using Parallel Sparse Incremental Voxels

*   [Geekgineer/CloudPeek](https://github.com/Geekgineer/CloudPeek) - CloudPeek is a lightweight, cross-platform, single-header C++ point cloud viewer. It‚Äôs designed for simplicity and efficiency, requiring no heavy libraries like PCL or Open3D. Ideal for visualizing and interacting with 3D data from LiDAR, photogrammetry, or other datasets, CloudPeek delivers powerful, real-time exploration in a minimalistic package

*   [zhuge2333/4DRadarSLAM](https://github.com/zhuge2333/4DRadarSLAM) -

*   [rubengooj/pl-slam](https://github.com/rubengooj/pl-slam) - This code contains an algorithm to compute stereo visual SLAM by using both point and line segment features.

*   [DapengFeng/cartgs](https://github.com/DapengFeng/cartgs) - \[RA-L] CaRtGS: Computational Alignment for Real-Time Gaussian Splatting SLAM

*   [hku-mars/LAMM](https://github.com/hku-mars/LAMM) -

*   [alsora/ros2-ORB\_SLAM2](https://github.com/alsora/ros2-ORB_SLAM2) - ROS2 node wrapping the ORB\_SLAM2 library

*   [GREAT-WHU/RoadLib](https://github.com/GREAT-WHU/RoadLib) - A lightweight library for instance-level visual road marking extraction, parameterization, mapping, etc.

*   [TohsakaZ/ppp\_ex](https://github.com/TohsakaZ/ppp_ex) - Prise Point Positioning Experiment

*   [hku-mars/Voxel-SLAM](https://github.com/hku-mars/Voxel-SLAM) -

*   [ChaoqinRobotics/LINS---LiDAR-inertial-SLAM](https://github.com/ChaoqinRobotics/LINS---LiDAR-inertial-SLAM) - A Lidar-Inertial State Estimator for Robust and Efficient Navigation based on iterated error-state Kalman filter

*   [udaysankar01/xfeatSLAM](https://github.com/udaysankar01/xfeatSLAM) - Real-time SLAM with deep features (XFeat + ORB-SLAM3)

*   [yanyan-li/Structure-SLAM-PointLine](https://github.com/yanyan-li/Structure-SLAM-PointLine) - This is a basic point-line SLAM system based on ORBSLAM2.

*   [APRIL-ZJU/lidar\_IMU\_calib](https://github.com/APRIL-ZJU/lidar_IMU_calib) - \[IROS 2020] Targetless Calibration of LiDAR-IMU System Based on Continuous-time Batch Estimation

*   [ashishkumar822/Jetson-SLAM](https://github.com/ashishkumar822/Jetson-SLAM) - A high Speed GPU accelerated SLAM for Low Powered Devices, IEEE- RAL-2023, ICRA 2024

*   [alejandrofontan/AnyFeature-VSLAM](https://github.com/alejandrofontan/AnyFeature-VSLAM) - Any-Feature V-SLAM is an automated visual SLAM library for Monocular cameras capable of switching to a chosen type of feature effortlessly and without manual intervention.

*   [Ji1Xingyu/SGBA](https://github.com/Ji1Xingyu/SGBA) -

*   [fishmarch/MS-SLAM](https://github.com/fishmarch/MS-SLAM) - \[JFR 2024] This is the official implementation of MS-SLAM, a memory-efficient visual SLAM system removing redundant map points to save memory consumption.

*   [2013fangwentao/Multi\_Sensor\_Fusion](https://github.com/2013fangwentao/Multi_Sensor_Fusion) - Multi-Sensor Fusion (GNSS, IMU, Camera) Â§öÊ∫êÂ§ö‰º†ÊÑüÂô®ËûçÂêàÂÆö‰Ωç GPS/INSÁªÑÂêàÂØºËà™  PPP/INSÁ¥ßÁªÑÂêà

*   [ethz-asl/kalibr](https://github.com/ethz-asl/kalibr) - The Kalibr visual-inertial calibration toolbox

*   [LuoXubo/JointLoc](https://github.com/LuoXubo/JointLoc) - \[IROS 2024] JointLoc: A Real-time Visual Localization Framework for Planetary UAVs Based on Joint Relative and Absolute Pose Estimation

*   [ethz-asl/okvis](https://github.com/ethz-asl/okvis) - OKVIS: Open Keyframe-based Visual-Inertial SLAM.

*   [jimazeyu/GraspSplats](https://github.com/jimazeyu/GraspSplats) - GraspSplats: Efficient Manipulation with 3D Feature Splatting

*   [ethz-mrl/okvis2](https://github.com/ethz-mrl/okvis2) - Open Keyframe-based Visual-Inertial SLAM (Version 2)

*   [HViktorTsoi/FAST\_LIO\_LOCALIZATION](https://github.com/HViktorTsoi/FAST_LIO_LOCALIZATION) - A simple localization framework that can re-localize in built maps based on FAST-LIO.

*   [GREAT-WHU/GREAT-PVT](https://github.com/GREAT-WHU/GREAT-PVT) - GREAT-PVT: Precision Positioning and Navigation Software by Wuhan University GREAT Group

*   [mengkai98/BA\_Play](https://github.com/mengkai98/BA_Play) - ÈöèÊâãÂÜô‰∏™BAÁé©Áé©

*   [Yixin-F/LiLoc](https://github.com/Yixin-F/LiLoc) - (ICRA 2025) LiLoc: Lifelong Localization using Adaptive Submap Joining and Egocentric Factor Graph

*   [Yixin-F/better\_fastlio2](https://github.com/Yixin-F/better_fastlio2) - Postgraduate Thesis: fast\_lio\_sam + dynamic removal (T-GRS 2024) + multi-session mapping (ICRA 2022 Kim) + object-level update + online relocalization (ICRA 2025) + real-world application (MD-LVIO)

*   [udaysankar01/xfeat\_cpp](https://github.com/udaysankar01/xfeat_cpp) - The C++ Implementation of XFeat (Accelerated Features).

*   [chengwei0427/ct-lio](https://github.com/chengwei0427/ct-lio) - CT-LIO: Continuous-Time LiDAR-Inertial Odometry

*   [felixendres/rgbdslam\_v2](https://github.com/felixendres/rgbdslam_v2) - RGB-D SLAM for ROS

*   [Eliaul/Eq-LIO](https://github.com/Eliaul/Eq-LIO) - A tightly coupled LIO framework based on the equivariant filter.

*   [HKUST-Aerial-Robotics/GVINS](https://github.com/HKUST-Aerial-Robotics/GVINS) - Tightly coupled GNSS-Visual-Inertial system for locally smooth and globally consistent state estimation in complex environment.

*   [HuajianUP/Photo-SLAM](https://github.com/HuajianUP/Photo-SLAM) - \[CVPR 2024] Photo-SLAM: Real-time Simultaneous Localization and Photorealistic Mapping for Monocular, Stereo, and RGB-D Cameras

*   [HKUST-Aerial-Robotics/EPSILON](https://github.com/HKUST-Aerial-Robotics/EPSILON) -

*   [rmsalinas/DBow3](https://github.com/rmsalinas/DBow3) - Improved version of DBow2

*   [MigVega/SLAM2REF](https://github.com/MigVega/SLAM2REF) -  This project allows the  alignment and correction of LiDAR-based SLAM session data with a reference map or another session, also the retrieval of 6-DoF poses with accuracy of up to 3 cm given an accurate TLS point cloud as a reference map (this map should be accurate at least regarding the position of permanent elements such as walls and columns).

*   [lava/matplotlib-cpp](https://github.com/lava/matplotlib-cpp) - Extremely simple yet powerful header-only C++ plotting library built on the popular matplotlib

*   [kuankuan-yue/VINS-FUSION-leanrning](https://github.com/kuankuan-yue/VINS-FUSION-leanrning) - VINS-FUSION‰∏≠ÊñáÊ≥®ÈáäÁâà.ÁõÆÂâçÁΩëÁªú‰∏äÂØπ‰∫éVINS-monoÁöÑ‰ª£Á†ÅÂ∑≤ÁªèÊúâÂæàÂ§öËÆ≤Ëß£ÂíåÊ≥®Èáä‰∫ÜÔºå‰ΩÜÊòØÂØπ‰∫éVINS-FUSIONÔºà‰ª•‰∏ãÁÆÄÁß∞VFÔºâÁöÑÊ≥®ÈáäËøòÊòØÂæàÂ∞ëÁöÑÔºåÂàöÂ•ΩÊú¨‰∫∫ÊúÄËøë‰πüÊ≠£Âú®Â≠¶‰π†VIOÁöÑÁõ∏ÂÖ≥Áü•ËØÜÔºåÊâÄ‰ª•ÂØπVFÊåâÁÖßÁ®ãÂ∫èÊâßË°åÈ°∫Â∫èËøõË°å‰∫ÜÂçÅÂàÜËØ¶ÁªÜÁöÑÊ≥®ÈáäÔºåÂêåÊó∂‰∏∫‰∫ÜÂíåÂ§ßÂÆ∂ËøõË°å‰∫§ÊµÅÂ≠¶‰π†ÔºåÊâÄ‰ª•ÊääÁõ∏ÂÖ≥Ê≥®Èáä‰ª£Á†ÅËøõË°åÂºÄÊ∫ê„ÄÇÂõ†Ê∞¥Âπ≥ÊúâÈôêÔºåÈîôËØØËÇØÂÆö‰∏çÂ∞ëÔºåËøòËØ∑ÂêÑ‰ΩçÂ§ß‰Ω¨‰ª¨ÊåáÊ≠£„ÄÇ

*   [gtrll/gpslam](https://github.com/gtrll/gpslam) - Sparse Gaussian Processes for SLAM

*   [UZ-SLAMLab/ORB\_SLAM3](https://github.com/UZ-SLAMLab/ORB_SLAM3) - ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM

*   [i3tyc/AdaptSLAM](https://github.com/i3tyc/AdaptSLAM) - AdaptSLAM: Edge-Assisted Adaptive SLAM with Resource Constraints via Uncertainty Minimization

*   [Zhefan-Xu/onboard\_detector](https://github.com/Zhefan-Xu/onboard_detector) - \[IEEE RA-L'24] Dynamic Obstacle Detection and Tracking (DODT) algorithm for Autonomous Robots (C++/ROS)

*   [nkliuhui/sync\_gps\_lidar\_imu\_cam](https://github.com/nkliuhui/sync_gps_lidar_imu_cam) - lidar-imu-cam-GPSÊó∂Èó¥Êà≥Á°¨‰ª∂ÂêåÊ≠•ÊñπÊ°à

*   [tum-vision/lsd\_slam](https://github.com/tum-vision/lsd_slam) - LSD-SLAM

*   [yutongwangBIT/GOReloc](https://github.com/yutongwangBIT/GOReloc) -

*   [HeYijia/VINS-Course](https://github.com/HeYijia/VINS-Course) - VINS-Mono code without Ceres or ROS

*   [SainingZhang/UC-GS](https://github.com/SainingZhang/UC-GS) - \[BMVC 2024] Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty

*   [AndreasArendt/OpenRTK](https://github.com/AndreasArendt/OpenRTK) - Open Source precise GNSS Software

*   [PetWorm/LARVIO](https://github.com/PetWorm/LARVIO) - A lightweight, accurate and robust monocular visual inertial odometry based on Multi-State Constraint Kalman Filter.

*   [ManiiXu/VINS-Mono-Learning](https://github.com/ManiiXu/VINS-Mono-Learning) - VINS-Mono‰ª£Á†ÅÊ≥®ÈáäÔºå‰ªÖ‰æõÂ≠¶‰π†

*   [sair-lab/AirSLAM](https://github.com/sair-lab/AirSLAM) - üöÄ AirVO upgrades to AirSLAM \[TRO]üöÄ

*   [city-super/Octree-GS](https://github.com/city-super/Octree-GS) - \[TPAMI 2025] Octree-GS: Towards Consistent Real-time Rendering with LOD-Structured 3D Gaussians

*   [CG050523/PPP-Navigation](https://github.com/CG050523/PPP-Navigation) - ‰º™Ë∑ùÂçïÁÇπÂÆö‰ΩçÁ®ãÂ∫èÂÆûÁé∞Ôºå‰ªÖÂ≠¶‰π†‰ΩøÁî®

*   [Alex-Beh/yolov5\_ros](https://github.com/Alex-Beh/yolov5_ros) -

*   [VIS4ROB-lab/ccm\_slam](https://github.com/VIS4ROB-lab/ccm_slam) - CCM-SLAM: Robust and Efficient Centralized Collaborative Monocular SLAM for Robotic Teams

*   [microsoft/Azure-Kinect-Sensor-SDK](https://github.com/microsoft/Azure-Kinect-Sensor-SDK) - A cross platform (Linux and Windows) user mode SDK to read data from your Azure Kinect device.

*   [hku-mars/FAST-LIVO2](https://github.com/hku-mars/FAST-LIVO2) - FAST-LIVO2: Fast, Direct LiDAR-Inertial-Visual Odometry

*   [yanyan-li/PlanarSLAM](https://github.com/yanyan-li/PlanarSLAM) - A RGB-D SLAM system for structural scenes, which makes use of point-line-plane features and the Manhattan World assumption.

*   [STAR-Center/VINS-RGBD](https://github.com/STAR-Center/VINS-RGBD) -

*   [TixiaoShan/LIO-SAM](https://github.com/TixiaoShan/LIO-SAM) - LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping

*   [TixiaoShan/LVI-SAM](https://github.com/TixiaoShan/LVI-SAM) - LVI-SAM: Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping

*   [danping/CoSLAM](https://github.com/danping/CoSLAM) - CoSLAM is a visual SLAM software that aims to use multiple freely moving cameras to simultaneously compute their egomotion and the 3D map of the surrounding scenes in a highly dynamic environment.

*   [google/or-tools](https://github.com/google/or-tools) - Google's Operations Research tools:

*   [JakobEngel/dso](https://github.com/JakobEngel/dso) - Direct Sparse Odometry

*   [AnswerDotAI/gpu.cpp](https://github.com/AnswerDotAI/gpu.cpp) - A lightweight library for portable low-level GPU computation using WebGPU.

*   [ethz-asl/wavemap](https://github.com/ethz-asl/wavemap) - Fast, efficient and accurate multi-resolution, multi-sensor 3D occupancy mapping

*   [RonaldSun/VI-Stereo-DSO](https://github.com/RonaldSun/VI-Stereo-DSO) - Direct sparse odometry combined with stereo cameras and IMU

*   [GREAT-WHU/iKalibr](https://github.com/GREAT-WHU/iKalibr) -

*   [MIT-SPARK/Kimera-RPGO](https://github.com/MIT-SPARK/Kimera-RPGO) - Robust Pose Graph Optimization

*   [lian-yue0515/MM-LINS](https://github.com/lian-yue0515/MM-LINS) - a Multi-Map LiDAR-Inertial System for Over-Degraded Environments

*   [engcang/vins-application](https://github.com/engcang/vins-application) - VINS-Fusion, VINS-Fisheye, OpenVINS, EnVIO, ROVIO, S-MSCKF, ORB-SLAM2, NVIDIA Elbrus application of different sets of cameras and imu on different board including desktop and Jetson boards

*   [floatlazer/semantic\_slam](https://github.com/floatlazer/semantic_slam) - Real time semantic slam in ROS with a hand held RGB-D camera

*   [koide3/gtsam\_points](https://github.com/koide3/gtsam_points) - A collection of GTSAM factors and optimizers for point cloud SLAM

*   [HKUST-SAIL/RaDe-GS](https://github.com/HKUST-SAIL/RaDe-GS) - RaDe-GS: Rasterizing Depth in Gaussian Splatting

*   [Unsigned-Long/iKalibr](https://github.com/Unsigned-Long/iKalibr) - \[IEEE T-RO 2025] iKalibr: Multi-Sensor Calibration (Extrinsics & Time Offsets)

*   [emiliofidalgo/ibow-lcd](https://github.com/emiliofidalgo/ibow-lcd) - Appearance-based Loop Closure Detection using Incremental Bags of Binary Words

*   [bxh1/VIDO-SLAM](https://github.com/bxh1/VIDO-SLAM) - VIDO-SLAM is a  Visual Inertial SLAM system for dynamic environments, and it can  also estimate dynamic objects motion and track objects.

*   [url-kaist/dynaVINS](https://github.com/url-kaist/dynaVINS) - DynaVINS : A Visual-Inertial SLAM for Dynamic Environments

*   [MAVIS-SLAM/OpenMAVIS](https://github.com/MAVIS-SLAM/OpenMAVIS) - An open-source implementation of MAVIS-SLAM.

*   [linyicheng1/OpenSLAM-Notes](https://github.com/linyicheng1/OpenSLAM-Notes) - ‰∏™‰∫∫ÂØπÁõÆÂâçËæÉ‰∏∫ÊàêÁÜüÁöÑËßÜËßâ/ÊøÄÂÖâSLAMÁÆóÊ≥ïËøõË°åÁöÑÊ≥®Èáä‰ª•ÂèäËß£ËØªÊñá‰ª∂

*   [guisoares9/VINS-Fusion](https://github.com/guisoares9/VINS-Fusion) - OpenCV 4, ROS Noetic, and Ceres adaptation of VINS-Fusion. An optimization-based multi-sensor state estimator

*   [cyp4x141/VINS-Fusion-noetic-Opencv4](https://github.com/cyp4x141/VINS-Fusion-noetic-Opencv4) - VINS-Fusion for opencv4 + noetic +ubuntu20.04

*   [shanpenghui/ORB\_SLAM3\_Fixed](https://github.com/shanpenghui/ORB_SLAM3_Fixed) - Optimized ORBSLAM3 to run on TUM/EuRoc/KITTI dataset

*   [karanchawla/GPS\_IMU\_Kalman\_Filter](https://github.com/karanchawla/GPS_IMU_Kalman_Filter) - Fusing GPS, IMU and Encoder sensors for accurate state estimation.

*   [yuefanhao/SuperPoint-SuperGlue-TensorRT](https://github.com/yuefanhao/SuperPoint-SuperGlue-TensorRT) - SuperPoint and SuperGlue with TensorRT. Deploy with C++.

*   [kajo-kurisu/D\_VINS](https://github.com/kajo-kurisu/D_VINS) - Merge superpoint„ÄÅlightglue„ÄÅMixVPR into VINS-FUSION for loop closure with TensorRT

*   [HeYijia/PL-VIO](https://github.com/HeYijia/PL-VIO) - monocular visual inertial system with point and line features

*   [openxrlab/xrslam](https://github.com/openxrlab/xrslam) - OpenXRLab Visual-inertial SLAM Toolbox and Benchmark

*   [KumarRobotics/msckf\_vio](https://github.com/KumarRobotics/msckf_vio) - Robust Stereo Visual Inertial Odometry for Fast Autonomous Flight

*   [i2Nav-WHU/IC-GVINS](https://github.com/i2Nav-WHU/IC-GVINS) - A Robust, Real-time, INS-Centric GNSS-Visual-Inertial Navigation System

*   [ydsf16/imu\_gps\_localization](https://github.com/ydsf16/imu_gps_localization) - Using error-state Kalman filter to fuse the IMU and GPS data for localization.

*   [cnqiangfu/PL-VINS](https://github.com/cnqiangfu/PL-VINS) - PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line Features

*   [microsoft/onnxruntime](https://github.com/microsoft/onnxruntime) - ONNX Runtime: cross-platform, high performance ML inferencing and training accelerator

*   [HKUST-Aerial-Robotics/VINS-Mono](https://github.com/HKUST-Aerial-Robotics/VINS-Mono) - A Robust and Versatile Monocular Visual-Inertial State Estimator

*   [ethz-asl/ethzasl\_msf](https://github.com/ethz-asl/ethzasl_msf) - MSF - Modular framework for multi sensor fusion based on an Extended Kalman Filter (EKF)

*   [zm0612/eskf-gps-imu-fusion](https://github.com/zm0612/eskf-gps-imu-fusion) - ËØØÂ∑ÆÁä∂ÊÄÅÂç°Â∞îÊõºESKFÊª§Ê≥¢Âô®ËûçÂêàGPSÂíåIMUÔºåÂÆûÁé∞Êõ¥È´òÁ≤æÂ∫¶ÁöÑÂÆö‰Ωç

*   [Ewenwan/MVision](https://github.com/Ewenwan/MVision) - Êú∫Âô®‰∫∫ËßÜËßâ ÁßªÂä®Êú∫Âô®‰∫∫ VS-SLAM ORB-SLAM2 Ê∑±Â∫¶Â≠¶‰π†ÁõÆÊ†áÊ£ÄÊµã yolov3 Ë°å‰∏∫Ê£ÄÊµã opencv  PCL Êú∫Âô®Â≠¶‰π† Êó†‰∫∫È©æÈ©∂

*   [raulmur/ORB\_SLAM2](https://github.com/raulmur/ORB_SLAM2) - Real-Time SLAM for Monocular, Stereo and RGB-D Cameras, with Loop Detection and Relocalization Capabilities

*   [ivipsourcecode/DS-SLAM](https://github.com/ivipsourcecode/DS-SLAM) -

*   [ceres-solver/ceres-solver](https://github.com/ceres-solver/ceres-solver) - A large scale non-linear optimization library

*   [shazraz/Extended-Kalman-Filter](https://github.com/shazraz/Extended-Kalman-Filter) - Implementation of an EKF in C++

*   [rpng/open\_vins](https://github.com/rpng/open_vins) - An open source platform for visual-inertial navigation research.

*   [Ewenwan/ORB\_SLAM2\_SSD\_Semantic](https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic) - Âä®ÊÄÅËØ≠‰πâSLAM ÁõÆÊ†áÊ£ÄÊµã+VSLAM+ÂÖâÊµÅ/Â§öËßÜËßíÂá†‰ΩïÂä®ÊÄÅÁâ©‰ΩìÊ£ÄÊµã+octomapÂú∞Âõæ+ÁõÆÊ†áÊï∞ÊçÆÂ∫ì

*   [KennyWGH/VINS-Fusion-Understood](https://github.com/KennyWGH/VINS-Fusion-Understood) - ÂÆåÂÖ®ÂèØÁêÜËß£ÁöÑVINS-FusionÔºö1.‰ª£Á†ÅÈ£éÊ†ºÈáçÊûÑ„ÄÅ2.ÂÖ®ÈáèÈù†Ë∞±Ê≥®Èáä„ÄÅ3.‰ª£Á†ÅÂç≥ÊñáÊ°£„ÄÅ4.Âø†ÂÆû‰∫éÂéü‰ª£Á†ÅÔºõ5.ROSËß£ËÄ¶„ÄÅ6.Áä∂ÊÄÅÈáèÂèØËßÜÂåñ„ÄÅ7.Êó•ÂøóÁ≥ªÁªü„ÄÇ

*   [hmartiro/kalman-cpp](https://github.com/hmartiro/kalman-cpp) - Basic Kalman filter implementation in C++ using Eigen

*   [HITSZ-NRSL/Dynamic-VINS](https://github.com/HITSZ-NRSL/Dynamic-VINS) - \[RA-L 2022] RGB-D Inertial Odometry for a Resource-restricted Robot in Dynamic Environments

*   [i2Nav-WHU/OB\_GINS](https://github.com/i2Nav-WHU/OB_GINS) - An Optimization-Based GNSS/INS Integrated Navigation System

*   [i2Nav-WHU/KF-GINS](https://github.com/i2Nav-WHU/KF-GINS) - An EKF-Based GNSS/INS Integrated Navigation System

*   [mherb/kalman](https://github.com/mherb/kalman) - Header-only C++11 Kalman Filtering Library (EKF, UKF) based on Eigen3

*   [bytedance/SchurVINS](https://github.com/bytedance/SchurVINS) - \[CVPR2024] SchurVINS: Schur Complement-Based Lightweight Visual Inertial Navigation System

*   [halajun/VDO\_SLAM](https://github.com/halajun/VDO_SLAM) - VDO-SLAM: A Visual Dynamic Object-aware SLAM System

*   [Unsigned-Long/RIs-Calib](https://github.com/Unsigned-Long/RIs-Calib) - \[IEEE TIM 2025] a continuous-time-based multi-radar multi-imu spatiotemporal calibrator

*   [Lab-of-AI-and-Robotics/GS\_ICP\_SLAM](https://github.com/Lab-of-AI-and-Robotics/GS_ICP_SLAM) - \[ECCV 2024] RGBD GS-ICP SLAM

## Jupyter Notebook

*   [Infrasys-AI/AISystem](https://github.com/Infrasys-AI/AISystem) - AISystem ‰∏ªË¶ÅÊòØÊåáAIÁ≥ªÁªüÔºåÂåÖÊã¨AIËäØÁâá„ÄÅAIÁºñËØëÂô®„ÄÅAIÊé®ÁêÜÂíåËÆ≠ÁªÉÊ°ÜÊû∂Á≠âAIÂÖ®Ê†àÂ∫ïÂ±ÇÊäÄÊúØ

*   [leggedrobotics/navitrace\_evaluation](https://github.com/leggedrobotics/navitrace_evaluation) -

*   [qiuzh20/gated\_attention](https://github.com/qiuzh20/gated_attention) - The official implementation for \[NeurIPS2025 Oral] Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free

*   [QwenLM/Qwen3-Omni](https://github.com/QwenLM/Qwen3-Omni) - Qwen3-omni is a natively end-to-end, omni-modal LLM developed by the Qwen team at Alibaba Cloud, capable of understanding text, audio, images, and video, as well as generating speech in real time.

*   [QwenLM/Qwen2.5-Omni](https://github.com/QwenLM/Qwen2.5-Omni) - Qwen2.5-Omni is an end-to-end multimodal model by Qwen team at Alibaba Cloud, capable of understanding text, audio, vision, video, and performing real-time speech generation.

*   [IDEA-Research/Grounded-SAM-2](https://github.com/IDEA-Research/Grounded-SAM-2) - Grounded SAM 2: Ground and Track Anything in Videos with Grounding DINO, Florence-2 and SAM 2

*   [nv-tlabs/GEN3C](https://github.com/nv-tlabs/GEN3C) - \[CVPR 2025 Highlight] GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control

*   [LaVi-Lab/VG-LLM](https://github.com/LaVi-Lab/VG-LLM) - The code for paper 'Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors'

*   [facebookresearch/dinov3](https://github.com/facebookresearch/dinov3) - Reference PyTorch implementation and models for DINOv3

*   [facebookresearch/dinov2](https://github.com/facebookresearch/dinov2) - PyTorch code and models for the DINOv2 self-supervised learning method.

*   [InternRobotics/InternNav](https://github.com/InternRobotics/InternNav) - InternRobotics' open platform for building generalized navigation foundation models.

*   [HeegerGao/FLIP](https://github.com/HeegerGao/FLIP) - Code for FLIP: Flow-Centric Generative Planning for General-Purpose Manipulation Tasks

*   [datawhalechina/easy-rl](https://github.com/datawhalechina/easy-rl) - Âº∫ÂåñÂ≠¶‰π†‰∏≠ÊñáÊïôÁ®ãÔºàËòëËèá‰π¶üçÑÔºâÔºåÂú®Á∫øÈòÖËØªÂú∞ÂùÄÔºöhttps://datawhalechina.github.io/easy-rl/

*   [RL4VLM/RL4VLM](https://github.com/RL4VLM/RL4VLM) - Official Repo for Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning

*   [ByteDance-Seed/Seed1.5-VL](https://github.com/ByteDance-Seed/Seed1.5-VL) - Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning, achieving state-of-the-art performance on 38 out of 60 public benchmarks.

*   [Robotics-STAR-Lab/DynamicPose](https://github.com/Robotics-STAR-Lab/DynamicPose) - \[IROS 2025] DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects

*   [microsoft/ai-agents-for-beginners](https://github.com/microsoft/ai-agents-for-beginners) - 12 Lessons to Get Started Building AI Agents

*   [NVIDIA/Isaac-GR00T](https://github.com/NVIDIA/Isaac-GR00T) - NVIDIA Isaac GR00T N1.6 -  A Foundation Model for Generalist Robots.

*   [google-gemini/gemini-fullstack-langgraph-quickstart](https://github.com/google-gemini/gemini-fullstack-langgraph-quickstart) - Get started with building Fullstack Agents using Gemini 2.5 and LangGraph

*   [datawhalechina/happy-llm](https://github.com/datawhalechina/happy-llm) - üìö ‰ªéÈõ∂ÂºÄÂßãÁöÑÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã

*   [Liuziyu77/Visual-RFT](https://github.com/Liuziyu77/Visual-RFT) - Official repository of 'Visual-RFT: Visual Reinforcement Fine-Tuning' & 'Visual-ARFT: Visual Agentic Reinforcement Fine-Tuning'‚Äô

*   [bagh2178/SG-Nav](https://github.com/bagh2178/SG-Nav) - \[NeurIPS 2024] SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation

*   [facebookresearch/EdgeTAM](https://github.com/facebookresearch/EdgeTAM) - \[CVPR 2025] Official PyTorch implementation of "EdgeTAM: On-Device Track Anything Model"

*   [zhanshijinwat/Steel-LLM](https://github.com/zhanshijinwat/Steel-LLM) - Train a 1B LLM with 1T tokens from scratch by personal

*   [facebookresearch/co-tracker](https://github.com/facebookresearch/co-tracker) - CoTracker is a model for tracking any point (pixel) on a video.

*   [arclab-hku/DEIO](https://github.com/arclab-hku/DEIO) - (ICCVW2025) Learning-based Event-Inertial Odometry

*   [mit-acl/dynus](https://github.com/mit-acl/dynus) -

*   [IDEA-Research/Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything) - Grounded SAM: Marrying Grounding DINO with Segment Anything & Stable Diffusion & Recognize Anything - Automatically Detect , Segment and Generate Anything

*   [facebookresearch/segment-anything](https://github.com/facebookresearch/segment-anything) - The repository provides code for running inference with the SegmentAnything Model (SAM), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.

*   [xinyu1205/recognize-anything](https://github.com/xinyu1205/recognize-anything) - Open-source and strong foundation image recognition models.

*   [CompVis/depth-fm](https://github.com/CompVis/depth-fm) - \[AAAI 2025, Oral] DepthFM: Fast Monocular Depth Estimation with Flow Matching

*   [luhengshiwo/LLMForEverybody](https://github.com/luhengshiwo/LLMForEverybody) - ÊØè‰∏™‰∫∫ÈÉΩËÉΩÁúãÊáÇÁöÑÂ§ßÊ®°ÂûãÁü•ËØÜÂàÜ‰∫´ÔºåLLMsÊò•/ÁßãÊãõÂ§ßÊ®°ÂûãÈù¢ËØïÂâçÂøÖÁúãÔºåËÆ©‰Ω†ÂíåÈù¢ËØïÂÆò‰æÉ‰æÉËÄåË∞à

*   [luohongk/Embodied-Navigation](https://github.com/luohongk/Embodied-Navigation) - ÂÖ≥‰∫éEmbodied-NavigationÁöÑ‰ªìÂ∫ìÔºå‰∏ªË¶ÅÁî®‰∫éÊï¥ÁêÜÊàëÂú®ÂÆö‰ΩçÔºåÊÑüÁü•ÔºåËßÑÊéßÔºå3D Vision, VLN‰∏≠ÁöÑÈÉ®ÂàÜÁü•ËØÜ

*   [GAP-LAB-CUHK-SZ/gaustudio](https://github.com/GAP-LAB-CUHK-SZ/gaustudio) - A Modular Framework for 3D Gaussian Splatting and Beyond

*   [HCPLab-SYSU/LH-VLN](https://github.com/HCPLab-SYSU/LH-VLN) - Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method (CVPR-25)

*   [HandsOnLLM/Hands-On-Large-Language-Models](https://github.com/HandsOnLLM/Hands-On-Large-Language-Models) - Official code repo for the O'Reilly Book - "Hands-On Large Language Models"

*   [CurryYuan/ZSVG3D](https://github.com/CurryYuan/ZSVG3D) - \[CVPR 2024] Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding

*   [datawhalechina/tiny-universe](https://github.com/datawhalechina/tiny-universe) - „ÄäÂ§ßÊ®°ÂûãÁôΩÁõíÂ≠êÊûÑÂª∫ÊåáÂçó„ÄãÔºö‰∏Ä‰∏™ÂÖ®ÊâãÊêìÁöÑTiny-Universe

*   [robot-pesg/BotanicGarden](https://github.com/robot-pesg/BotanicGarden) - BotanicGarden: A high-quality dataset for robot navigation in unstructured natural environments

*   [QwenLM/Qwen3-VL](https://github.com/QwenLM/Qwen3-VL) - Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.

*   [DjangoPeng/LLM-quickstart](https://github.com/DjangoPeng/LLM-quickstart) - Quick Start for Large Language Models (Theoretical Learning and Practical Fine-tuning) Â§ßËØ≠Ë®ÄÊ®°ÂûãÂø´ÈÄüÂÖ•Èó®ÔºàÁêÜËÆ∫Â≠¶‰π†‰∏éÂæÆË∞ÉÂÆûÊàòÔºâ

*   [zju3dv/LoFTR](https://github.com/zju3dv/LoFTR) - Code for "LoFTR: Detector-Free Local Feature Matching with Transformers", CVPR 2021, T-PAMI 2022

*   [hesamsheikh/ml-retreat](https://github.com/hesamsheikh/ml-retreat) - Machine Learning Journal for Intermediate to Advanced Topics.

*   [DataExpert-io/data-engineer-handbook](https://github.com/DataExpert-io/data-engineer-handbook) - This is a repo with links to everything you'd ever want to learn about data engineering

*   [florinshen/FlashSplat](https://github.com/florinshen/FlashSplat) - \[ECCV2024] \[3DV Nectar 2025] FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally

*   [yzslab/gaussian-splatting-lightning](https://github.com/yzslab/gaussian-splatting-lightning) - A 3D Gaussian Splatting framework with various derived algorithms and an interactive web viewer

*   [CyberOrigin2077/Cyber](https://github.com/CyberOrigin2077/Cyber) - This repo is designed for General Robotic Operation System

*   [Tencent-Hunyuan/HunyuanDiT](https://github.com/Tencent-Hunyuan/HunyuanDiT) - Hunyuan-DiT : A Powerful Multi-Resolution Diffusion Transformer with Fine-Grained Chinese Understanding

*   [microsoft/OmniParser](https://github.com/microsoft/OmniParser) - A simple screen parsing tool towards pure vision based GUI agent

*   [TommyZihao/vlm\_arm](https://github.com/TommyZihao/vlm_arm) - Êú∫Ê¢∞ËáÇ+Â§ßÊ®°Âûã+Â§öÊ®°ÊÄÅ=‰∫∫Êú∫Âçè‰ΩúÂÖ∑Ë∫´Êô∫ËÉΩ‰Ωì

*   [cumtcssuld/RSP\_of\_CUMTCS](https://github.com/cumtcssuld/RSP_of_CUMTCS) - „ÄêÁüøÂ§ßËÆ°ÁÆóÊú∫Â≠¶Èô¢ËµÑÊ∫êÂÖ±‰∫´ËÆ°ÂàíÔºàResource SharingPlan of CUMTCSÔºâ„ÄëÊú¨‰ªìÂ∫ìÁî±ÁüøÂ§ßËÆ°ÁÆóÊú∫Â≠¶Èô¢Â≠¶Áîü‰ºöÂ≠¶‰π†ÈÉ®ÁâµÂ§¥Áª¥Êä§ÔºåÁî±ËÆ°ÁÆóÊú∫Â≠¶Èô¢ÂÖ®‰ΩìÂêåÂ≠¶ÂÖ±Âª∫ÂÖ±‰∫´„ÄÇÊ¨¢ËøéÂ§ßÂÆ∂ÁßØÊûÅÁöÑÂèÇÂä†Âà∞Êú¨ËµÑÊ∫êÂ∫ìÁöÑÂª∫ËÆæ‰∏≠Êù•ÂêßÔºÅÔºàÊØèÂΩìÊúâÈáçÂ§ßÊõ¥Êñ∞ÔºåÊàë‰ª¨ÈÉΩ‰ºöÂ∞ÜÊï¥‰∏™Â∫ìÂÖãÈöÜÂà∞Á†Å‰∫ëÔºåÁÇπÂáª‰∏ãËæπÈìæÊé•ÔºåÂà∞Êàë‰ª¨ÁöÑÁ†Å‰∫ë‰ªìÂ∫ìÂèØ‰ª•Ëé∑ÂæóÊõ¥Â•ΩÁöÑ‰∏ãËΩΩ‰ΩìÈ™åÔºâ

*   [ut-amrl/ObVi-SLAM](https://github.com/ut-amrl/ObVi-SLAM) - Long-Term Object Visual SLAM

*   [Infrasys-AI/AIInfra](https://github.com/Infrasys-AI/AIInfra) - AIInfraÔºàAI Âü∫Á°ÄËÆæÊñΩÔºâÊåáAIÁ≥ªÁªü‰ªéÂ∫ïÂ±ÇËäØÁâáÁ≠âÁ°¨‰ª∂ÔºåÂà∞‰∏äÂ±ÇËΩØ‰ª∂Ê†àÊîØÊåÅAIÂ§ßÊ®°ÂûãËÆ≠ÁªÉÂíåÊé®ÁêÜ„ÄÇ

*   [CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) - A latent text-to-image diffusion model

*   [AnyLoc/Revisit-Anything](https://github.com/AnyLoc/Revisit-Anything) - Code release for Revisit Anything: Visual Place Recognition via Image Segment Retrieval (ECCV 2024)

*   [haksorus/gsplatloc](https://github.com/haksorus/gsplatloc) - \[IROS 2025] GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for Improved Visual Localization

*   [TommyZihao/Train\_Custom\_Dataset](https://github.com/TommyZihao/Train_Custom_Dataset) - Ê†áÊ≥®Ëá™Â∑±ÁöÑÊï∞ÊçÆÈõÜÔºåËÆ≠ÁªÉ„ÄÅËØÑ‰º∞„ÄÅÊµãËØï„ÄÅÈÉ®ÁΩ≤Ëá™Â∑±ÁöÑ‰∫∫Â∑•Êô∫ËÉΩÁÆóÊ≥ï

*   [isl-org/ZoeDepth](https://github.com/isl-org/ZoeDepth) - Metric depth estimation from a single image

*   [datawhalechina/leedl-tutorial](https://github.com/datawhalechina/leedl-tutorial) - „ÄäÊùéÂÆèÊØÖÊ∑±Â∫¶Â≠¶‰π†ÊïôÁ®ã„ÄãÔºàÊùéÂÆèÊØÖËÄÅÂ∏àÊé®ËçêüëçÔºåËãπÊûú‰π¶üçéÔºâÔºåPDF‰∏ãËΩΩÂú∞ÂùÄÔºöhttps://github.com/datawhalechina/leedl-tutorial/releases

*   [Fafa-DL/Lhy\_Machine\_Learning](https://github.com/Fafa-DL/Lhy_Machine_Learning) - ÊùéÂÆèÊØÖ2021/2022/2023Êò•Â≠£Êú∫Âô®Â≠¶‰π†ËØæÁ®ãËØæ‰ª∂Âèä‰Ωú‰∏ö

*   [yubaoliu/RDS-SLAM](https://github.com/yubaoliu/RDS-SLAM) - DS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods

*   [SakanaAI/AI-Scientist](https://github.com/SakanaAI/AI-Scientist) - The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery üßë‚Äçüî¨

*   [datawhalechina/self-llm](https://github.com/datawhalechina/self-llm) - „ÄäÂºÄÊ∫êÂ§ßÊ®°ÂûãÈ£üÁî®ÊåáÂçó„ÄãÈíàÂØπ‰∏≠ÂõΩÂÆùÂÆùÈáèË∫´ÊâìÈÄ†ÁöÑÂü∫‰∫éLinuxÁéØÂ¢ÉÂø´ÈÄüÂæÆË∞ÉÔºàÂÖ®ÂèÇÊï∞/LoraÔºâ„ÄÅÈÉ®ÁΩ≤ÂõΩÂÜÖÂ§ñÂºÄÊ∫êÂ§ßÊ®°ÂûãÔºàLLMÔºâ/Â§öÊ®°ÊÄÅÂ§ßÊ®°ÂûãÔºàMLLMÔºâÊïôÁ®ã

*   [facebookresearch/sam2](https://github.com/facebookresearch/sam2) - The repository provides code for running inference with the Meta Segment Anything Model 2 (SAM 2), links for downloading the trained model checkpoints, and example notebooks that show how to use the model.

*   [hustvl/4DGaussians](https://github.com/hustvl/4DGaussians) - \[CVPR 2024] 4D Gaussian Splatting for Real-Time Dynamic Scene Rendering

*   [heucoder/ML-DL\_book](https://github.com/heucoder/ML-DL_book) - Êú∫Âô®Â≠¶‰π†„ÄÅÊ∑±Â∫¶Â≠¶‰π†‰∏Ä‰∫õ‰∏™‰∫∫ËÆ§‰∏∫‰∏çÈîôÁöÑ‰π¶Á±ç„ÄÇ

*   [verlab/accelerated\_features](https://github.com/verlab/accelerated_features) - Implementation of XFeat (CVPR 2024). Do you need robust and fast local feature extraction? You are in the right place!

*   [rpautrat/SuperPoint](https://github.com/rpautrat/SuperPoint) - Efficient neural feature detector and descriptor

*   [rlabbe/Kalman-and-Bayesian-Filters-in-Python](https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python) - Kalman Filter book using Jupyter Notebook. Focuses on building intuition and experience, not formal proofs.  Includes Kalman filters,extended Kalman filters, unscented Kalman filters, particle filters, and more. All exercises include solutions.

*   [microsoft/generative-ai-for-beginners](https://github.com/microsoft/generative-ai-for-beginners) - 21 Lessons, Get Started Building with Generative AI

*   [Lordog/dive-into-llms](https://github.com/Lordog/dive-into-llms) - „ÄäÂä®ÊâãÂ≠¶Â§ßÊ®°ÂûãDive into LLMs„ÄãÁ≥ªÂàóÁºñÁ®ãÂÆûË∑µÊïôÁ®ã

## miscellaneous

*   [curionox/lifekline](https://github.com/curionox/lifekline) - ‰∫∫ÁîüKÁ∫ø - Âü∫‰∫éAIÁöÑÂÖ´Â≠óÂëΩÁêÜÂèØËßÜÂåñÂ∑•ÂÖ∑

*   [OpenDriveLab/WholebodyVLA](https://github.com/OpenDriveLab/WholebodyVLA) - Towards Unified Latent VLA for Whole-body Loco-manipulation Control

*   [thomaschabal/fom-nav](https://github.com/thomaschabal/fom-nav) - Official implementation of "FOM-Nav: Frontier-Object Maps for Object Goal Navigation". Code release expected in December 2025.

*   [DennisRotondi/awesome-3D-scene-graphs](https://github.com/DennisRotondi/awesome-3D-scene-graphs) - Awesome 3D Scene Graphs: a curated list of 3D scene graph generation and related resources!

*   [AMAP-EAI/SocialNav](https://github.com/AMAP-EAI/SocialNav) - Official implementation for "SocialNav: Training Human-Inspired Foundation Model for Socially-Aware Embodied Navigation"

*   [HorizonRobotics/HoloAgent](https://github.com/HorizonRobotics/HoloAgent) - A unified agent system for general-purpose robots.

*   [NVlabs/LocateAnything3D](https://github.com/NVlabs/LocateAnything3D) -

*   [jc-bao/awesome-mujoco](https://github.com/jc-bao/awesome-mujoco) - A collection of awesome projects using MuJoCo.

*   [Ma-Zhuang/OmniNWM](https://github.com/Ma-Zhuang/OmniNWM) - OmniNWM: Omniscient Navigation World Models for Autonomous Driving

*   [yukangcao/Awesome-4D-Spatial-Intelligence](https://github.com/yukangcao/Awesome-4D-Spatial-Intelligence) - A curated list of awesome papers for reconstructing 4D spatial intelligence from video. (arXiv 2507.21045)

*   [0311lzy/PVSet\_data](https://github.com/0311lzy/PVSet_data) - This is a 10-meter resolution photovoltaic power station distribution map extracted using the SolarSegNet model, integrating Sentinel-1 and Sentinel-2 imagery. The coverage area includes 14 coastal provincial-level administrative regions and special administrative regions of China in 2024.

*   [hulxgit/EC3R-SLAM](https://github.com/hulxgit/EC3R-SLAM) -

*   [BaiShuanghao/Awesome-Robotics-Manipulation](https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation) - A comprehensive list of papers about Robot Manipulation, including papers, codes, and related websites.

*   [YanjieZe/awesome-humanoid-robot-learning](https://github.com/YanjieZe/awesome-humanoid-robot-learning) - A Paper List for Humanoid Robot Learning.

*   [jiangranlv/embodied-ai-start](https://github.com/jiangranlv/embodied-ai-start) - \[PKU EPIC Lab] Èù¢ÂêëÂ∞èÁôΩÁöÑÂÖ∑Ë∫´Êô∫ËÉΩÂÖ•Èó®ÊåáÂçó

*   [Tsinghua-MARS-Lab/SLAM-Former](https://github.com/Tsinghua-MARS-Lab/SLAM-Former) - SLAM-Former: Putting SLAM into One Transformer

*   [bagh2178/GC-VLN](https://github.com/bagh2178/GC-VLN) - \[CoRL 2025] GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation

*   [KwanWaiPang/Awesome-Transformer-based-SLAM](https://github.com/KwanWaiPang/Awesome-Transformer-based-SLAM) - Paper Survey for Transformer-based SLAM

*   [KwanWaiPang/Awesome-VLN](https://github.com/KwanWaiPang/Awesome-VLN) - Paper Survey for Visual Language Navigation

*   [wengminghe/Dynamic-DINO](https://github.com/wengminghe/Dynamic-DINO) - \[ICCV 2025] Official implementation of the paper: "Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection"

*   [GWxuan/IGL-Nav](https://github.com/GWxuan/IGL-Nav) - \[ICCV 2025] IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation

*   [manycore-research/InteriorGS](https://github.com/manycore-research/InteriorGS) - InteriorGS: 3D Gaussian Splatting Dataset of Semantically Labeled Indoor Scenes

*   [MoonshotAI/Kimi-K2](https://github.com/MoonshotAI/Kimi-K2) - Kimi K2 is the large language model series developed by Moonshot AI team

*   [knemik97/Manifesto-against-the-Plagiarist-Yunhe-Wang](https://github.com/knemik97/Manifesto-against-the-Plagiarist-Yunhe-Wang) - ËÆ®Ë¥ºÁéã‰∫ëÈπ§Ê™ÑÊñá

*   [LiteReality/LiteReality](https://github.com/LiteReality/LiteReality) - \[NeurIPS 2025] LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans

*   [Franky-X/TON-VIO](https://github.com/Franky-X/TON-VIO) -

*   [HW-whistleblower/True-Story-of-Pangu](https://github.com/HW-whistleblower/True-Story-of-Pangu) - ËØ∫‰∫öÁõòÂè§Â§ßÊ®°ÂûãÁ†îÂèëËÉåÂêéÁöÑÁúüÊ≠£ÁöÑÂøÉÈÖ∏‰∏éÈªëÊöóÁöÑÊïÖ‰∫ã„ÄÇ

*   [zijie0/HumanSystemOptimization](https://github.com/zijie0/HumanSystemOptimization) - ÂÅ•Â∫∑Â≠¶‰π†Âà∞150Â≤Å - ‰∫∫‰ΩìÁ≥ªÁªüË∞É‰ºò‰∏çÂÆåÂÖ®ÊåáÂçó

*   [yuanpengtu/PlayerOne](https://github.com/yuanpengtu/PlayerOne) - PlayerOne: Egocentric World Simulator

*   [TurtleZhong/Map-based-Visual-Localization](https://github.com/TurtleZhong/Map-based-Visual-Localization) - A general framework for map-based visual localization. It contains 1) Map Generation which support traditional features or deeplearning features. 2) Hierarchical-Localizationvisual in visual(points or line) map. 3)Fusion framework with IMU, wheel odom and GPS sensors.

*   [ZJU-LLMs/Foundations-of-LLMs](https://github.com/ZJU-LLMs/Foundations-of-LLMs) - A book for Learning the Foundations of LLMs

*   [buaa-colalab/OctoNav-R1](https://github.com/buaa-colalab/OctoNav-R1) - Code for OctoNav-R1

*   [PRBonn/2DGS-SLAM](https://github.com/PRBonn/2DGS-SLAM) - 2DGS-SLAM: Globally Consistent RGB-D SLAM with 2D Gaussian Splatting

*   [DEEP-PolyU/Awesome-GraphRAG](https://github.com/DEEP-PolyU/Awesome-GraphRAG) - Awesome-GraphRAG: A curated list of resources (surveys, papers, benchmarks, and opensource projects) on graph-based retrieval-augmented generation.

*   [Xnhyacinth/Awesome-LLM-Long-Context-Modeling](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling) - üì∞ Must-read papers and blogs on LLM based Long Context Modeling üî•

*   [Titrom025/LEG-SLAM](https://github.com/Titrom025/LEG-SLAM) -

*   [cchester25/FAST\_LIVO2\_Noted](https://github.com/cchester25/FAST_LIVO2_Noted) - ‰ªéÂ∞èÁôΩÁöÑËßÜËßíÂéªÂàÜÊûêÂ§öÊ∫êËûçÂêàSLAMÁöÑSOTAÊ°ÜÊû∂

*   [KalyanKS-NLP/llm-engineer-toolkit](https://github.com/KalyanKS-NLP/llm-engineer-toolkit) - A curated list of  120+ LLM libraries category wise.

*   [MoonshotAI/Kimi-VL](https://github.com/MoonshotAI/Kimi-VL) - Kimi-VL: Mixture-of-Experts Vision-Language Model for Multimodal Reasoning, Long-Context Understanding, and Strong Agent Capabilities

*   [KwanWaiPang/Awesome-Learning-based-VO-VIO](https://github.com/KwanWaiPang/Awesome-Learning-based-VO-VIO) - Paper Survey for Learning-based Odometry

*   [mli/paper-reading](https://github.com/mli/paper-reading) - Ê∑±Â∫¶Â≠¶‰π†ÁªèÂÖ∏„ÄÅÊñ∞ËÆ∫ÊñáÈÄêÊÆµÁ≤æËØª

*   [formulahendry/955.WLB](https://github.com/formulahendry/955.WLB) - 955 ‰∏çÂä†Áè≠ÁöÑÂÖ¨Âè∏ÂêçÂçï - Â∑•‰Ωú 955Ôºåwork‚Äìlife balance (Â∑•‰Ωú‰∏éÁîüÊ¥ªÁöÑÂπ≥Ë°°)

*   [iminolee/Awesome-Vision-and-Language-Navigation](https://github.com/iminolee/Awesome-Vision-and-Language-Navigation) - A curated list of awesome Vision-and-Language Navigation(VLN) resources (continually updated)

*   [zhangyuejoslin/VLN-Survey-with-Foundation-Models](https://github.com/zhangyuejoslin/VLN-Survey-with-Foundation-Models) - \[TMLR 2024] repository for VLN with foundation models

*   [TJU-Aerial-Robotics/YOPO-Tracker](https://github.com/TJU-Aerial-Robotics/YOPO-Tracker) - An End-to-End Agile Tracking and Navigation Method for UAVs

*   [matterport/habitat-matterport-3dresearch](https://github.com/matterport/habitat-matterport-3dresearch) -

*   [DoongLi/ICRA2025-Paper-List](https://github.com/DoongLi/ICRA2025-Paper-List) - ICRA2025 Paper List

*   [LinusNEP/EnvoDat](https://github.com/LinusNEP/EnvoDat) - EnvoDat: A Large-Scale Multisensory Dataset for Robotic Spatial  Awareness and Semantic Reasoning in Heterogeneous Environments

*   [LiHaoy-ux/MLINE-VINS](https://github.com/LiHaoy-ux/MLINE-VINS) -

*   [Songwxuan/Embodied-AI-Paper-TopConf](https://github.com/Songwxuan/Embodied-AI-Paper-TopConf) - \[Actively Maintainedüî•] A list of Embodied AI papers accepted by top conferences (ICLR, NeurIPS, ICML, RSS, CoRL, ICRA, IROS, CVPR, ICCV, ECCV).

*   [iceberg1369/PPPLib\_v2.0](https://github.com/iceberg1369/PPPLib_v2.0) -

*   [jonyzhang2023/awesome-embodied-vla-va-vln](https://github.com/jonyzhang2023/awesome-embodied-vla-va-vln) - A curated list of state-of-the-art research in embodied AI, focusing on vision-language-action (VLA) models, vision-language navigation (VLN), and related multimodal learning approaches.

*   [wz0919/VLN-SRDF](https://github.com/wz0919/VLN-SRDF) - Official implementation of: Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel

*   [aikit-wrc/robosense\_ac\_slam](https://github.com/aikit-wrc/robosense_ac_slam) - A Fast and Tightly-coupled Sparse-Direct LiDAR-Inertial-Visual Odometry (LIVO).

*   [haoranD/Awesome-Embodied-AI](https://github.com/haoranD/Awesome-Embodied-AI) - A curated list of awesome papers on Embodied AI and related research/industry-driven resources.

*   [peakpang/UGP](https://github.com/peakpang/UGP) - \[CVPR 2025 Highlight] Unlocking Generalization Power in LiDAR Point Cloud Registration

*   [fffaraz/awesome-cpp](https://github.com/fffaraz/awesome-cpp) - A curated list of awesome C++ (or C) frameworks, libraries, resources, and shiny things. Inspired by awesome-... stuff.

*   [ikaijua/Awesome-AITools](https://github.com/ikaijua/Awesome-AITools) - Collection of AI-related utilities. Welcome to submit issues and pull requests /Êî∂ËóèAIÁõ∏ÂÖ≥ÁöÑÂÆûÁî®Â∑•ÂÖ∑ÔºåÊ¨¢ËøéÊèê‰∫§issues ÊàñËÄÖpull requests

*   [flyingGH/open\_vio](https://github.com/flyingGH/open_vio) - Âü∫‰∫évins-fusion ‰øÆÊîπÔºåÊèêÂèñÂÖ≥ÈîÆÂ∏ßÁî®‰∫é‰∏âÁª¥ÈáçÂª∫

*   [PetWorm/IMU-Preintegration-Propogation-Doc](https://github.com/PetWorm/IMU-Preintegration-Propogation-Doc) - ‰∏≠ÊñáÊñáÊ°£ÔºöIMUÈ¢ÑÁßØÂàÜÊÄªÁªì‰∏éÂÖ¨ÂºèÊé®ÂØº

*   [MIT-SPARK/Kimera](https://github.com/MIT-SPARK/Kimera) - Index repo for Kimera code

*   [AdrianWilczynski/OneDarkPro](https://github.com/AdrianWilczynski/OneDarkPro) - "One Dark Pro" theme for Visual Studio generated using Alexander Teinum's "Dainty for Visual Studio", saved with "Visual Studio Color Theme Designer" and tweaked to closer match Binaryify's "One Dark Pro" theme for Visual Studio Code.

*   [szx-0633/DeepSeek-R1-learning-note](https://github.com/szx-0633/DeepSeek-R1-learning-note) - My learning note about DeepSeek-R1 reasoning LLM

*   [John19187/v2ray-SSR-Clash-Verge-Shadowrocke](https://github.com/John19187/v2ray-SSR-Clash-Verge-Shadowrocke) - 2025Âπ¥ÂÖçË¥πÈ´òÈÄüÔºà23.6M/SÔºâv2ray„ÄÅss„ÄÅsing-box„ÄÅClash„ÄÅVerge„ÄÅSSR„ÄÅShadowrocke-Â∞èÁÅ´ÁÆ≠Êú∫Âú∫ËäÇÁÇπËÆ¢ÈòÖÊåáÂçóÔºåÁøªÂ¢ôÊ¢ØÂ≠êÔºåÁîµËÑë„ÄÅÊâãÊú∫„ÄÅiOS„ÄÅÂÆâÂçì„ÄÅwindows„ÄÅMac„ÄÅLinux„ÄÅË∑ØÁî±Âô®ÁøªÂ¢ô„ÄÅÁßëÂ≠¶‰∏äÁΩë„ÄÅËß£ÈîÅYouTube„ÄÅNetflix„ÄÅTikTok„ÄÅChatGPT„ÄÅbilibiliÊ∏ØÊæ≥Âè∞„ÄÇÁßëÂ≠¶‰∏äÁΩë„ÄÅÊ¢ØÂ≠ê„ÄÅVPNÊµãËØÑÔºåÈÄÇÁî®Clash„ÄÅV2ray„ÄÅÂ∞èÁÅ´ÁÆ≠„ÄÅsing-boxÁ≠âÂÆ¢Êà∑Á´Ø

*   [Lee-JaeWon/2024-Arxiv-Paper-List-Gaussian-Splatting](https://github.com/Lee-JaeWon/2024-Arxiv-Paper-List-Gaussian-Splatting) - 2024 Gaussian Splatting Paper List(Arxiv)

*   [StarCycle/Awesome-Embodied-AI-Job](https://github.com/StarCycle/Awesome-Embodied-AI-Job) - Lumina Robotics Talent Call | LuminaÁ§æÂå∫ÂÖ∑Ë∫´Êô∫ËÉΩÊãõË¥§Ê¶ú | A list for Embodied AI / Robotics Jobs (PhD, RA, intern, full-time, etc

*   [Pawdroid/Free-servers](https://github.com/Pawdroid/Free-servers) - üöÄ ÂÖçË¥πËÆ¢ÈòÖÂú∞ÂùÄÔºåüöÄ ÂÖçË¥πËäÇÁÇπÔºåüöÄ 6Â∞èÊó∂Êõ¥Êñ∞‰∏ÄÊ¨°ÔºåÂÖ±‰∫´ËäÇÁÇπÔºåËäÇÁÇπË¥®ÈáèÈ´òÂèØÁî®ÔºåÂÆåÂÖ®ÂÖçË¥π„ÄÇÂÖçË¥πclashËÆ¢ÈòÖÂú∞ÂùÄÔºåÂÖçË¥πÁøªÂ¢ô„ÄÅÂÖçË¥πÁßëÂ≠¶‰∏äÁΩë„ÄÅÂÖçË¥πÊ¢ØÂ≠ê„ÄÅÂÖçË¥πss/v2ray/trojanËäÇÁÇπ„ÄÅË∞∑Ê≠åÂïÜÂ∫ó„ÄÅÁøªÂ¢ôÊ¢ØÂ≠ê„ÄÇüöÄ Free subscription address, üöÄ Free node, üöÄ Updated every 6 hours, shared node, high-quality node availability, completely free. Free clash subscription address, free ss/v2ray/trojan node.

*   [getActivity/EmojiPackage](https://github.com/getActivity/EmojiPackage) - Ë°®ÊÉÖÂåÖËµÑÊ∫êÂêàÈõÜÔºåÂº†Âº†ÈÉΩÊòØÁªèÂÖ∏

*   [BestJunYu/Awesome-Physics-aware-Generation](https://github.com/BestJunYu/Awesome-Physics-aware-Generation) - Physical laws underpin all existence, and harnessing them for generative modeling opens boundless possibilities for advancing science and shaping the future!

*   [deepseek-ai/awesome-deepseek-integration](https://github.com/deepseek-ai/awesome-deepseek-integration) - Integrate the DeepSeek API into popular softwares

*   [LongHZ140516/awesome-framework-gallery](https://github.com/LongHZ140516/awesome-framework-gallery) - Awesome lists about framework figures in papers

*   [deepseek-ai/DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1) -

*   [shinyypig/latex-vscode-config](https://github.com/shinyypig/latex-vscode-config) - Use LaTeX in VSCode.

*   [cheryyunl/awesome-generalist-agents](https://github.com/cheryyunl/awesome-generalist-agents) - A curated list of papers for generalist agents

*   [jinyummiao/map-in-mono-reloc](https://github.com/jinyummiao/map-in-mono-reloc) - a paper list of visual re-localization algorithms

*   [Vincentqyw/Recent-Stars-2025](https://github.com/Vincentqyw/Recent-Stars-2025) - üî•SLAM, VIsual localization, keypoint detection, Image matching, Pose/Object tracking, Depth/Disparity/Flow Estimation, 3D-graphic, etc. related papers and code

*   [youngguncho/awesome-slam-datasets](https://github.com/youngguncho/awesome-slam-datasets) - A curated list of awesome datasets for SLAM

*   [zju3dv/MatchAnything](https://github.com/zju3dv/MatchAnything) - Code for "MatchAnything: Universal Cross-Modality Image Matching with Large-Scale Pre-Training", Arxiv 2025.

*   [Hannibal046/Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM) - Awesome-LLM: a curated list of Large Language Model

*   [sirius1024/iterm2-with-oh-my-zsh](https://github.com/sirius1024/iterm2-with-oh-my-zsh) - iTerm2 + Oh My Zsh ÊâìÈÄ†ËàíÈÄÇÁªàÁ´Ø‰ΩìÈ™å

*   [dtc111111/awesome-representation-for-robotics](https://github.com/dtc111111/awesome-representation-for-robotics) -

*   [siyuanliii/SLAck](https://github.com/siyuanliii/SLAck) - Official Implementation of ECCV2024 paper: SLAck

*   [Active-SLAM/Active-SLAM-Paper-List](https://github.com/Active-SLAM/Active-SLAM-Paper-List) - This repository primarily organizes papers, code, and other relevant materials related to Active SLAM and Robotic Exploration.

*   [serhii-londar/open-source-mac-os-apps](https://github.com/serhii-londar/open-source-mac-os-apps) - üöÄ Awesome list of open source applications for macOS. https://t.me/s/opensourcemacosapps

*   [linyicheng1/Quaternion-Kinematics-for-the-Error-State-Kalman-Filter](https://github.com/linyicheng1/Quaternion-Kinematics-for-the-Error-State-Kalman-Filter) - Quaternion Kinematics for the Error-State Kalman Filter (‰∏≠ÊñáÂÖ®ÊñáÁøªËØë)

*   [SJTU-ViSYS/M2DGR-plus](https://github.com/SJTU-ViSYS/M2DGR-plus) - Extension and update of M2DGR: a novel Multi-modal and Multi-scenario SLAM Dataset for Ground Robots (ICRA2022 & ICRA2024)

*   [HKUST-Aerial-Robotics/OmniNxt](https://github.com/HKUST-Aerial-Robotics/OmniNxt) - \[IROS'24 Oral] A Fully Open-source and Compact Aerial Robot with Omnidirectional Visual Perception

*   [GeekLiB/Lee-SLAM-source](https://github.com/GeekLiB/Lee-SLAM-source) - SLAM ÂºÄÂèëÂ≠¶‰π†ËµÑÊ∫ê‰∏éÁªèÈ™åÂàÜ‰∫´

*   [uzh-rpg/event-based\_vision\_resources](https://github.com/uzh-rpg/event-based_vision_resources) - Event-based Vision Resources. Community effort to collect knowledge on event-based vision technology (papers, workshops, datasets, code, videos, etc)

*   [bkhanal-11/awesome-360-depth-estimation](https://github.com/bkhanal-11/awesome-360-depth-estimation) - State-of-the-art papers for depth estimation of 360 images.

*   [L3Y1Q2/MyBrain](https://github.com/L3Y1Q2/MyBrain) - Knowledge makes up the brain

*   [franciscoliu/Awesome-GenAI-Unlearning](https://github.com/franciscoliu/Awesome-GenAI-Unlearning) -

*   [changh95/visual-slam-roadmap](https://github.com/changh95/visual-slam-roadmap) - Roadmap to become a Visual-SLAM developer in 2023

*   [kmk97/DGS-SLAM](https://github.com/kmk97/DGS-SLAM) -

*   [SJTU-ViSYS/M2DGR](https://github.com/SJTU-ViSYS/M2DGR) - M2DGRÔºö a Multi-modal and Multi-scenario Dataset for Ground Robots(RA-L2021 & ICRA2022)

*   [IntelliSensing/UAV-VisLoc](https://github.com/IntelliSensing/UAV-VisLoc) - UAV-VisLoc: A Large-scale Dataset for UAV Visual Localization

*   [luohongk/slam-handbook-chinese](https://github.com/luohongk/slam-handbook-chinese) - Êú¨È°πÁõÆ‰∏ªË¶ÅÊòØÂÖ≥‰∫éslam handbookÁöÑ‰∏≠ÊñáÁâàÊú¨

*   [RipplePiam/MobaXterm-Chinese-Simplified](https://github.com/RipplePiam/MobaXterm-Chinese-Simplified) - MobaXterm ÁÆÄ‰Ωì‰∏≠ÊñáÊ±âÂåñÁâàüåèüñ•üñ•üñ• „ÄêüíåÊÖ¢Â∑•Á≤æÂøÉÂà∂‰ΩúÔºå"ÊèêÁ§∫"‰πüÊ±âÂåñüíª„Äë „ÄêüòçÊéß‰ª∂Â∏ÉÂ±ÄÁ≤æÁªÜË∞ÉÊï¥„Äë

*   [OpenDriveLab/End-to-end-Autonomous-Driving](https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving) - \[IEEE T-PAMI 2024] All you need for End-to-end Autonomous Driving

*   [xiliu8006/3DGS-Enhancer](https://github.com/xiliu8006/3DGS-Enhancer) -

*   [maziarraissi/Applied-Deep-Learning](https://github.com/maziarraissi/Applied-Deep-Learning) - Applied Deep Learning Course

*   [520xyxyzq/awesome-object-SLAM](https://github.com/520xyxyzq/awesome-object-SLAM) - A curated list of Object SLAM papers and resources

*   [HuaiyuanXu/3D-Occupancy-Perception](https://github.com/HuaiyuanXu/3D-Occupancy-Perception) - \[Information Fusion 2025] A Survey on Occupancy Perception for Autonomous Driving: The Information Fusion Perspective

*   [chicleee/Image-Matching-Paper-List](https://github.com/chicleee/Image-Matching-Paper-List) - A personal list of papers and resources of image matching and pose estimation, including perspective images and panoramas.

*   [sheng00125/LIV-GaussMap](https://github.com/sheng00125/LIV-GaussMap) -

*   [SilenceOverflow/Awesome-SLAM](https://github.com/SilenceOverflow/Awesome-SLAM) - A curated list of SLAM resources

*   [52CV/awesome-huggingface](https://github.com/52CV/awesome-huggingface) - ü§ó A list of wonderful open-source projects & applications integrated with Hugging Face libraries.

*   [HCPLab-SYSU/Embodied\_AI\_Paper\_List](https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List) - \[Embodied-AI-Survey-2025] Paper List and Resource Repository for Embodied AI

*   [william-sto/JusticeNeverTooLate](https://github.com/william-sto/JusticeNeverTooLate) - Â≠óËäÇË∑≥Âä®ÁìúÊúÄÁªàÁúüÂÆûÊÉÖÂÜµÔºåÁî®‰∫ãÂÆûËØ¥ËØùÔºåÊ≠£‰πâ‰ºöËøüÂà∞‰ΩÜ‰∏ç‰ºöÁº∫Â∏≠ÔºÅ

*   [amusi/CVPR2025-Papers-with-Code](https://github.com/amusi/CVPR2025-Papers-with-Code) - CVPR 2025 ËÆ∫ÊñáÂíåÂºÄÊ∫êÈ°πÁõÆÂêàÈõÜ

*   [zhuhu00/Awesome\_Dynamic\_SLAM](https://github.com/zhuhu00/Awesome_Dynamic_SLAM) - Dynamic SLAM, Life-long SLAM Research(Lidar, Visual, Sensor Fusion etc.)

*   [hongwenjun/tmux\_for\_windows](https://github.com/hongwenjun/tmux_for_windows) - tmuxÊòØ‰∏Ä‰∏™ÂºÄÊ∫êÂ∑•ÂÖ∑ÔºåÁî®‰∫éÂú®‰∏Ä‰∏™ÁªàÁ´ØÁ™óÂè£‰∏≠ËøêË°åÂ§ö‰∏™ÁªàÁ´Ø‰ºöËØù„ÄÇÊú¨Â∑•ÂÖ∑‰ªémsys2ÈáåÊèêÂèñÔºåÂèØ‰ª•Âú®Git for WindowsÁöÑGit Bash (MingW64)‰∏ãÊ≠£Â∏∏‰ΩøÁî®„ÄÇ

*   [HumanAIGC/AnimateAnyone](https://github.com/HumanAIGC/AnimateAnyone) - Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation

*   [Open3DVLab/StreetSurfGS](https://github.com/Open3DVLab/StreetSurfGS) - StreetSurfGS: Scalable Large Scene Surface Reconstruction with Gaussian Splatting for Urban Street Scences

*   [TianxingChen/Embodied-AI-Guide](https://github.com/TianxingChen/Embodied-AI-Guide) - \[Lumina Embodied AI] ÂÖ∑Ë∫´Êô∫ËÉΩÊäÄÊúØÊåáÂçó Embodied-AI-Guide

*   [Open3DVLab/GigaGS](https://github.com/Open3DVLab/GigaGS) - \[AAAI 2025] GigaGS: Scaling up Planar-Based 3D Gaussians for Large Scene Surface Reconstruction

*   [Thinklab-SJTU/Awesome-LLM4AD](https://github.com/Thinklab-SJTU/Awesome-LLM4AD) - A curated list of awesome LLM/VLM/VLA for Autonomous Driving(LLM4AD) resources (continually updated)

*   [ai-vip/stable-diffusion-tutorial](https://github.com/ai-vip/stable-diffusion-tutorial) - ÂÖ®ÁΩëÊúÄÂÖ®Stable DiffusionÂÖ®Â•óÊïôÁ®ãÔºå‰ªéÂÖ•Èó®Âà∞ËøõÈò∂ÔºåËÄóÊó∂‰∏â‰∏™ÊúàÂà∂‰Ωú

*   [AlbertSlam/Lee-SLAM-source](https://github.com/AlbertSlam/Lee-SLAM-source) - SLAM ÂºÄÂèëÂ≠¶‰π†ËµÑÊ∫ê‰∏éÁªèÈ™åÂàÜ‰∫´

*   [YangSiri/OR-LIM](https://github.com/YangSiri/OR-LIM) - OR-LIM: Observability-aware robust LiDAR-Inertial-Mapping  under High Dynamic Sensor Motion

*   [DeepLabc/LargeScale\_3DGS](https://github.com/DeepLabc/LargeScale_3DGS) - 3D Gaussian Splatting Papers Relating to Large-Scale Scene.

*   [sjtuyinjie/awesome-LiDAR-Visual-SLAM](https://github.com/sjtuyinjie/awesome-LiDAR-Visual-SLAM) - A curated list of resources relevant to LiDAR-Visual-Fusion-SLAM

*   [perkfly/reverse-interview-zh](https://github.com/perkfly/reverse-interview-zh) - ÊäÄÊúØÈù¢ËØïÊúÄÂêéÂèçÈóÆÈù¢ËØïÂÆòÁöÑËØù

*   [kwea123/gaussian\_splatting\_notes](https://github.com/kwea123/gaussian_splatting_notes) - A detailed formulae explanation on gaussian splatting

*   [zju3dv/100-Phones](https://github.com/zju3dv/100-Phones) -

*   [623637646/996.Leave](https://github.com/623637646/996.Leave) - ÈÄÉÁ¶ª996

*   [Meltwin/Noetic-Ubuntu22.04](https://github.com/Meltwin/Noetic-Ubuntu22.04) - Manual instructions on how to install ROS1 Noetic on Ubuntu 22.04

*   [StevenCui/VIO-Doc](https://github.com/StevenCui/VIO-Doc) - ‰∏ªÊµÅVIOËÆ∫ÊñáÊé®ÂØºÂèä‰ª£Á†ÅËß£Êûê

*   [ericzzj1989/Awesome-Image-Matching](https://github.com/ericzzj1989/Awesome-Image-Matching) - Bibliographic list for papers of image matching

*   [miss-mumu/developer2gwy](https://github.com/miss-mumu/developer2gwy) - ÂÖ¨Âä°Âëò‰ªéÂÖ•Èó®Âà∞‰∏äÂ≤∏ÔºåÊúÄ‰Ω≥Á®ãÂ∫èÂëòÂÖ¨ËÄÉÂÆûË∑µÊïôÁ®ã

*   [llamastack/llama-stack-apps](https://github.com/llamastack/llama-stack-apps) - Agentic components of the Llama Stack APIs

*   [0voice/expert\_readed\_books](https://github.com/0voice/expert_readed_books) - 2021Âπ¥ÊúÄÊñ∞ÊÄªÁªìÔºåÊé®ËçêÂ∑•Á®ãÂ∏àÂêàÈÄÇËØªÊú¨ÔºåËÆ°ÁÆóÊú∫ÁßëÂ≠¶ÔºåËΩØ‰ª∂ÊäÄÊúØÔºåÂàõ‰∏öÔºåÊÄùÊÉ≥Á±ªÔºåÊï∞Â≠¶Á±ªÔºå‰∫∫Áâ©‰º†ËÆ∞‰π¶Á±ç

*   [jianzongwu/Awesome-Open-Vocabulary](https://github.com/jianzongwu/Awesome-Open-Vocabulary) - (TPAMI 2024) A Survey on Open Vocabulary Learning

*   [lvchuandong/Awesome-Multi-Camera-3D-Occupancy-Prediction](https://github.com/lvchuandong/Awesome-Multi-Camera-3D-Occupancy-Prediction) - Awesome papers and code about Multi-Camera 3D Occupancy Prediction, such as TPVFormer, SurroundOcc, PanoOcc, OccFormer, FB-OCC, SelfOcc, COTR, SparseOcc, GaussianFormer, GaussianOcc, STCOcc, OccMamba. In this repository, you will see the latest 3D occupancy prediction papers and code.

*   [eriksandstroem/Splat-SLAM](https://github.com/eriksandstroem/Splat-SLAM) -

*   [bdvisl/DriveInsight](https://github.com/bdvisl/DriveInsight) -

*   [pubsys/ReviewSystem](https://github.com/pubsys/ReviewSystem) - ÂÆ°Á®øÁ≥ªÁªüÁöÑËá™Ëø∞

*   [weisongwen/UrbanNavDataset](https://github.com/weisongwen/UrbanNavDataset) - UrbanNav: an¬†Open-Sourcing Localization Data Collected in Asian Urban Canyons, Including Tokyo and Hong Kong

*   [datawhalechina/pumpkin-book](https://github.com/datawhalechina/pumpkin-book) - „ÄäÊú∫Âô®Â≠¶‰π†„ÄãÔºàË•øÁìú‰π¶ÔºâÂÖ¨ÂºèËØ¶Ëß£

*   [pengsida/learning\_research](https://github.com/pengsida/learning_research) - Êú¨‰∫∫ÁöÑÁßëÁ†îÁªèÈ™å

*   [zzzzxxxx111/SLslam](https://github.com/zzzzxxxx111/SLslam) -

*   [amusi/Deep-Learning-Interview-Book](https://github.com/amusi/Deep-Learning-Interview-Book) - Ê∑±Â∫¶Â≠¶‰π†Èù¢ËØïÂÆùÂÖ∏ÔºàÂê´Êï∞Â≠¶„ÄÅÊú∫Âô®Â≠¶‰π†„ÄÅÊ∑±Â∫¶Â≠¶‰π†„ÄÅËÆ°ÁÆóÊú∫ËßÜËßâ„ÄÅËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÂíåSLAMÁ≠âÊñπÂêëÔºâ

*   [cbamls/AI\_Tutorial](https://github.com/cbamls/AI_Tutorial) - Á≤æÈÄâÊú∫Âô®Â≠¶‰π†ÔºåNLPÔºåÂõæÂÉèËØÜÂà´Ôºå Ê∑±Â∫¶Â≠¶‰π†Á≠â‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÂ≠¶‰π†ËµÑÊñôÔºåÊêúÁ¥¢ÔºåÊé®ËçêÔºåÂπøÂëäÁ≥ªÁªüÊû∂ÊûÑÂèäÁÆóÊ≥ïÊäÄÊúØËµÑÊñôÊï¥ÁêÜ„ÄÇÁÆóÊ≥ïÂ§ßÁâõÁ¨îËÆ∞Ê±áÊÄª

*   [3D-Vision-World/awesome-NeRF-and-3DGS-SLAM](https://github.com/3D-Vision-World/awesome-NeRF-and-3DGS-SLAM) - A comprehensive list of Implicit Representations, NeRF and 3D Gaussian Splatting papers relating to SLAM/Robotics domain, including papers, videos, codes, and related websites

*   [Franky-X/Awesome-Embodied-Navigation](https://github.com/Franky-X/Awesome-Embodied-Navigation) - Awesome Embodied Navigation: Concept, Paradigm and State-of-the-arts

*   [nubot-nudt/TD-NeRF](https://github.com/nubot-nudt/TD-NeRF) - \[IROS24] TD-NeRF: Novel Truncated Depth Prior for Joint Camera Pose and Neural Radiance Field Optimization

## TypeScript

*   [thedotmack/claude-mem](https://github.com/thedotmack/claude-mem) - A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude's agent-sdk), and injects relevant context back into future sessions.

*   [dongguaguaguagua/zhihu\_obsidian](https://github.com/dongguaguaguagua/zhihu_obsidian) - Zhihu on Obsidian | Áü•‰πé Obsidian Êèí‰ª∂

*   [OpenCut-app/OpenCut](https://github.com/OpenCut-app/OpenCut) - The open-source CapCut alternative

*   [chanhx/crabviz](https://github.com/chanhx/crabviz) - Generate interactive call graphs for various languages

*   [plait-board/drawnix](https://github.com/plait-board/drawnix) - ÂºÄÊ∫êÁôΩÊùøÂ∑•ÂÖ∑ÔºàSaaSÔºâÔºå‰∏Ä‰ΩìÂåñÁôΩÊùøÔºåÂåÖÂê´ÊÄùÁª¥ÂØºÂõæ„ÄÅÊµÅÁ®ãÂõæ„ÄÅËá™Áî±ÁîªÁ≠â„ÄÇAll in one open-source whiteboard tool with mind, flowchart, freehand and etc.

*   [google-gemini/gemini-cli](https://github.com/google-gemini/gemini-cli) - An open-source AI agent that brings the power of Gemini directly into your terminal.

*   [n8n-io/n8n](https://github.com/n8n-io/n8n) - Fair-code workflow automation platform with native AI capabilities. Combine visual building with custom code, self-host or cloud, 400+ integrations.

*   [123xiao/sex-agreement-app](https://github.com/123xiao/sex-agreement-app) - XË°å‰∏∫ÂêåÊÑèÂçèËÆÆÁ≥ªÁªü

*   [microsoft/vscode](https://github.com/microsoft/vscode) - Visual Studio Code

*   [mastra-ai/mastra](https://github.com/mastra-ai/mastra) - The TypeScript AI agent framework. ‚ö° Assistants, RAG, observability. Supports any LLM: GPT-4, Claude, Gemini, Llama.

*   [Binaryify/OneDark-Pro](https://github.com/Binaryify/OneDark-Pro) - Atom's iconic One Dark theme for Visual Studio Code

*   [f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts) - Share, discover, and collect prompts from the community. Free and open source ‚Äî self-host for your organization with complete privacy.

*   [David-patrick-chuks/Riona-AI-Agent](https://github.com/David-patrick-chuks/Riona-AI-Agent) - Riona Ai Agent üå∏ is built using Node.js and TypeScript üõ†Ô∏è, designed for seamless job execution üì∏. It's lightweight, efficient, and still evolving üöß‚Äîexciting new features coming soon! üåü

*   [MegaScenes/web-viewer](https://github.com/MegaScenes/web-viewer) - web viewer for 3d reconstructions

*   [coaidev/coai](https://github.com/coaidev/coai) - üöÄ Next Generation AI One-Stop Internationalization Solution. üöÄ ‰∏ã‰∏Ä‰ª£ AI ‰∏ÄÁ´ôÂºè B/C Á´ØËß£ÂÜ≥ÊñπÊ°àÔºåÊîØÊåÅ OpenAIÔºåMidjourneyÔºåClaudeÔºåËÆØÈ£ûÊòüÁÅ´ÔºåStable DiffusionÔºåDALL¬∑EÔºåChatGLMÔºåÈÄö‰πâÂçÉÈóÆÔºåËÖæËÆØÊ∑∑ÂÖÉÔºå360 Êô∫ËÑëÔºåÁôæÂ∑ù AIÔºåÁÅ´Â±±ÊñπËàüÔºåÊñ∞ÂøÖÂ∫îÔºåGeminiÔºåMoonshot Á≠âÊ®°ÂûãÔºåÊîØÊåÅÂØπËØùÂàÜ‰∫´ÔºåËá™ÂÆö‰πâÈ¢ÑËÆæÔºå‰∫ëÁ´ØÂêåÊ≠•ÔºåÊ®°ÂûãÂ∏ÇÂú∫ÔºåÊîØÊåÅÂºπÊÄßËÆ°Ë¥πÂíåËÆ¢ÈòÖËÆ°ÂàíÊ®°ÂºèÔºåÊîØÊåÅÂõæÁâáËß£ÊûêÔºåÊîØÊåÅËÅîÁΩëÊêúÁ¥¢ÔºåÊîØÊåÅÊ®°ÂûãÁºìÂ≠òÔºå‰∏∞ÂØåÁæéËßÇÁöÑÂêéÂè∞ÁÆ°ÁêÜ‰∏é‰ª™Ë°®ÁõòÊï∞ÊçÆÁªüËÆ°„ÄÇ

*   [Eugeny/tabby](https://github.com/Eugeny/tabby) - A terminal for a more modern age

*   [amir9480/vscode-cpp-helper](https://github.com/amir9480/vscode-cpp-helper) - vscode extension to create implementation for c++ function prototypes.

*   [hcengineering/platform](https://github.com/hcengineering/platform) - Huly ‚Äî All-in-One Project Management Platform (alternative to Linear, Jira, Slack, Notion, Motion)

*   [conwnet/github1s](https://github.com/conwnet/github1s) - One second to read GitHub code with VS Code.

*   [ocsjs/ocsjs](https://github.com/ocsjs/ocsjs) - OCS ÁΩëËØæÂä©ÊâãÔºåÂà∑ËØæËÑöÊú¨ÔºåÁΩëËØæËÑöÊú¨ÔºåÂ∏ÆÂä©Â§ßÂ≠¶ÁîüËß£ÂÜ≥ÁΩëËØæÈöæÈ¢òÔºåÊîØÊåÅ„ÄêË∂ÖÊòüÂ≠¶‰π†ÈÄö„Äë„ÄêÁü•ÈÅìÊô∫ÊÖßÊ†ë„Äë„ÄêËÅåÊïô‰∫ë„Äë„ÄêÊô∫ÊÖßËÅåÊïô„Äë„Äê‰∏≠ÂõΩÂ§ßÂ≠¶MOOC„ÄëÁ≠âÁΩëËØæ  Ôºå ÂèØ‰ª•Âú® ËÑöÊú¨Áå´ ‰ª•Âèä Ê≤πÁå¥ Á≠âÂºÄÊ∫êËÑöÊú¨ÁÆ°ÁêÜÂô®‰∏ãËøêË°å„ÄÇ

*   [immich-app/immich](https://github.com/immich-app/immich) - High performance self-hosted photo and video management solution.

*   [clash-verge-rev/clash-verge-rev](https://github.com/clash-verge-rev/clash-verge-rev) - A modern GUI client based on Tauri, designed to run in Windows, macOS and Linux for tailored proxy experience

## CSS

*   [fengtt42/U2UData-2](https://github.com/fengtt42/U2UData-2) - THU-EAI Lab

*   [luohongk/AcademicHomepage](https://github.com/luohongk/AcademicHomepage) - This project is a beautiful personal academic homepage template created by me. Welcome to use it.

*   [zzwu29/Arxiv](https://github.com/zzwu29/Arxiv) -

*   [Maserhe/VScode-Markdown-theme-Maserhe](https://github.com/Maserhe/VScode-Markdown-theme-Maserhe) - vscode Ëá™ÂÆö‰πâMarkdownÊéíÁâàÈ£éÊ†ºÔºå‰ª•Âèä‰ª£Á†ÅÂùóÊ†∑ÂºèÈ£éÊ†º„ÄÇ

*   [wzzheng/GaussianFormer](https://github.com/wzzheng/GaussianFormer) - Project Page for GaussianFormer

## CMake

*   [aCodeDog/awesome-loco-manipulation](https://github.com/aCodeDog/awesome-loco-manipulation) -

*   [i2Nav-WHU/i2Nav-Robot](https://github.com/i2Nav-WHU/i2Nav-Robot) - A Large-Scale Indoor-Outdoor Robot Dataset for Multi-Sensor Fusion Navigation and Mapping

*   [lesaf92/ros\_noetic\_ubuntu22](https://github.com/lesaf92/ros_noetic_ubuntu22) - Instructions for installing ROS Noetic on Ubuntu 22.04

## Swift

*   [caol64/wenyan](https://github.com/caol64/wenyan) - ÊñáÈ¢ú- MarkdownÊñáÁ´†ÊéíÁâàÁæéÂåñÂ∑•ÂÖ∑ÔºåÊîØÊåÅÂæÆ‰ø°ÂÖ¨‰ºóÂè∑„ÄÅ‰ªäÊó•Â§¥Êù°„ÄÅÁü•‰πéÁ≠âÂπ≥Âè∞„ÄÇ

*   [jordanbaird/Ice](https://github.com/jordanbaird/Ice) - Powerful menu bar manager for macOS

*   [lwouis/alt-tab-macos](https://github.com/lwouis/alt-tab-macos) - Windows alt-tab on macOS

*   [exelban/stats](https://github.com/exelban/stats) - macOS system monitor in your menu bar

*   [ejbills/DockDoor](https://github.com/ejbills/DockDoor) - Window peeking, alt-tab and other enhancements for macOS

*   [Caldis/Mos](https://github.com/Caldis/Mos) - ‰∏Ä‰∏™Áî®‰∫éÂú® macOS ‰∏äÂπ≥Êªë‰Ω†ÁöÑÈº†Ê†áÊªöÂä®ÊïàÊûúÊàñÂçïÁã¨ËÆæÁΩÆÊªöÂä®ÊñπÂêëÁöÑÂ∞èÂ∑•ÂÖ∑, ËÆ©‰Ω†ÁöÑÊªöËΩÆÁàΩÂ¶ÇËß¶ÊéßÊùø  |  A lightweight tool used to smooth scrolling and set scroll direction independently for your mouse on macOS

*   [gao-sun/eul](https://github.com/gao-sun/eul) - üñ•Ô∏è macOS status monitoring app written in SwiftUI.

## Kotlin

*   [pppscn/SmsForwarder](https://github.com/pppscn/SmsForwarder) - Áü≠‰ø°ËΩ¨ÂèëÂô®‚Äî‚ÄîÁõëÊéßAndroidÊâãÊú∫Áü≠‰ø°„ÄÅÊù•Áîµ„ÄÅAPPÈÄöÁü•ÔºåÂπ∂Ê†πÊçÆÊåáÂÆöËßÑÂàôËΩ¨ÂèëÂà∞ÂÖ∂‰ªñÊâãÊú∫ÔºöÈíâÈíâÁæ§Ëá™ÂÆö‰πâÊú∫Âô®‰∫∫„ÄÅÈíâÈíâ‰ºÅ‰∏öÂÜÖÊú∫Âô®‰∫∫„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Áæ§Êú∫Âô®‰∫∫„ÄÅÈ£û‰π¶Êú∫Âô®‰∫∫„ÄÅ‰ºÅ‰∏öÂæÆ‰ø°Â∫îÁî®Ê∂àÊÅØ„ÄÅÈÇÆÁÆ±„ÄÅbark„ÄÅwebhook„ÄÅTelegramÊú∫Âô®‰∫∫„ÄÅServerÈÖ±„ÄÅPushPlus„ÄÅÊâãÊú∫Áü≠‰ø°Á≠â„ÄÇÂåÖÊã¨‰∏ªÂä®ÊéßÂà∂ÊúçÂä°Á´Ø‰∏éÂÆ¢Êà∑Á´ØÔºåËÆ©‰Ω†ËΩªÊùæËøúÁ®ãÂèëÁü≠‰ø°„ÄÅÊü•Áü≠‰ø°„ÄÅÊü•ÈÄöËØù„ÄÅÊü•ËØùÁ∞ø„ÄÅÊü•ÁîµÈáèÁ≠â„ÄÇÔºàV3.0 Êñ∞Â¢ûÔºâPS.Ëøô‰∏™APK‰∏ªË¶ÅÊòØÂ≠¶‰π†‰∏éËá™Áî®ÔºåÂ¶ÇÊúâBUGËØ∑ÊèêISSUEÔºåÂêåÊó∂Ê¨¢ËøéÂ§ßÂÆ∂ÊèêPRÊåáÊ≠£

## TeX

*   [MCG-NKU/NSFC-LaTex](https://github.com/MCG-NKU/NSFC-LaTex) -

*   [Azure1210/elegantbook-magic-revision](https://github.com/Azure1210/elegantbook-magic-revision) - ElegentbookÈ≠îÊîπÁâàÊú¨!

*   [natolambert/rlhf-book](https://github.com/natolambert/rlhf-book) - Textbook on reinforcement learning from human feedback

*   [fky2015/resume-ng](https://github.com/fky2015/resume-ng) - A LaTeX resume template designed for optimal information density and aesthetic appeal.

*   [SLAM-Handbook-contributors/slam-handbook-public-release](https://github.com/SLAM-Handbook-contributors/slam-handbook-public-release) - Release repo for our SLAM Handbook

*   [lliei0x/Moderncv-LateX](https://github.com/lliei0x/Moderncv-LateX) - LaTeXÁÆÄÂéÜÊ®°ÁâàüëÄüìë

*   [HouJP/resume](https://github.com/HouJP/resume) - ‰ΩøÁî®LaTeXÁºñËØëÁîüÊàêÁöÑ‰∏≠Ëã±Êñá‰∏™‰∫∫ÁÆÄÂéÜ

*   [whutug/whu-thesis](https://github.com/whutug/whu-thesis) - Ê≠¶Ê±âÂ§ßÂ≠¶ÊØï‰∏öËÆ∫Êñá LaTeX Ê®°Áâà 2025

## Shell

*   [nelvko/clash-for-linux-install](https://github.com/nelvko/clash-for-linux-install) - üòº ‰ºòÈõÖÂú∞‰ΩøÁî®Âü∫‰∫é clash/mihomo ÁöÑ‰ª£ÁêÜÁéØÂ¢É

*   [yuaotian/go-cursor-help](https://github.com/yuaotian/go-cursor-help) - Ëß£ÂÜ≥CursorÂú®ÂÖçË¥πËÆ¢ÈòÖÊúüÈó¥Âá∫Áé∞‰ª•‰∏ãÊèêÁ§∫ÁöÑÈóÆÈ¢ò:  Your request has been blocked as our system has detected suspicious activity / You've reached your trial request limit.  /  Too many free trial accounts used on this machine.

*   [dockur/macos](https://github.com/dockur/macos) - MacOS inside a Docker container.

*   [wnlen/clash-for-linux](https://github.com/wnlen/clash-for-linux) - clash-for-linux

*   [VocabVictor/clash-for-AutoDL](https://github.com/VocabVictor/clash-for-AutoDL) - AutoDLÂπ≥Âè∞ÊúçÂä°Âô®ÈÄÇÈÖçÊ¢ØÂ≠êÔºå ‰ΩøÁî® Clash ‰Ωú‰∏∫‰ª£ÁêÜÂ∑•ÂÖ∑

*   [techahold/rustdeskinstall](https://github.com/techahold/rustdeskinstall) - Easy install Script for Rustdesk

*   [liguodongiot/llm-resource](https://github.com/liguodongiot/llm-resource) - LLMÂÖ®Ê†à‰ºòË¥®ËµÑÊ∫êÊ±áÊÄª

*   [atakandag/data\_collection\_vloc](https://github.com/atakandag/data_collection_vloc) - Data Collection with Zed2 and Ouster LiDAR and 3D Reconstruction with Rtabmap

## Astro

*   [saicaca/fuwari](https://github.com/saicaca/fuwari) - ‚ú®A static blog template built with Astro.

*   [RomanHauksson/academic-project-astro-template](https://github.com/RomanHauksson/academic-project-astro-template) - Astro template to help you build an interactive project page for your research paper

## JavaScript

*   [zxkmm/firefox\_plugin\_github\_user\_notes](https://github.com/zxkmm/firefox_plugin_github_user_notes) - This tool allows you to take notes on specific users and sync them across devices with Firefox installed.

*   [remarkjs/remark-github](https://github.com/remarkjs/remark-github) - remark plugin to link references to commits, issues, pull-requests, and users, like on GitHub

*   [Gar-b-age/CookLikeHOC](https://github.com/Gar-b-age/CookLikeHOC) - ü•¢ÂÉèËÄÅ‰π°È∏°üêîÈÇ£Ê†∑ÂÅöÈ•≠„ÄÇ‰∏ªË¶ÅÈÉ®ÂàÜ‰∫é2024Âπ¥ÂÆåÂ∑•ÔºåÈùûËÄÅ‰π°È∏°ÂÆòÊñπ‰ªìÂ∫ì„ÄÇÊñáÂ≠óÊù•Ëá™„ÄäËÄÅ‰π°È∏°ËèúÂìÅÊ∫ØÊ∫êÊä•Âëä„ÄãÔºåÂπ∂ÂÅöÂΩíÁ∫≥„ÄÅÁºñËæë‰∏éÊï¥ÁêÜ„ÄÇCookLikeHOC.

*   [MarSeventh/CloudFlare-ImgBed](https://github.com/MarSeventh/CloudFlare-ImgBed) - Open-source file hosting solution based on CloudFlare  (Image hosting/File storage/Cloud drive) / Âü∫‰∫é CloudFlare ÁöÑÂºÄÊ∫êÊñá‰ª∂ÊâòÁÆ°Ëß£ÂÜ≥ÊñπÊ°àÔºàÂõæÂ∫ä/Êñá‰ª∂Â∫ä/ÁΩëÁõòÔºâ

*   [x-dr/telegraph-Image](https://github.com/x-dr/telegraph-Image) -

*   [xixu-me/xget](https://github.com/xixu-me/xget) - Ultra-high-performance, secure, all-in-one acceleration engine for developer resources

*   [shareAI-lab/analysis\_claude\_code](https://github.com/shareAI-lab/analysis_claude_code) - Êú¨‰ªìÂ∫ìÂåÖÂê´ÂØπ Claude Code v1.0.33 ËøõË°åÈÄÜÂêëÂ∑•Á®ãÁöÑÂÆåÊï¥Á†îÁ©∂ÂíåÂàÜÊûêËµÑÊñô„ÄÇÂåÖÊã¨ÂØπÊ∑∑Ê∑ÜÊ∫ê‰ª£Á†ÅÁöÑÊ∑±Â∫¶ÊäÄÊúØÂàÜÊûê„ÄÅÁ≥ªÁªüÊû∂ÊûÑÊñáÊ°£Ôºå‰ª•ÂèäÈáçÊûÑ Claude      Code agent Á≥ªÁªüÁöÑÂÆûÁé∞ËìùÂõæ„ÄÇ‰∏ªË¶ÅÂèëÁé∞ÂåÖÊã¨ÂÆûÊó∂ Steering Êú∫Âà∂„ÄÅÂ§ö Agent      Êû∂ÊûÑ„ÄÅÊô∫ËÉΩ‰∏ä‰∏ãÊñáÁÆ°ÁêÜÂíåÂ∑•ÂÖ∑ÊâßË°åÁÆ°ÈÅì„ÄÇËØ•È°πÁõÆ‰∏∫ÁêÜËß£Áé∞‰ª£ AI agent Á≥ªÁªüËÆæËÆ°ÂíåÂÆûÁé∞Êèê‰æõÊäÄÊúØÂèÇËÄÉ„ÄÇ

*   [StrayMeteor3337/WechatRealFriends](https://github.com/StrayMeteor3337/WechatRealFriends) - ÂæÆ‰ø°Â•ΩÂèãÂÖ≥Á≥ª‰∏ÄÈîÆÊ£ÄÊµãÔºåÂü∫‰∫éÂæÆ‰ø°ipadÂçèËÆÆÔºåÁúãÁúãÊúâÊ≤°ÊúâÊúãÂèãÂÅ∑ÂÅ∑Âà†ÊéâÊàñËÄÖÊãâÈªë‰Ω†

*   [Z-Siqi/Clash-for-Windows\_Chinese](https://github.com/Z-Siqi/Clash-for-Windows_Chinese) - clash for windowsÊ±âÂåñÁâà. Êèê‰æõclash for windowsÁöÑÊ±âÂåñÁâà, Ê±âÂåñË°•‰∏ÅÂèäÊ±âÂåñÁâàÂÆâË£ÖÁ®ãÂ∫è

*   [mrdoob/three.js](https://github.com/mrdoob/three.js) - JavaScript 3D Library.

*   [ConardLi/easy-dataset](https://github.com/ConardLi/easy-dataset) - A powerful tool for creating fine-tuning datasets for LLM

*   [kishimisu/Gaussian-Splatting-WebGL](https://github.com/kishimisu/Gaussian-Splatting-WebGL) - 3D Gaussian Splatting Renderer for WebGL

*   [overleaf/overleaf](https://github.com/overleaf/overleaf) - A web-based collaborative LaTeX editor

*   [antimatter15/splat](https://github.com/antimatter15/splat) - WebGL 3D Gaussian Splat Viewer

*   [eliahuhorwitz/Academic-project-page-template](https://github.com/eliahuhorwitz/Academic-project-page-template) - A project page template for academic papers. Demo at https://eliahuhorwitz.github.io/Academic-project-page-template/

*   [lutzroeder/netron](https://github.com/lutzroeder/netron) - Visualizer for neural network, deep learning and machine learning models

*   [NaiboWang/EasySpider](https://github.com/NaiboWang/EasySpider) - A visual no-code/code-free web crawler/spiderÊòìÈááÈõÜÔºö‰∏Ä‰∏™ÂèØËßÜÂåñÊµèËßàÂô®Ëá™Âä®ÂåñÊµãËØï/Êï∞ÊçÆÈááÈõÜ/Áà¨Ëô´ËΩØ‰ª∂ÔºåÂèØ‰ª•Êó†‰ª£Á†ÅÂõæÂΩ¢ÂåñÁöÑËÆæËÆ°ÂíåÊâßË°åÁà¨Ëô´‰ªªÂä°„ÄÇÂà´ÂêçÔºöServiceWrapperÈù¢ÂêëWebÂ∫îÁî®ÁöÑÊô∫ËÉΩÂåñÊúçÂä°Â∞ÅË£ÖÁ≥ªÁªü„ÄÇ

## HTML

*   [luohongk/SurveyAlgo](https://github.com/luohongk/SurveyAlgo) - üí™\[SurveyAlgo] ÊµãÁªòÁÆóÊ≥ïÂ∫ì! Êú¨È°πÁõÆÁ´ãË∂≥‰∫éÊµãÁªòÁ®ãÂ∫èËÆæËÆ°Á´ûËµõÂàõÂª∫ÁöÑÊµãÁªòÁ±ªÁÆóÊ≥ï‰ªìÂ∫ìÔºàAn open-source code of surveying and mapping algorithms for programming design.Ôºâ

*   [xuankuzcr/xuankuzcr.github.io](https://github.com/xuankuzcr/xuankuzcr.github.io) - My personal website.

*   [Jun-CEN/Jun-CEN.github.io](https://github.com/Jun-CEN/Jun-CEN.github.io) - My personal webset: cen-jun.com

*   [zouzhekang/YJYpaper](https://github.com/zouzhekang/YJYpaper) - ‰∏Ä‰∏™Áî®Êù•ËÆ∞ÂΩïÊ≠¶Ê±âÂ§ßÂ≠¶Êù®ÊôØÂ™õËÆ∫ÊñáÈóÆÈ¢òÁöÑ‰ªìÂ∫ì

*   [wdndev/llm\_interview\_note](https://github.com/wdndev/llm_interview_note) - ‰∏ªË¶ÅËÆ∞ÂΩïÂ§ßËØ≠Ë®ÄÂ§ßÊ®°ÂûãÔºàLLMsÔºâ ÁÆóÊ≥ïÔºàÂ∫îÁî®ÔºâÂ∑•Á®ãÂ∏àÁõ∏ÂÖ≥ÁöÑÁü•ËØÜÂèäÈù¢ËØïÈ¢ò

*   [MIT-SPARK/GNSS-ROS](https://github.com/MIT-SPARK/GNSS-ROS) -

*   [VINGS-Mono/VINGS-Mono.github.io](https://github.com/VINGS-Mono/VINGS-Mono.github.io) -

*   [PKUFlyingPig/cs-self-learning](https://github.com/PKUFlyingPig/cs-self-learning) - ËÆ°ÁÆóÊú∫Ëá™Â≠¶ÊåáÂçó

*   [vernesong/OpenClash](https://github.com/vernesong/OpenClash) - A Clash Client For OpenWrt

*   [rohinmanvi/GeoLLM](https://github.com/rohinmanvi/GeoLLM) -

*   [beichensky/Font](https://github.com/beichensky/Font) - FiraCode Âíå Operator Mono Â≠ó‰Ωì

*   [MrNeRF/awesome-3D-gaussian-splatting](https://github.com/MrNeRF/awesome-3D-gaussian-splatting) - Curated list of papers and resources focused on 3D Gaussian Splatting, intended to keep pace with the anticipated surge of research in the coming months.

## C

*   [JunweiLiang/xf\_mic\_asr\_offline\_junwei](https://github.com/JunweiLiang/xf_mic_asr_offline_junwei) -

*   [Robotics-STAR-Lab/ApexNav](https://github.com/Robotics-STAR-Lab/ApexNav) - \[RA-L'25] An Reliable and Efficient Framework for Zero-Shot Object Navigation

*   [PrideLab/PRIDE-PPPAR](https://github.com/PrideLab/PRIDE-PPPAR) - An open‚Äësource software for Multi-GNSS PPP ambiguity resolution

*   [0voice/algorithm-structure](https://github.com/0voice/algorithm-structure) - 2021Âπ¥ÊúÄÊñ∞ÊÄªÁªì 500‰∏™Â∏∏Áî®Êï∞ÊçÆÁªìÊûÑÔºåÁÆóÊ≥ïÔºåÁÆóÊ≥ïÂØºËÆ∫ÔºåÈù¢ËØïÂ∏∏Áî®ÔºåÂ§ßÂéÇÈ´òÁ∫ßÂ∑•Á®ãÂ∏àÊï¥ÁêÜÊÄªÁªì

*   [kevin2431/Traj-LO](https://github.com/kevin2431/Traj-LO) - \[RA-L 2024] In Defense of LiDAR-Only Odometry Using an Effective Continuous-Time Trajectory

*   [rtklibexplorer/RTKLIB](https://github.com/rtklibexplorer/RTKLIB) - A version of RTKLIB optimized for low cost GNSS receivers, especially u-blox receivers.  It is based on RTKLIB 2.4.3.  This software is provided ‚ÄúAS IS‚Äù without any warranties of any kind so please be careful, especially if using it in any kind of real-time application.  Click on the "Releases" label below to see the latest Windows pre-release.

*   [tomojitakasu/RTKLIB](https://github.com/tomojitakasu/RTKLIB) -

*   [MichaelBeechan/PPP-RTK](https://github.com/MichaelBeechan/PPP-RTK) - SPP„ÄÅRTD„ÄÅPPP„ÄÅRTK„ÄÅPPP-RTK„ÄÅRAIM„ÄÅARAIM et al

*   [Amoiensis/Matrix\_hub](https://github.com/Amoiensis/Matrix_hub) - A lib of Matrix operation for C language. (Áü©ÈòµËøêÁÆóÂ∫ì--CËØ≠Ë®Ä)

## Rust

*   [openai/harmony](https://github.com/openai/harmony) - Renderer for the harmony response format to be used with gpt-oss

*   [prefix-dev/pixi](https://github.com/prefix-dev/pixi) - Package management made easy

*   [rustdesk/rustdesk](https://github.com/rustdesk/rustdesk) - An open-source remote desktop application designed for self-hosting, as an alternative to TeamViewer.

*   [lapce/lapce](https://github.com/lapce/lapce) - Lightning-fast and Powerful Code Editor written in Rust

*   [typst/typst](https://github.com/typst/typst) - A markup-based typesetting system that is powerful and easy to learn.

*   [makeecat/Peng](https://github.com/makeecat/Peng) - A minimal quadrotor autonomy framework in Rust (Mac, Linux, Windows)

## Go

*   [tailscale/tailscale](https://github.com/tailscale/tailscale) - The easiest, most secure way to use WireGuard and 2FA.

*   [fatedier/frp](https://github.com/fatedier/frp) - A fast reverse proxy to help you expose a local server behind a NAT or firewall to the internet.

*   [XTLS/Xray-core](https://github.com/XTLS/Xray-core) - Xray, Penetrates Everything. Also the best v2ray-core. Where the magic happens. An open platform for various uses.

*   [yorukot/superfile](https://github.com/yorukot/superfile) - Pretty fancy and modern terminal file manager

*   [JanDeDobbeleer/oh-my-posh](https://github.com/JanDeDobbeleer/oh-my-posh) - The most customisable and low-latency cross platform/shell prompt renderer

*   [sourcegraph/sourcegraph-public-snapshot](https://github.com/sourcegraph/sourcegraph-public-snapshot) - Code AI platform with Code Search & Cody

## Lua

*   [ayamir/nvimdots](https://github.com/ayamir/nvimdots) - A well configured and structured Neovim.

*   [gaboolic/rime-shuangpin-fuzhuma](https://github.com/gaboolic/rime-shuangpin-fuzhuma) - Â¢®Â•áÈü≥ÂΩ¢ÔºåÊâìÈÄ†ÊúÄÂº∫ÂèåÊãºËæÖÂä©Á†ÅrimeËæìÂÖ•ÊñπÊ°àÔºåËÆ©Â§©‰∏ãÂèåÊãºÁî®Êà∑‰∫∫‰∫∫Áî®Âæó‰∏äËæÖÂä©Á†Å„ÄÇÂü∫‰∫éÈõæÂáá-ÁôΩÈúúËØçÂ∫ìÔºåÊîØÊåÅÂ∞èÈπ§ÂèåÊãº„ÄÅËá™ÁÑ∂Á†ÅÂèåÊãº„ÄÅÊêúÁãóÂèåÊãº„ÄÅÂæÆËΩØÂèåÊãºÁ≠âÂ§öÁßçÂèåÊãºÔºåËæÖÂä©Á†ÅÊîØÊåÅÂ¢®Â•áÁ†ÅÔºàÂéüÂàõÊãÜÂàÜÂºÄÊ∫êÊîØÊåÅ4‰∏áÂ≠óÔºâ„ÄÅËá™ÁÑ∂Á†ÅÈÉ®È¶ñËæÖ„ÄÅÂ∞èÈπ§Èü≥ÂΩ¢ÔºàÈπ§ÂΩ¢ËæÖÔºâÁ≠âÔºåÊîØÊåÅÂèåÊãºÂíåËæÖÂä©Á†Å‰πãÈó¥ÊéíÂàóÁªÑÂêàÔºåÊîØÊåÅÊï¥Âè•/Â≠óËØçËæìÂÖ•„ÄÇ‰∏çËÆ§ËØÜÁöÑÂ≠óÂèØ‰ª•Á¨îÁîª„ÄÅÈÉ®‰ª∂ÊãÜÂ≠ó„ÄÅ‰ªìÈ¢âÁ†ÅÂèçÊü•„ÄÇÊîØÊåÅaw„ÄÅajÊ®°ÂºèËæìÂÖ•Ëã±Êñá„ÄÅÊó•ÊñáÔºåÊîØÊåÅÂèåÊãºÂπ∂ÂáªËæìÂÖ•„ÄÅemoji„ÄÅÂø´Á¨¶„ÄÅÊó•Êúü„ÄÅÂ§ßÂÜôÊï∞Â≠ó„ÄÅËÆ°ÁÆóÂô®Á≠âÈ´òÁ∫ßÂäüËÉΩ„ÄÇÈõæÂááÈπ§|ÈõæÂááËá™ÁÑ∂|Â¢®Â•áÁ†Å|Â¢®Â•áÈü≥ÂΩ¢

## Vim Script

*   [neovim/neovim](https://github.com/neovim/neovim) - Vim-fork focused on extensibility and usability

*   [amix/vimrc](https://github.com/amix/vimrc) - The ultimate Vim configuration (vimrc)

*   [vim/vim](https://github.com/vim/vim) - The official Vim repository

*   [linrongbin16/lin.vim](https://github.com/linrongbin16/lin.vim) - Lin Rongbin's (Neo)Vim Distribution

## Cuda

*   [yanchi-3dv/diff-gaussian-rasterization-for-gsslam](https://github.com/yanchi-3dv/diff-gaussian-rasterization-for-gsslam) - The modified differential Gaussian rasterization in the CVPR 2024 highlight paper: GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting.

*   [computerhistory/AlexNet-Source-Code](https://github.com/computerhistory/AlexNet-Source-Code) - This package contains the original 2012 AlexNet code.

*   [carlinds/splatad](https://github.com/carlinds/splatad) - SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving

*   [YOUNG-bit/open\_semantic\_slam](https://github.com/YOUNG-bit/open_semantic_slam) - ICRA2025: OpenGS-SLAM: Open-Set Dense Semantic SLAM with 3D Gaussian Splatting for Object-Level Scene Understanding

*   [qdLMF/LightGlue-with-FlashAttentionV2-TensorRT](https://github.com/qdLMF/LightGlue-with-FlashAttentionV2-TensorRT) - A cutlass cute  implementation of headdim-64 flashattentionv2 TensorRT plugin for LightGlue. Run on Jetson Orin NX 8GB with TensorRT 8.5.2.

*   [deepseek-ai/DeepGEMM](https://github.com/deepseek-ai/DeepGEMM) - DeepGEMM: clean and efficient FP8 GEMM kernels with fine-grained scaling

*   [nerfstudio-project/gsplat](https://github.com/nerfstudio-project/gsplat) - CUDA accelerated rasterization of gaussian splatting

*   [MarvinChung/Orbeez-SLAM](https://github.com/MarvinChung/Orbeez-SLAM) -

## Roff

*   [TapXWorld/ChinaTextbook](https://github.com/TapXWorld/ChinaTextbook) - ÊâÄÊúâÂ∞èÂàùÈ´ò„ÄÅÂ§ßÂ≠¶PDFÊïôÊùê„ÄÇ

## Dockerfile

*   [Anduin2017/HowToCook](https://github.com/Anduin2017/HowToCook) - Á®ãÂ∫èÂëòÂú®ÂÆ∂ÂÅöÈ•≠ÊñπÊ≥ïÊåáÂçó„ÄÇProgrammer's guide about how to cook at home (Simplified Chinese only).

*   [linyicheng1/Dockers](https://github.com/linyicheng1/Dockers) - ‰∏Ä‰∫õÂ∏∏Áî®ÁöÑDockerfile Êñá‰ª∂ÔºåËÉΩÂ§üÂø´ÈÄüÈÉ®ÁΩ≤ËøêË°å‰∏Ä‰∫õÂ∏∏Áî®ÁÆóÊ≥ïÔºåÈÅøÂÖçÈáçÂ§çÈÖçÁΩÆÁéØÂ¢É

*   [jaeseok4104/slam-docker](https://github.com/jaeseok4104/slam-docker) - SLAM Docker for research

## C\#

*   [Achuan-2/SlideSCI](https://github.com/Achuan-2/SlideSCI) - PPT plugin, supports one-click to add image titles, copy and paste positions, one-click image alignment, and one-click to insert Markdown (including bold, hyperlinks, and other inline styles, as well as code blocks, LaTeX, and other block-level styles)! PPTÊèí‰ª∂ÔºåÊîØÊåÅ‰∏ÄÈîÆÊ∑ªÂä†ÂõæÁâáÊ†áÈ¢òÔºåÂ§çÂà∂Á≤òË¥¥‰ΩçÁΩÆ„ÄÅ‰∏ÄÈîÆÂõæÁâáÂØπÈΩê„ÄÅ‰∏ÄÈîÆÊèíÂÖ•MarkdownÔºàÂä†Á≤ó„ÄÅË∂ÖÈìæÊé•Á≠âË°åÂÜÖÊ†∑Âºè„ÄÅ‰ª£Á†ÅÂùó„ÄÅLaTeXÁ≠âÂùóÁ∫ßÊ†∑ÂºèÔºâ„ÄÅ‰æøÊç∑ÂØºÂá∫ÂõæÁâáÔºÅ

*   [2dust/v2rayN](https://github.com/2dust/v2rayN) - A GUI client for Windows, Linux and macOS, support Xray and sing-box and others

*   [mahoshojo0805/ContestPrograms](https://github.com/mahoshojo0805/ContestPrograms) - ÊµãÁªòÊäÄËÉΩÂ§ßËµõÁ®ãÂ∫è

*   [Code52/carnac](https://github.com/Code52/carnac) - A utility to give some insight into how you use your keyboard

## MATLAB

*   [MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning](https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning) - This is the homepage of a new book entitled "Mathematical Foundations of Reinforcement Learning."

*   [i2Nav-WHU/KF-GINS-Matlab](https://github.com/i2Nav-WHU/KF-GINS-Matlab) - An EKF-based GNSS/INS Integrated Navigation Systems in Matlab (Matlab Version of KF-GINS)

*   [zhao-zhibo/INS](https://github.com/zhao-zhibo/INS) - INS.IMU. Inertial navigation mechanical arrangement algorithm, based on Yan Gongmin's PSINS ÊÉØÂØºÊú∫Ê¢∞ÁºñÊéíÁÆóÊ≥ïÔºå‰ª•‰∏•ÊÅ≠ÊïèÁöÑPSINS‰∏∫Âü∫Á°ÄÔºåÂèØ‰ª•ÂÆåÊàêÊ≠¶Ê±âÂ§ßÂ≠¶ÁöÑÊú∫Ê¢∞ÁºñÊéíËØæÁ®ã‰Ωú‰∏ö.

## Vue

*   [JunyaoHu/academic-project-page-template-vue](https://github.com/JunyaoHu/academic-project-page-template-vue) - A vue-based project page template for academic papers. (in development) https://junyaohu.github.io/academic-project-page-template-vue

## Makefile

*   [zh-google-styleguide/zh-google-styleguide](https://github.com/zh-google-styleguide/zh-google-styleguide) - Google ÂºÄÊ∫êÈ°πÁõÆÈ£éÊ†ºÊåáÂçó (‰∏≠ÊñáÁâà)

*   [kahowang/FAST\_LIO\_SAM](https://github.com/kahowang/FAST_LIO_SAM) - Front\_end : fastlio2  Back\_end : lio\_sam

## LLVM

*   [llvm/llvm-project](https://github.com/llvm/llvm-project) - The LLVM Project is a collection of modular and reusable compiler and toolchain technologies.

## Svelte

*   [open-webui/open-webui](https://github.com/open-webui/open-webui) - User-friendly AI Interface (Supports Ollama, OpenAI API, ...)

## Matlab

*   [Relja/netvlad](https://github.com/Relja/netvlad) - NetVLAD: CNN architecture for weakly supervised place recognition

## Cython

*   [stereolabs/zed-python-api](https://github.com/stereolabs/zed-python-api) - Python API for the ZED SDK

## SCSS

*   [RayeRen/acad-homepage.github.io](https://github.com/RayeRen/acad-homepage.github.io) - AcadHomepage: A Modern and Responsive Academic Personal Homepage

*   [mcdviral/mcdviral.github.io](https://github.com/mcdviral/mcdviral.github.io) -

## Dart

*   [localsend/localsend](https://github.com/localsend/localsend) - An open-source cross-platform alternative to AirDrop

*   [chen08209/FlClash](https://github.com/chen08209/FlClash) - A multi-platform proxy client based on ClashMeta,simple and easy to use, open-source and ad-free.

## Markdown

*   [labuladong/fucking-algorithm](https://github.com/labuladong/fucking-algorithm) - Âà∑ÁÆóÊ≥ïÂÖ®Èù†Â•óË∑ØÔºåËÆ§ÂáÜ labuladong Â∞±Â§ü‰∫ÜÔºÅEnglish version supported! Crack LeetCode, not only how, but also why.

*   [codecrafters-io/build-your-own-x](https://github.com/codecrafters-io/build-your-own-x) - Master programming by recreating your favorite technologies from scratch.

## Clojure

*   [tonsky/FiraCode](https://github.com/tonsky/FiraCode) - Free monospaced font with programming ligatures

## Java

*   [krahets/hello-algo](https://github.com/krahets/hello-algo) - „ÄäHello ÁÆóÊ≥ï„ÄãÔºöÂä®ÁîªÂõæËß£„ÄÅ‰∏ÄÈîÆËøêË°åÁöÑÊï∞ÊçÆÁªìÊûÑ‰∏éÁÆóÊ≥ïÊïôÁ®ã„ÄÇÊîØÊåÅ Python, Java, C++, C, C#, JS, Go, Swift, Rust, Ruby, Kotlin, TS, Dart ‰ª£Á†Å„ÄÇÁÆÄ‰ΩìÁâàÂíåÁπÅ‰ΩìÁâàÂêåÊ≠•Êõ¥Êñ∞ÔºåEnglish version in translation
